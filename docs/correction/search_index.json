[
["index.html", "Machine learning Présentation", " Machine learning Laurent Rouvière 2020-09-11 Présentation Ce tutoriel présente une introduction au machine learning avec R. On pourra trouver : les supports de cours associés à ce tutoriel ainsi que les données utilisées à l’adresse suivante https://lrouviere.github.io/ml_lecture/ ; le tutoriel sans les correction à l’url https://lrouviere.github.io/TUTO_ML/ le tutoriel avec les corrigés (à certains moment) à l’url https://lrouviere.github.io/TUTO_ML/correction/. Il est recommandé d’utiliser mozilla firefox pour lire le tutoriel. Les thèmes suivants sont abordés : Estimation du risque, présentation du package caret ; SVM, cas séparable, non séparable et astuce du noyau ; Arbres, notamment l’algorithme CART ; Agrégation d’arbres, forêts aléatoires et gradient boosting ; Réseaux de neurones et introduction au deep learning, perceptron multicouches avec keras. Il existe de nombreuses références sur le machine learning, la plus connue étant certainement Hastie, Tibshirani, and Friedman (2009), disponible en ligne à l’url https://web.stanford.edu/~hastie/ElemStatLearn/. On pourra également consulter Boehmke and Greenwell (2019) qui propose une présentation très claire des algorithmes machine learning avec R. Cet ouvrage est également disponible en ligne à l’url https://bradleyboehmke.github.io/HOML/. Références "],
["caret.html", "Chapitre 1 Estimation du risque avec caret 1.1 Notion de risque en apprentissage supervisé 1.2 La validation croisée 1.3 Le package caret 1.4 Compléments", " Chapitre 1 Estimation du risque avec caret 1.1 Notion de risque en apprentissage supervisé L’apprentissage supervisé consiste à expliquer ou prédire une sortie \\(y\\in\\mathcal Y\\) par des entrées \\(x\\in\\mathcal X\\) (le plus souvent \\(\\mathcal X=\\mathbb R^p\\)). Cela revient à trouver un algorithme ou machine représenté par une fonction \\[f:\\mathcal X\\to\\mathcal Y\\] qui à une nouvelle observation \\(x\\) associe la prévision \\(f(x)\\). Bien entendu le problème consiste à chercher le meilleur algorithme pour le cas d’intérêt. Cette notion nécessite de meilleur algorithme la définition de critères que l’on va chercher à optimiser. Les critères sont le plus souvent définis à partir du fonction de perte \\[\\begin{align*} \\ell:\\mathcal Y \\times\\mathcal Y &amp; \\mapsto \\mathbb R^+ \\\\ (y,y^\\prime) &amp; \\to\\ell(y,y^\\prime) \\end{align*}\\] où \\(\\ell(y,y^\\prime)\\) représentera l’erreur (ou la perte) pour la prévision \\(y^\\prime\\) par rapport à l’observation \\(y\\). Si on représente le phénomène d’intérêt par un couple aléatoire \\((X,Y)\\) à valeurs dans \\(\\mathcal X\\times\\mathcal Y\\), on mesurera la performance d’un algorithme \\(f\\) par son risque \\[\\mathcal R(f)=\\mathbf E[\\ell(Y,f(X))].\\] Trouver le meilleur algorithme revient alors à trouver \\(f\\) qui minimise \\(\\mathcal R(f)\\). Bien entendu, ce cadre possède une utilité limitée en pratique puisqu’on ne connaît jamais la loi de \\((X,Y)\\), on ne pourra donc jamais calculé le vrai risque d’un algorithme \\(f\\). Tout le problème va donc être de trouver l’algorithme qui a le plus petit risque à partir de \\(n\\) observations \\((x_1,y_1),\\dots,(x_n,y_n)\\). Nous verrons dans les chapitres suivants plusieurs façons de construire des algorithmes mais, dans tous les cas, un algorithme est représenté par une fonction \\[f_n:\\mathcal X\\times(\\mathcal X\\times\\mathcal Y)^n\\to\\mathcal Y\\] qui, pour une nouvelle donnée \\(x\\), renverra la prévision \\(f_n(x)\\) calculée à partir de l’échantillon qui vit dans \\((\\mathcal X\\times\\mathcal Y)^n\\). Dès lors la question qui se pose est de calculer (ou plutôt d’estimer) le risque (inconnu) \\(\\mathcal R(f_n)\\) d’un algorithme \\(f_n\\). Les techniques classiques reposent sur des algorithmes de type validation croisée. Nous les mettons en œuvre dans cette partie pour un algorithme simple : les \\(k\\) plus proches voisins. On commencera par programmer ces techniques “à la main” puis on utilisera le package caret qui permet de calculer des risques pour quasiment tous les algorithmes que l’on retrouver en apprentissage supervisé. 1.2 La validation croisée On cherche à expliquer une variable binaire \\(Y\\) par deux variables quantitatives \\(X_1\\) et \\(X_2\\) à l’aide du jeu de données suivant n &lt;- 2000 set.seed(12345) X1 &lt;- runif(n) X2 &lt;- runif(n) set.seed(9012) R1 &lt;- X1&lt;=0.25 R2 &lt;- (X1&gt;0.25 &amp; X2&gt;=0.75) R3 &lt;- (X1&gt;0.25 &amp; X2&lt;0.75) Y &lt;- rep(0,n) Y[R1] &lt;- rbinom(sum(R1),1,0.25) Y[R2] &lt;- rbinom(sum(R2),1,0.25) Y[R3] &lt;- rbinom(sum(R3),1,0.75) donnees &lt;- data.frame(X1,X2,Y) donnees$Y &lt;- as.factor(donnees$Y) ggplot(donnees)+aes(x=X1,y=X2,color=Y)+geom_point() On considère la perte indicatrice : \\(\\ell(y,y^\\prime)=\\mathbf 1_{y\\neq y^\\prime}\\), le risque d’un algorithme \\(f\\) est donc \\[\\mathcal R(f)=\\mathbf E[\\mathbf 1_{Y\\neq f(X)}]=\\mathbf P(Y\\neq f(X)),\\] il est appelé probabilité d’erreur ou erreur de classification. Séparer le jeu de données en un échantillon d’apprentissage dapp de taille 1500 et un échantillon test dtest de taille 500. set.seed(234) indapp &lt;- sample(nrow(donnees),1500) dapp &lt;- donnees[indapp,] dtest &lt;- donnees[-indapp,] On considère la règle de classification des \\(k\\) plus proches voisins. Pour un entier \\(k\\) plus petit que \\(n\\) et un nouvel individu \\(x\\), cette règle affecte à \\(x\\) le label majoritaire des \\(k\\) plus proches voisins de \\(x\\). Sur R on utilise la fonction knn du package class. On peut par exemple obtenir les prévisions des individus de l’échantillon test de la règle des 3 plus proches voisins avec library(class) knn3 &lt;- knn(dapp[,1:2],dtest[,1:2],cl=dapp$Y,k=3) library(class) knn3 &lt;- knn(dapp[,1:2],dtest[,1:2],cl=dapp$Y,k=3) Calculer l’erreur de classification de la règle des 3 plus proches voisins sur les données test (procédure validation hold out). mean(knn3!=dtest$Y) [1] 0.338 Expliquer la fonction knn.cv. Cette fonction permet, pour la règle des plus proches voisins, de prédire le groupe de chaque individu par validation croisée leave-one-out : \\[\\widehat y_i=g_{k,i}(x_i),\\quad i=1,\\dots,n\\] où \\(g_{k,i}\\) désigne la règle de \\(k\\) plus proche voisins construites à partir de l’échantillon amputé de la \\(i\\)ème observation. Calculer l’erreur de classification de la règle des 3 plus proches voisins par validation croisée leave-one-out. prev_cv &lt;- knn.cv(donnees[,-3],cl=donnees$Y,k=3) On peut alors estimer l’erreur de la règle des 10 ppv par \\[\\frac{1}{n}\\sum_{i=1}^n1_{g_{k,i}(x_i)\\neq y_i}.\\] mean(prev_cv!=donnees$Y) [1] 0.334 On considère le vecteur de plus proches voisins suivant : K_cand &lt;- seq(1,500,by=20) Sélectionner une valeur de \\(k\\) dans ce vecteur à l’aide d’une validation hold out et d’un leave-one-out : On calcule l’erreur de classification par validation hold out pour chaque valeur de \\(k\\) : err.ho &lt;- rep(0,length(K_cand)) for (i in 1:length(K_cand)){ knni &lt;- knn(dapp[,1:2],dtest[,1:2],cl=dapp$Y,k=K_cand[i]) err.ho[i] &lt;- mean(knni!=dtest$Y) } Puis on choisit la valeur de \\(k\\) pour laquelle l’erreur est minimale. K_cand[which.min(err.ho)] [1] 41 On de même chose avec la validation croisée leave-one-out : err.loo &lt;- rep(0,length(K_cand)) for (i in 1:length(K_cand)){ knni &lt;- knn.cv(donnees[,-3],cl=donnees$Y,k=K_cand[i]) err.loo[i] &lt;- mean(knni!=donnees$Y) } K_cand[which.min(err.loo)] [1] 121 Faire la même chose à l’aide d’une validation croisée 10 blocs. On pourra construire les blocs avec set.seed(2345) blocs &lt;- caret::createFolds(1:nrow(donnees),10,returnTrain = TRUE) err.cv &lt;- rep(0,length(K_cand)) prev &lt;- donnees$Y for (i in 1:length(K_cand)){ for (j in 1:length(blocs)){ train &lt;- donnees[blocs[[j]],] test &lt;- donnees[-blocs[[j]],] prev[-blocs[[j]]] &lt;- knn(train[,1:2],test[,1:2],cl=train$Y,k=K_cand[i]) } err.cv[i] &lt;- mean(prev!=donnees$Y) } K_cand[which.min(err.cv)] [1] 101 1.3 Le package caret Dans la partie précédente, nous avons utiliser des méthodes de validation croisée pour sélectionner le nombre de voisins dans l’algorithme des plus proches voisins. L’approche revenait à estimer un risque pour une grille de valeurs candidates de \\(k\\) choisir la valeur de \\(k\\) qui minimise le risque estimé. Cette pratique est courante en machine learning : on la retrouve fréquemment pour calibrer les algorithmes. Le protocole est toujours le même, pour un méthode donnée il faut spécifier : une grille de valeurs pour les paramètres un risque un algorithme pour estimer le risque. Le package caret permet d’appliquer ce protocole pour plus de 200 algorithmes machine learning. On pourra trouver une documentation complète à cette url http://topepo.github.io/caret/index.html. Deux fonctions sont à utiliser : traincontrol qui permettra notamment de spécifier l’algorithme pour estimer le risque ainsi que les paramètres de cet algorithme ; train dans laquelle on renseignera les données, la grille de candidats… On reprend les données de la partie précédente. Expliquer les sorties des commandes library(caret) set.seed(321) ctrl1 &lt;- trainControl(method=&quot;LGOCV&quot;,number=1) KK &lt;- data.frame(k=K_cand) caret.ho &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl1,tuneGrid=KK) caret.ho k-Nearest Neighbors 2000 samples 2 predictor 2 classes: &#39;0&#39;, &#39;1&#39; No pre-processing Resampling: Repeated Train/Test Splits Estimated (1 reps, 75%) Summary of sample sizes: 1500 Resampling results across tuning parameters: k Accuracy Kappa 1 0.602 0.1956346 21 0.690 0.3649415 41 0.694 0.3736696 61 0.706 0.3992546 81 0.700 0.3867338 101 0.712 0.4122641 121 0.700 0.3882944 141 0.706 0.4017971 161 0.700 0.3903629 181 0.702 0.3941710 201 0.700 0.3898471 221 0.696 0.3806637 241 0.692 0.3714491 261 0.698 0.3829078 281 0.692 0.3693074 301 0.696 0.3764358 321 0.682 0.3474407 341 0.682 0.3468831 361 0.678 0.3352601 381 0.672 0.3214167 401 0.668 0.3113633 421 0.666 0.3057172 441 0.658 0.2853800 461 0.658 0.2841354 481 0.654 0.2732314 Accuracy was used to select the optimal model using the largest value. The final value used for the model was k = 101. plot(caret.ho) On obtient ici l’accuracy (1 moins l’erreur de classification) pour chaque valeur de \\(k\\) calculé par validation hold out. Cette technique a été précisée dans la fonction trainControl via l’option method=\"LGOCV\". Un autre indicateur est calculé : le kappa de Cohen. Cet indicateur peut se révéler pertinent en présence de données déséquilibrées, on pourra trouver de l’information sur cet indicateur dans ce document https://lrouviere.github.io/INP-HB/cours_don_des.pdf En modifiant les paramètres du code précédent, retrouver les résultats de la validation hold out de la partie précédente. On pourra utiliser l’option index dans la fonction trainControl. ctrl2 &lt;- trainControl(method=&quot;LGOCV&quot;,number=1,index=list(indapp)) caret.ho2 &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl2,tuneGrid=KK) caret.ho2$bestTune k 3 41 On retrouve bien la même valeur de \\(k\\). Utiliser caret pour sélectionner \\(k\\) par validation croisée leave-one-out. ctrl3 &lt;- trainControl(method=&quot;LOOCV&quot;,number=1) caret.loo &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl3,tuneGrid=KK) caret.loo$bestTune k 7 121 Faire de même pour la validation croisée 10 blocs en gardant les mêmes blocs que dans la partie précédente. ctrl4 &lt;- trainControl(method=&quot;cv&quot;,index=blocs) caret.cv &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl4,tuneGrid=KK) caret.cv$bestTune k 6 101 1.4 Compléments 1.4.1 Calcul parallèle Les validations croisées peuvent se révéler coûteuses en temps de calcul. On utilise souvent des techniques de parallélisation pour améliorer les performances computationnelles. Ces techniques sont relativement facile à mettre en œuvre avec caret, on peut par exemple utiliser la librairie doParallel : library(doParallel) cl &lt;- makePSOCKcluster(1) registerDoParallel(cl) system.time(ee3 &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl4,tuneGrid=KK)) user system elapsed 12.708 0.042 12.803 stopCluster(cl) cl &lt;- makePSOCKcluster(4) registerDoParallel(cl) system.time(ee3 &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl4,tuneGrid=KK)) user system elapsed 0.504 0.013 5.703 stopCluster(cl) 1.4.2 Répéter les méthodes de rééchantillonnage Les méthodes d’estimation du risque présentées dans cette partie (hold out, validation croisée) sont basées sur du rééchantillonnage. Elles peuvent se révéler sensible à la manière de couper l’échantillon. C’est pourquoi il est recommandé de les répéter plusieurs fois et de moyenner les erreurs sur les répétitions. Ces répétitions sont très faciles à mettre en œuvre avec caret, par exemple pour la validation hold out on utilise l’option number: ctrl &lt;- trainControl(method=&quot;LGOCV&quot;,number=5) caret.ho.rep &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl,tuneGrid=KK) la validation croisée on utilise les options repeatedcv et repeats : ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;,repeats=5) caret.ho.rep &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl,tuneGrid=KK) 1.4.3 Modifier le risque Enfin nous avons uniquement considéré l’erreur de classification. Il est bien entendu possible d’utiliser d’autres risques pour évaluer les performances. C’est l’option metric de la fonction train qui permet généralement de spécifier le risque, si on est par exemple intéressé par l’aire sur la courbe ROC (AUC) on fera : donnees1 &lt;- donnees names(donnees1)[3] &lt;- &quot;Class&quot; levels(donnees1$Class) &lt;- c(&quot;G0&quot;,&quot;G1&quot;) ctrl &lt;- trainControl(method=&quot;LGOCV&quot;,number=1,classProbs=TRUE,summary=twoClassSummary) caret.auc &lt;- train(Class~.,data=donnees1,method=&quot;knn&quot;,trControl=ctrl,tuneGrid=KK,metric=&quot;ROC&quot;) caret.auc k-Nearest Neighbors 2000 samples 2 predictor 2 classes: &#39;G0&#39;, &#39;G1&#39; No pre-processing Resampling: Repeated Train/Test Splits Estimated (1 reps, 75%) Summary of sample sizes: 1500 Resampling results across tuning parameters: k ROC Sens Spec 1 0.5904827 0.5758929 0.6050725 21 0.6945765 0.6294643 0.7789855 41 0.7206506 0.6250000 0.7789855 61 0.7291424 0.6250000 0.7753623 81 0.7247752 0.6205357 0.7934783 101 0.7241282 0.6250000 0.7934783 121 0.7204322 0.6250000 0.8007246 141 0.7198580 0.6294643 0.7862319 161 0.7221791 0.6250000 0.7826087 181 0.7225188 0.6205357 0.7826087 201 0.7170597 0.6205357 0.7934783 221 0.7143100 0.6160714 0.7862319 241 0.7196801 0.6205357 0.7898551 261 0.7150055 0.6205357 0.7898551 281 0.7184669 0.6116071 0.7898551 301 0.7187096 0.5892857 0.7971014 321 0.7187904 0.5758929 0.8007246 341 0.7178927 0.5491071 0.8079710 361 0.7158789 0.5178571 0.8224638 381 0.7177957 0.5089286 0.8224638 401 0.7168818 0.4776786 0.8297101 421 0.7174964 0.4598214 0.8478261 441 0.7134770 0.4330357 0.8623188 461 0.7141401 0.4241071 0.8695652 481 0.7131535 0.4107143 0.8659420 ROC was used to select the optimal model using the largest value. The final value used for the model was k = 61. "],
["SVM.html", "Chapitre 2 Support Vector Machine (SVM) 2.1 Cas séparable 2.2 Cas non séparable 2.3 L’astuce du noyau 2.4 Exercices", " Chapitre 2 Support Vector Machine (SVM) Etant donnée un échantillon \\((x_1,y_1),\\dots,(x_n,y_n)\\) où les \\(x_i\\) sont à valeurs dans \\(\\mathbb R^p\\) et les \\(y_i\\) sont binaires à valeurs dans \\(\\{-1,1\\}\\), l’approche SVM cherche le meilleur hyperplan en terme de séparation des données. Globalement on veut que les 1 se trouvent d’un coté de l’hyperplan et les -1 de l’autre. Dans cette partie on propose d’étudier la mise en œuvre de cet algorithme tout d’abord dans le cas idéal où les données sont séparables puis dans le cas plus réel où elles ne le sont pas. Nous verrons ensuite comment introduire de la non linéarité ne utilisant l’astuce du noyau. 2.1 Cas séparable Le cas séparable est le cas facile : il correspond à la situation où il existe effectivement un (même plusieurs) hyperplan(s) qui sépare(nt) parfaitement les 1 des -1. Il ne se produit quasiment jamais en pratique mais il convient de l’étudier pour comprendre comment est construit l’algorithme. Dans ce cas on cherche l’hyperplan d’équation \\(\\langle w,x\\rangle+b=w^tx+b=0\\) tel que la marge (qui peut être vue comme la distance entre les observations les plus proches de l’hyperplan et l’hyperplan) soit maximale. Mathématiquement le problème se réécrit comme un problème d’optimisation sous contraintes : \\[\\begin{equation} \\min_{w,b}\\frac{1}{2}\\|w\\|^2 \\tag{2.1} \\end{equation}\\] \\[\\text{sous les contraintes } y_i(w^tx_i+b)\\geq 1,\\ i=1,\\dots,n.\\] La solution s’obtient de façon classique en résolvant le problème dual et elle s’écrit comme une combinaison linéaire des \\(x_i\\) \\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\] De plus, les conditions KKT impliquent que pour tout \\(i=1,\\dots,n\\): \\(\\alpha_i^\\star=0\\) ou \\(y_i(x_i^tw+b)-1=0.\\) Ces conditions impliquent que \\(w^\\star\\) s’écrit comme une combinaison linéaire de quelques points, appelés vecteurs supports qui se trouvent sur la marge. Nous proposons maintenant de retrouver ces points et de tracer la marge sur un exemple simple. On considère le nuage de points suivant : n &lt;- 20 set.seed(123) X1 &lt;- scale(runif(n)) set.seed(567) X2 &lt;- scale(runif(n)) Y &lt;- rep(-1,n) Y[X1&gt;X2] &lt;- 1 Y &lt;- as.factor(Y) donnees &lt;- data.frame(X1=X1,X2=X2,Y=Y) p &lt;- ggplot(donnees)+aes(x=X2,y=X1,color=Y)+geom_point() p La fonction svm du package e1071 permet d’ajuster une SVM : library(e1071) mod.svm &lt;- svm(Y~.,data=donnees,kernel=&quot;linear&quot;,cost=10000000000) Récupérer les vecteurs supports et visualiser les sur le graphe (en utilisant une autre couleur par exemple). On les affectera à un data.frame dont les 2 premières colonnes représenteront les valeurs de \\(X_1\\) et \\(X_2\\) des vecteurs supports. Les vecteurs supports se trouvent dans la sortie index de la fonction svm : ind.svm &lt;- mod.svm$index sv &lt;- donnees %&gt;% slice(ind.svm) sv X1 X2 Y 1 -1.61179777 -0.6599042 -1 2 0.06962369 0.7140262 -1 3 -0.31095135 -0.5332139 1 p1 &lt;- p+geom_point(data=sv,aes(x=X2,y=X1),color=&quot;blue&quot;,size=2) On peut ainsi représenter la marge en traçant les droites qui passent par ces points. sv1 &lt;- sv[,2:1] b &lt;- (sv1[1,2]-sv1[2,2])/(sv1[1,1]-sv1[2,1]) a &lt;- sv1[1,2]-b*sv1[1,1] a1 &lt;- sv1[3,2]-b*sv1[3,1] p1+geom_abline(intercept = c(a,a1),slope=b,col=&quot;blue&quot;,size=1) Retrouver ce graphe à l’aide de la fonction plot. plot(mod.svm,data=donnees,grid=250) Rappeler la règle de décision associée à la méthode SVM. Donner les estimations des paramètres de la règle de décision sur cet exemple. On pourra notamment regarder la sortie coef de la fonction svm. Une fois \\(w^\\star\\) et \\(b^\\star\\) obtenus, la règle s’écrit \\[g(x)=1_{\\langle w^\\star,x\\rangle+b^\\star\\leq 0}-1_{\\langle w^\\star,x\\rangle+b^\\star&gt;0}.\\] L’objet mod.svm$coefs contient les coefficients \\(\\alpha_i^\\star y_i\\) pour chaque vecteur support. On peut ainsi récupérer l’équation de l’hyperplan et faire la prévision avec w &lt;- apply(mod.svm$coefs*donnees[mod.svm$index,1:2],2,sum) w X1 X2 -1.745100 2.136029 b &lt;- -mod.svm$rho b [1] -0.4035113 L’hyperplan séparateur a donc pour équation : \\[-1.74x_1+2.12x_2-0.40=0.\\] On dispose d’un nouvel individu \\(x=(-0.5,0.5)\\). Expliquer comment on peut prédire son groupe. Il suffit de calculer \\(\\langle w^\\star,x\\rangle+b\\) et de prédire en fonction du signe de cette valeur : newX &lt;- data.frame(X1=-0.5,X2=0.5) sum(w*newX)+b [1] 1.537053 On prédira le groupe -1 pour ce nouvel individu. Retrouver les résultats de la question précédente à l’aide de la fonction predict. On pourra utiliser l’option decision.values = TRUE. predict(mod.svm,newX,decision.values = TRUE) 1 -1 attr(,&quot;decision.values&quot;) -1/1 1 1.537053 Levels: -1 1 Plus cette valeur est élevée, plus on est loin de l’hyperplan. On peut donc l’interpréter comme un score. Obtenir les probabilités prédites à l’aide de la fonction predict. On pourra utiliser probability=TRUE dans la fonction svm. mod.svm1 &lt;- svm(Y~.,data=donnees,kernel=&quot;linear&quot;,cost=10000000000,probability=TRUE) predict(mod.svm1,newX,decision.values=TRUE,probability=TRUE) 1 -1 attr(,&quot;decision.values&quot;) -1/1 1 1.537053 attr(,&quot;probabilities&quot;) -1 1 1 0.8294474 0.1705526 Levels: -1 1 Comme souvent, il est possible d’obtenir une estimation des probabilités d’être dans les groupes -1 et 1 à partir du score, il “suffit” de ramener ce score sur l’échelle \\([0,1]\\) avec des transformations de type logit par exemple. Pour la svm, ces probabilités sont obtenues en ajustant un modèle logistique sur les scores \\(S(x)\\) : \\[P(Y=1|X=x)=\\frac{1}{1+\\exp(aS(x)+b)}.\\] On peut retrouver ces probabilités avec : score.newX &lt;- sum(w*newX)+b 1/(1+exp(-(mod.svm1$probB+mod.svm1$probA*score.newX))) [1] 0.1705526 2.2 Cas non séparable Dans la vraie vie, les groupes ne sont généralement pas séparables et il n’existe donc pas de solution au problème (2.1). On va donc autoriser certains points à être : mal classés et/ou bien classés mais à l’intérieur de la marge. Mathématiquement, cela revient à introduire des variables ressorts (slacks variables) \\(\\xi_1,\\dots,\\xi_n\\) positives telles que : \\(\\xi_i\\in [0,1]\\Longrightarrow\\) \\(i\\) bien classé mais dans la région définie par la marge ; \\(\\xi_i&gt;1 \\Longrightarrow\\) \\(i\\) mal classé. Le problème d’optimisation est alors de minimiser en \\((w,b,\\xi)\\) \\[\\frac{1}{2}\\|w\\|^2 +C\\sum_{i=1}^n\\xi_i\\] \\[\\textrm{sous les contraintes } \\left\\{ \\begin{array}{l} y_i(w^tx_i+b)\\geq 1 -\\xi_i \\\\ \\xi_i\\geq 0, i=1,\\dots,n. \\end{array}\\right.\\] Le paramètre \\(C&gt;0\\) est à calibrer et on remarque que le cas séparable correspond à \\(C\\to +\\infty\\). Les solutions de ce nouveau problème d’optimisation s’obtiennent de la même façon que dans le cas séparable, en particulier \\(w^\\star\\) s’écrite toujours comme une combinaison linéaire \\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\] de vecteurs supports sauf qu’on distingue deux types de vecteurs supports (\\(\\alpha_i^\\star&gt;0\\)): ceux sur la frontière définie par la marge : \\(\\xi_i^\\star=0\\) ; ceux en dehors : \\(\\xi_i^\\star&gt;0\\) et \\(\\alpha_i^\\star=C\\). Le choix de \\(C\\) est crucial : ce paramètre régule le compromis biais/variance de la svm : \\(C\\searrow\\): la marge est privilégiée et les \\(\\xi_i\\nearrow\\) \\(\\Longrightarrow\\) beaucoup d’observations dans la marge ou mal classées (et donc beaucoup de vecteurs supports). \\(C\\nearrow\\Longrightarrow\\) \\(\\xi_i\\searrow\\) donc moins d’observations mal classées \\(\\Longrightarrow\\) meilleur ajustement mais petite marge \\(\\Longrightarrow\\) risque de surajustement. On choisit généralement ce paramètre à l’aide des techiques présentées dans le chapitre 1 : choix d’une grille de valeurs de \\(C\\) et d’un critère ; choix d’une méthode de ré-échantillonnage pour estimer le critère ; choix de la valeur de \\(C\\) qui minimise le critère estimé. On considère le jeu de données df3 définie ci-dessous. n &lt;- 1000 set.seed(1234) df &lt;- as.data.frame(matrix(runif(2*n),ncol=2)) df1 &lt;- df %&gt;% filter(V1&lt;=V2)%&gt;% mutate(Y=rbinom(nrow(.),1,0.95)) df2 &lt;- df %&gt;% filter(V1&gt;V2)%&gt;% mutate(Y=rbinom(nrow(.),1,0.05)) df3 &lt;- bind_rows(df1,df2) %&gt;% mutate(Y=as.factor(Y)) ggplot(df3)+aes(x=V2,y=V1,color=Y)+geom_point()+ scale_color_manual(values=c(&quot;#FFFFC8&quot;, &quot;#7D0025&quot;))+ theme(panel.background = element_rect(fill = &quot;#BFD5E3&quot;, colour = &quot;#6D9EC1&quot;,size = 2, linetype = &quot;solid&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Ajuster 3 svm en considérant comme valeur de \\(C\\) : 0.000001, 0.1 et 5. On pourra utiliser l’option cost. mod.svm1 &lt;- svm(Y~.,data=df3,kernel=&quot;linear&quot;,cost=0.000001) mod.svm2 &lt;- svm(Y~.,data=df3,kernel=&quot;linear&quot;,cost=0.1) mod.svm3 &lt;- svm(Y~.,data=df3,kernel=&quot;linear&quot;,cost=5) Calculer les nombres de vecteurs supports pour chaque valeur de \\(C\\). mod.svm1$nSV [1] 469 469 mod.svm2$nSV [1] 178 178 mod.svm3$nSV [1] 150 150 Visualiser les 3 svm obtenues. Interpréter. plot(mod.svm1,data=df3,grid=250) plot(mod.svm2,data=df3,grid=250) plot(mod.svm3,data=df3,grid=250) Pour \\(C\\) petit, toutes les observations sont classées 0, la marge est grande et le nombre de vecteurs supports importants. On remarque que ces deux quantités diminuent lorsque \\(C\\) augmente. 2.3 L’astuce du noyau Les SVM présentées précédemment font l’hypothèse que les groupes sont linéairement séparables, ce qui n’est bien entendu pas toujours le cas en pratique. L’astuce du noyau permet de mettre de la non linéarité, elle consiste à : plonger les données dans un nouvel espace appelé espace de représentation ou feature space ; appliquer une svm linéaire dans ce nouvel espace. Le terme astuce vient du fait que ce procédé ne nécessite pas de connaître explicitement ce nouvel espace : pour résoudre le problème d’optimisation dans le feature space on a juste besoin de connaître le noyau associé au feature space. D’un point de vu formel un noyau est une fonction \\[K:\\mathcal X\\times\\mathcal X\\to\\mathbb R\\] dont les propriétés sont proches d’un produit scalaire. Il existe donc tout un tas de noyau avec lesquels on peut faire des SVM, par exemple Linéaire (sur \\(\\mathbb R^d\\)) : \\(K(x,x&#39;)=x^tx&#39;\\). Polynomial (sur \\(\\mathbb R^d\\)) : \\(K(x,x&#39;)=(x^tx&#39;+1)^d\\). Gaussien (Gaussian radial basis function ou RBF) (sur \\(\\mathbb R^d\\)) \\[K(x,x&#39;)=\\exp\\left(-\\frac{\\|x-x&#39;\\|}{2\\sigma^2}\\right).\\] Laplace (sur \\(\\mathbb R\\)) : \\(K(x,x&#39;)=\\exp(-\\gamma|x-x&#39;|)\\). Noyau min (sur \\(\\mathbb R^+\\)) : \\(K(x,x&#39;)=\\min(x,x&#39;)\\). … Bien entendu, en pratique tout le problème va consister à trouver le bon noyau ! On considère le jeu de données suivant où le problème est d’expliquer \\(Y\\) par \\(V1\\) et \\(V2\\). n &lt;- 500 set.seed(13) X &lt;- matrix(runif(n*2,-2,2),ncol=2) %&gt;% as.data.frame() Y &lt;- rep(0,n) cond &lt;- (X$V1^2+X$V2^2)&lt;=2.8 Y[cond] &lt;- rbinom(sum(cond),1,0.9) Y[!cond] &lt;- rbinom(sum(!cond),1,0.1) df &lt;- X %&gt;% mutate(Y=as.factor(Y)) ggplot(df)+aes(x=V2,y=V1,color=Y)+geom_point()+theme_classic() Ajuster une svm linéaire et visualiser l’hyperplan séparateur. Que remarquez-vous ? mod.svm0 &lt;- svm(Y~.,data=df,kernel=&quot;linear&quot;,cost=1) plot(mod.svm0,df,grid=250) La svm linéaire ne permet pas de séparer les groupes (on pouvait s’y attendre). Exécuter la commande suivante et commenter la sortie. mod.svm1 &lt;- svm(Y~.,data=df,kernel=&quot;radial&quot;,gamma=1,cost=1) plot(mod.svm1,df,grid=250) Le noyau radial permet de mettre en évidence une séparation non linéaire. Faire varier les paramètres gamma et cost. Interpréter (on pourra notamment étudier l’évolution du nombre de vecteurs supports en fonction du paramètre cost). mod.svm2 &lt;- svm(Y~.,data=df,kernel=&quot;radial&quot;,gamma=1,cost=0.0001) mod.svm3 &lt;- svm(Y~.,data=df,kernel=&quot;radial&quot;,gamma=1,cost=1) mod.svm4 &lt;- svm(Y~.,data=df,kernel=&quot;radial&quot;,gamma=1,cost=100000) plot(mod.svm2,df,grid=250) plot(mod.svm3,df,grid=250) plot(mod.svm4,df,grid=250) mod.svm2$nSV [1] 244 244 mod.svm3$nSV [1] 114 114 mod.svm4$nSV [1] 78 77 Le nombre de vecteurs supports diminue lorsque \\(C\\) augmente. Une forte valeur de \\(C\\) autorise moins d’observations à être dans la marge, elle a donc tendance à diminuer (risque de surapprentissage). Sélectionner automatiquement ces paramètres. On pourra utiliser la fonction tune en faisant varier C dans c(0.1,1,10,100,1000) et gamma dans c(0.5,1,2,3,4). set.seed(1234) tune.out &lt;- tune(svm,Y~.,data=df,kernel=&quot;radial&quot;, ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4))) summary(tune.out) Parameter tuning of &#39;svm&#39;: - sampling method: 10-fold cross validation - best parameters: cost gamma 10 0.5 - best performance: 0.108 - Detailed performance results: cost gamma error dispersion 1 1e-01 0.5 0.182 0.04565572 2 1e+00 0.5 0.148 0.03155243 3 1e+01 0.5 0.108 0.03425395 4 1e+02 0.5 0.116 0.03373096 5 1e+03 0.5 0.112 0.03425395 6 1e-01 1.0 0.184 0.04402020 7 1e+00 1.0 0.120 0.03651484 8 1e+01 1.0 0.120 0.03126944 9 1e+02 1.0 0.112 0.03155243 10 1e+03 1.0 0.120 0.03887301 11 1e-01 2.0 0.170 0.04136558 12 1e+00 2.0 0.124 0.02458545 13 1e+01 2.0 0.122 0.03457681 14 1e+02 2.0 0.124 0.03502380 15 1e+03 2.0 0.142 0.03705851 16 1e-01 3.0 0.160 0.03651484 17 1e+00 3.0 0.124 0.02458545 18 1e+01 3.0 0.126 0.03134042 19 1e+02 3.0 0.132 0.04022161 20 1e+03 3.0 0.166 0.03272783 21 1e-01 4.0 0.154 0.03777124 22 1e+00 4.0 0.124 0.02458545 23 1e+01 4.0 0.126 0.03134042 24 1e+02 4.0 0.138 0.04467164 25 1e+03 4.0 0.190 0.05754226 La sélection est faite en minimisant l’erreur de classification par validation croisée 10 blocs. Faire de même avec caret, on utilisera method=“svmRadial” et prob.model=TRUE. C &lt;- c(0.001,0.01,1,10,100,1000) sigma &lt;- c(0.5,1,2,3,4) gr &lt;- expand.grid(C=C,sigma=sigma) ctrl &lt;- trainControl(method=&quot;cv&quot;) res.caret1 &lt;- train(Y~.,data=df,method=&quot;svmRadial&quot;,trControl=ctrl,tuneGrid=gr,prob.model=TRUE) res.caret1 Support Vector Machines with Radial Basis Function Kernel 500 samples 2 predictor 2 classes: &#39;0&#39;, &#39;1&#39; No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 449, 450, 451, 450, 449, 450, ... Resampling results across tuning parameters: C sigma Accuracy Kappa 1e-03 0.5 0.8359976 0.6734429 1e-03 1.0 0.8439584 0.6890006 1e-03 2.0 0.8398359 0.6806352 1e-03 3.0 0.8578816 0.7164180 1e-03 4.0 0.8577623 0.7162786 1e-02 0.5 0.8400384 0.6813511 1e-02 1.0 0.8419584 0.6851382 1e-02 2.0 0.8458784 0.6926478 1e-02 3.0 0.8518407 0.7044624 1e-02 4.0 0.8577623 0.7162786 1e+00 0.5 0.8676871 0.7347434 1e+00 1.0 0.8857719 0.7713024 1e+00 2.0 0.8838127 0.7672737 1e+00 3.0 0.8798519 0.7594972 1e+00 4.0 0.8838928 0.7675507 1e+01 0.5 0.8798095 0.7596483 1e+01 1.0 0.8818111 0.7634823 1e+01 2.0 0.8838928 0.7676862 1e+01 3.0 0.8778503 0.7554248 1e+01 4.0 0.8720480 0.7438434 1e+02 0.5 0.8818111 0.7635668 1e+02 1.0 0.8839320 0.7677447 1e+02 2.0 0.8680480 0.7359331 1e+02 3.0 0.8517231 0.7031601 1e+02 4.0 0.8377591 0.6749363 1e+03 0.5 0.8760088 0.7521217 1e+03 1.0 0.8760496 0.7520939 1e+03 2.0 0.8498816 0.6998254 1e+03 3.0 0.8297967 0.6590033 1e+03 4.0 0.8100352 0.6192088 Accuracy was used to select the optimal model using the largest value. The final values used for the model were sigma = 1 and C = 1. On peut également répéter plusieurs fois la validation croisée pour stabiliser les résultats (on parallélise avec doParallel) : library(doParallel) ## pour paralléliser cl &lt;- makePSOCKcluster(4) registerDoParallel(cl) set.seed(12345) ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;,number=10,repeats=5) res.caret2 &lt;- train(Y~.,data=df,method=&quot;svmRadial&quot;,trControl=ctrl,tuneGrid=gr,prob.model=TRUE) on.exit(stopCluster(cl)) res.caret2 Support Vector Machines with Radial Basis Function Kernel 500 samples 2 predictor 2 classes: &#39;0&#39;, &#39;1&#39; No pre-processing Resampling: Cross-Validated (10 fold, repeated 5 times) Summary of sample sizes: 450, 449, 450, 451, 450, 449, ... Resampling results across tuning parameters: C sigma Accuracy Kappa 1e-03 0.5 0.8222641 0.6462662 1e-03 1.0 0.8458598 0.6928018 1e-03 2.0 0.8507000 0.7022920 1e-03 3.0 0.8555163 0.7118454 1e-03 4.0 0.8607898 0.7222353 1e-02 0.5 0.8242566 0.6502516 1e-02 1.0 0.8462519 0.6935931 1e-02 2.0 0.8499238 0.7007974 1e-02 3.0 0.8543078 0.7094622 1e-02 4.0 0.8639097 0.7284572 1e+00 0.5 0.8640626 0.7275460 1e+00 1.0 0.8839933 0.7678344 1e+00 2.0 0.8843858 0.7685479 1e+00 3.0 0.8823531 0.7644392 1e+00 4.0 0.8799766 0.7596759 1e+01 0.5 0.8848178 0.7696785 1e+01 1.0 0.8803851 0.7606211 1e+01 2.0 0.8775757 0.7549666 1e+01 3.0 0.8751989 0.7501291 1e+01 4.0 0.8727989 0.7453460 1e+02 0.5 0.8815531 0.7631204 1e+02 1.0 0.8751107 0.7501217 1e+02 2.0 0.8743443 0.7484731 1e+02 3.0 0.8615653 0.7229593 1e+02 4.0 0.8507228 0.7011391 1e+03 0.5 0.8803600 0.7607328 1e+03 1.0 0.8731277 0.7462531 1e+03 2.0 0.8499715 0.7000011 1e+03 3.0 0.8319834 0.6640949 1e+03 4.0 0.8089513 0.6174340 Accuracy was used to select the optimal model using the largest value. The final values used for the model were sigma = 0.5 and C = 10. Visualiser la règle sélectionnée. caret utilise la fonction ksvm du package kernlab. Ce package propose un choix plus large pour les noyaux. Par conséquent, si on souhaite visualiser la svm sélectionnée par caret, il est préférable d’utiliser cette fonction. library(kernlab) C.opt &lt;- res.caret2$bestTune$C sigma.opt &lt;- res.caret2$bestTune$sigma svm.sel &lt;- ksvm(Y~.,data=df,kernel=&quot;rbfdot&quot;,kpar=list(sigma=sigma.opt),C=C.opt) plot(svm.sel,data=df) 2.4 Exercices Exercice 2.1 (Résolution du problème d’optimisation dans le cas séparable) On considère \\(n\\) observations \\((x_1,y_1),\\dots,(x_n,y_n)\\) telles que \\((x_i,y_i)\\in\\mathbb R^p\\times\\{-1,1\\}\\). On cherche à expliquer la variable \\(Y\\) par \\(X\\). On considère l’algorithme SVM et on se place dans le cas où les données sont séparables. Soit \\(\\mathcal H\\) un hyperplan séparateur d’équation \\(\\langle w,x\\rangle+b=0\\) où \\(w\\in\\mathbb R^p,b\\in\\mathbb R\\). Exprimer la distance entre \\(x_i,i=1,\\dots,n\\) et \\(\\mathcal H\\) en fonction de \\(w\\) et \\(b\\). Soit \\(x_0\\in\\mathcal H\\). La solution correspond à la norme du projeté orthogonal de \\(x-x_0\\) sur \\(\\mathcal H\\), elle est donc colinéaire à \\(w\\) (car \\(w\\) est normal à \\(\\mathcal H\\)) et s’écrit \\[\\frac{\\langle x-x_0,w\\rangle}{\\|w\\|}w=\\frac{\\langle x,w\\rangle}{\\|w\\|}w-\\frac{\\langle x_0,w\\rangle}{\\|w\\|}w,\\] Comme \\(\\langle x_0,w\\rangle=-b\\), on déduit \\(d_{\\mathcal H}(x)=\\frac{|\\langle w,x\\rangle+b|}{\\|w\\|}=x^tw+b\\) si \\(\\|w\\|=1\\). Expliquer la logique du problème d’optimisation \\[\\max_{w,b,\\|w\\|=1}M\\] \\[\\textrm{sous les contraintes } y_i(\\langle w,x_i\\rangle+b)\\geq M,\\ i=1,\\dots,n.\\] L’approche consiste à choisir l’hyperplan : qui sépare les groupes ; tel que la distance entre les observations et lui soit maximale. Montrer que ce problème peut se réécrire \\[\\min_{w,b}\\frac{1}{2}\\|w\\|^2\\] \\[\\textrm{sous les contraintes } y_i(\\langle w,x_i\\rangle+b)\\geq 1,\\ i=1,\\dots,n.\\] Il suffit de poser comme contrainte \\(M=1/\\|w\\|\\). On rappelle que pour la minimisation d’une fonction \\(h:\\mathbb R^p\\to\\mathbb R\\) sous contraintes affines \\(g_i(u)\\geq 0,i=1,\\dots,n\\), le Lagrangien s’écrit \\[L(u,\\alpha)=h(u)-\\sum_{i=1}^n\\alpha_ig_i(u).\\] Si on désigne par \\(u_\\alpha=\\mathop{\\mathrm{argmin}}_uL(u,\\alpha)\\), la fonction duale est alors donnée par \\[\\theta(\\alpha)=L(u_\\alpha,\\alpha)=\\min_{u\\in\\mathbb R^p}L(u,\\alpha),\\] et le problème dual consiste à maximiser \\(\\theta(\\alpha)\\) sous les contraintes \\(\\alpha_i\\geq 0\\). En désignant par \\(\\alpha^\\star\\) la solution de ce problème, on déduit la solution du problème primal \\(u^\\star=u_{\\alpha^\\star}\\). Les conditions de Karush-Kuhn-Tucker sont données par \\(\\alpha_i^\\star\\geq 0\\). \\(g_i(u_{\\alpha^\\star})\\geq 0\\). \\(\\alpha_i^\\star g_i(u_{\\alpha^\\star})=0\\). Écrire le Lagrangien du problème considéré et en déduire une expression de \\(w\\) en fonction des \\(\\alpha_i\\) et des observations. Le lagrangien s’écrit \\[L(w,b;\\alpha)=\\frac{1}{2}\\|w\\|^2-\\sum_{i=1}^n\\alpha_i[y_i(x_i^tw+b)-1].\\] On a alors \\[\\frac{\\partial L(w,b;\\alpha)}{\\partial w}=w-\\sum_{i=1}^n\\alpha_iy_ix_i=0\\] et \\[\\frac{\\partial L(w,b;\\alpha)}{\\partial b}=-\\sum_{i=1}^n\\alpha_iy_i=0.\\] D’où \\(w_\\alpha=\\sum_{i=1}^n\\alpha_iy_ix_i\\). Écrire la fonction duale. La fonction duale s’écrit \\[\\begin{align*} \\theta(\\alpha)=L(w_\\alpha,b_\\alpha;\\alpha)= &amp;\\ \\frac{1}{2}\\langle \\sum_i\\alpha_iy_ix_i,\\sum_j\\alpha_jy_jx_j\\rangle-\\sum_i\\alpha_iy_i\\langle \\sum_j\\alpha_jy_jx_j,x_i\\rangle-\\sum_i\\alpha_iy_ib+\\sum_i\\alpha_i \\\\ = &amp;\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^tx_j \\end{align*}\\] Écrire les conditions KKT et en déduire les solutions \\(w^\\star\\) et \\(b^\\star\\). Les conditions KKT sont pour tout \\(i=1,\\dots,n\\) : \\[\\alpha_i^\\star\\geq 0 \\quad\\text{et}\\quad \\alpha_i^\\star[y_i(x_i^tw+b)-1]=0.\\] On obtient ainsi \\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\] et \\(b^\\star\\) en résolvant \\[\\alpha_i^\\star[y_i(x_i^tw+b)-1]=0\\] pour un \\(\\alpha_i^\\star\\) non nul. Interpréter les conditions KKT. Les \\(x_i\\) tels que \\(\\alpha_i^\\star&gt;0\\) vérifient \\[y_i(x_i^tw^\\star+b^\\star)=1.\\] Ils se situent donc sur la frontière définissant la marge maximale. Ce sont les vecteurs supports. Exercice 2.2 (Règle svm à partir de sorties R) On considère \\(n\\) observations \\((x_1,y_1),\\dots,(x_n,y_n)\\) telles que \\((x_i,y_i)\\in\\mathbb R^3\\times\\{-1,1\\}\\). On cherche à expliquer la variable \\(Y\\) par \\(X=(X_1,X_2,X_3)\\). On considère l’algorithme SVM et on se place dans le cas où les données sont séparables. On rappelle que cet algorithme consiste à chercher une droite d’équation \\(w^tx+b=0\\) où \\((w,b)\\in\\mathbb R^3\\times\\mathbb R\\) sont solutions du problème d’optimisation (problème primal) \\[\\min_{w,b}\\frac{1}{2}\\|w\\|^2\\] \\[\\textrm{sous les contraintes } y_i(w^tx_i+b)\\geq 1,\\ i=1,\\dots,n.\\] On désigne par \\(\\alpha_i^\\star,i=1,\\dots,n\\), les solutions du problème dual et par \\((w^\\star,b^\\star)\\) les solutions du problème ci-dessus. Donner la formule permettant de calculer \\(w^\\star\\) en fonction des \\(\\alpha_i^\\star\\). \\(w^\\star\\) se calcule selon \\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\] Les \\(\\alpha_i^\\star\\) étant nuls pour les vecteurs non supports, il suffit de sommer sur les vecteur supports. Expliquer comment on classe un nouveau point \\(x\\in\\mathbb R^3\\) par la méthode svm. Une fois \\(w^\\star\\) et \\(b^\\star\\) obtenus, la règle s’écrit \\[g(x)=1_{\\langle w^\\star,x\\rangle+b^\\star\\leq 0}-1_{\\langle w^\\star,x\\rangle+b^\\star&gt;0}.\\] Les données se trouvent dans un dataframe df. On exécute set.seed(1234) n &lt;- 100 X &lt;- data.frame(X1=runif(n),X2=runif(n),X3=runif(n)) X &lt;- data.frame(X1=scale(runif(n)),X2=scale(runif(n)),X3=scale(runif(n))) Y &lt;- rep(-1,100) Y[X[,1]&lt;X[,2]] &lt;- 1 #Y &lt;- (apply(X,1,sum)&lt;=0) %&gt;% as.numeric() %&gt;% as.factor() df &lt;- data.frame(X,Y=as.factor(Y)) mod.svm &lt;- svm(Y~.,data=df,kernel=&quot;linear&quot;,cost=10000000000) et on obtient df[mod.svm$index,] X1 X2 X3 Y 51 -1.1 -1.0 -1.0 1 92 0.7 0.8 1.1 1 31 0.7 0.5 -1.0 -1 37 -0.5 -0.6 0.3 -1 mod.svm$coefs [,1] [1,] 59 [2,] 49 [3,] -30 [4,] -79 mod.svm$rho [1] -0.5 Calculer les valeurs de \\(w^\\star\\) et \\(b^\\star\\). En déduire la règle de classification. \\(b^\\star\\) est l’opposé de mod.svm$rho. Pour \\(w^\\star\\) il suffit d’appliquer la formule et on trouve X1 X2 X3 -12.1 12.6 1.2 On dispose d’une nouvelle observation \\(x=(1,-0.5,-1)\\). Dans quel groupe (-1 ou 1) l’algorithme affecte cette nouvelle donnée ? On calcule la combinaison linéaire \\(\\langle w^\\star,x\\rangle+b^\\star\\) : newX &lt;- data.frame(X1=1,X2=-0.5,X3=-1) sum(w*newX)+b [1] -19.1 On affectera donc la nouvelle donnée au groupe -1. "],
["arbres.html", "Chapitre 3 Arbres 3.1 Coupures CART en fonction de la nature des variables 3.2 Élagage", " Chapitre 3 Arbres Les méthodes par arbres sont des algorithmes où la prévision s’effectue à partir de moyennes locales. Plus précisément, étant donné un échantillon \\((x_1,y_1)\\dots,(x_n,y_n)\\), l’approche consiste à : construire une partition de l’espace de variables explicatives (\\(\\mathbb R^p\\)) ; prédire la sortie d’une nouvelle observation \\(x\\) en faisant : la moyenne des \\(y_i\\) pour les \\(x_i\\) qui sont dans la même classe que \\(x\\) si on est en régression ; un vote à la majorité parmi les \\(y_i\\) tels que les \\(x_i\\) qui sont dans la même classe que \\(x\\) si on est en classification. Bien entendu toute la difficulté est de trouver la “bonne partition” pour le problème d’intérêt. Il existe un grand nombre d’algorithmes qui permettent de trouver une partition. Le plus connu est l’algorithme CART (Breiman et al. 1984) où la partition est construite par divisions successives au moyen d’hyperplan orthogonaux aux axes de \\(\\mathbb R^p\\). L’algorithme est récursif : il va à chaque étape séparer un groupe d’observations (nœuds) en deux groupes (nœuds fils) en cherchant la meilleure variable et le meilleur seuil de coupure. Ce choix s’effectue à partir d’un critère d’impureté : la meilleure coupure est celle pour laquelle l’impureté des 2 nœuds fils sera minimale. Nous étudions cet algorithme dans cette partie. 3.1 Coupures CART en fonction de la nature des variables Une partition CART s’obtient en séparant les observations en 2 selon une coupure parallèle aux axes puis en itérant ce procédé de séparation binaire sur les deux groupes… Par conséquent la première question à se poser est : pour un ensemble de données \\((x_1,y_1),\\dots,(x_n,y_n)\\) fixé, comment obtenir la meilleure coupure ? Comme souvent ce sont les données qui vont répondre à cette question. La sélection de la meilleur coupure s’effectue en introduisant une fonction d’impureté \\(\\mathcal I\\) qui va mesurer le degrés d’hétérogénéité d’un nœud \\(\\mathcal N\\). Cette fonction prendra de grandes valeurs pour les nœuds hétérogènes (les valeurs de \\(Y\\) diffèrent à l’intérieur du nœud) ; faibles valeurs pour les nœuds homogènes (les valeurs de \\(Y\\) sont proches à l’intérieur du nœud). On utilise souvent comme fonction d’impureté : la variance en régression \\[\\mathcal I(\\mathcal N)=\\frac{1}{|\\mathcal N|}\\sum_{i:x_i\\in\\mathcal N}(y_i-\\overline{y}_\\mathcal N)^2,\\] où \\(\\overline{y} _\\mathcal N\\) désigne la moyenne des \\(y_i\\) dans \\(\\mathcal N\\). l’impureté de Gini en classification binaire \\[\\mathcal I(\\mathcal N)=2p(\\mathcal N)(1-p(\\mathcal N))\\] où \\(p(\\mathcal N)\\) représente la proportion de 1 dans \\(\\mathcal N\\). Les coupures considérées par l’algorithme CART sont des hyperplans orthogonaux aux axes de \\(\\mathbb R^p\\), choisir une coupure revient donc à choisir une variable \\(j\\) parmi les \\(p\\) variables explicatives et un seuil \\(s\\) dans \\(\\mathbb R\\). On peut donc représenter une coupure par un couple \\((j,s)\\). Une fois l’impureté définie, on choisira la coupure \\((j,s)\\) qui maximise le gain d’impureté entre le noeud père et ses deux noeuds fils : \\[\\Delta(\\mathcal I)=\\mathbf P(\\mathcal N)\\mathcal I(\\mathcal N)-(\\mathbf P(\\mathcal N_1(j,s))\\mathcal I(\\mathcal N_1(j,s))+\\mathbf P(\\mathcal N_2(j,s))\\mathcal I(\\mathcal N_2(j,s))\\] où * \\(\\mathcal N_1(j,s)\\) et \\(\\mathcal N_2(j,s)\\) sont les 2 nœuds fils de \\(\\mathcal N\\) engendrés par la coupure \\((j,s)\\) ; * \\(\\mathbf P(\\mathcal N)\\) représente la proportion d’observations dans le nœud \\(\\mathcal N\\). 3.1.1 Arbres de régression On considère le jeu de données suivant où le problème est d’expliquer la variable quantitative \\(Y\\) par la variable quantitative \\(X\\). n &lt;- 50 set.seed(1234) X &lt;- runif(n) set.seed(5678) Y &lt;- 1*X*(X&lt;=0.6)+(-1*X+3.2)*(X&gt;0.6)+rnorm(n,sd=0.1) data1 &lt;- data.frame(X,Y) ggplot(data1)+aes(x=X,y=Y)+geom_point() A l’aide de la fonction rpart du package rpart, construire un arbre permettant d’expliquer \\(Y\\) par \\(X\\). library(rpart) tree &lt;- rpart(Y~X,data=data1) Visualiser l’arbre à l’aide des fonctions prp et rpart.plot du package rpart.plot. library(rpart.plot) prp(tree) rpart.plot(tree) Écrire l’estimateur associé à l’arbre. On a un modèle de régression \\[Y=m(X)+\\varepsilon\\] où la fonction de régression (inconnue) \\(m(x)\\) est estimée par \\[\\widehat m(x)=0.31\\, \\mathbf{1}_{x\\lt 0.58}+2.4\\,\\mathbf{1}_{x\\geq 0.58}.\\] Ajouter sur le graphe de la question 1 la partition définie par l’arbre ainsi que les valeurs prédites. On obtient une partition avec 2 nœuds terminaux. Cette partition peut être résumée par la question : “est-ce que \\(X\\) est plus petit que 0.58 ?”. df1 &lt;- data.frame(x=c(0,0.58),xend=c(0.58,1),y=c(0.31,2.41),yend=c(0.31,2.41)) ggplot(data1)+aes(x=X,y=Y)+geom_point()+geom_vline(xintercept = 0.58,size=1,color=&quot;blue&quot;)+ geom_segment(data=df1,aes(x=x,y=y,xend=xend,yend=yend),size=1,color=&quot;red&quot;) 3.1.2 Arbres de classification On considère les données suivantes où le problème est d’expliquer la variable binaire \\(Y\\) par deux variables quantitatives \\(X_1\\) et \\(X_2\\). n &lt;- 50 set.seed(12345) X1 &lt;- runif(n) set.seed(5678) X2 &lt;- runif(n) Y &lt;- rep(0,n) set.seed(54321) Y[X1&lt;=0.45] &lt;- rbinom(sum(X1&lt;=0.45),1,0.85) set.seed(52432) Y[X1&gt;0.45] &lt;- rbinom(sum(X1&gt;0.45),1,0.15) data2 &lt;- data.frame(X1,X2,Y) ggplot(data2)+aes(x=X1,y=X2,color=Y)+geom_point(size=2)+scale_x_continuous(name=&quot;&quot;)+ scale_y_continuous(name=&quot;&quot;)+theme_classic() Construire un arbre permettant d’expliquer \\(Y\\) par \\(X_1\\) et \\(X_2\\). Représenter l’arbre et identifier l’éventuel problème. tree &lt;- rpart(Y~.,data=data2) rpart.plot(tree) On observe que l’arbre construit est un arbre de régression, pas de classification. Cela vient du fait que \\(Y\\) est considérée par R comme une variable quantitative, il faut la convertir en facteur. data2$Y &lt;- as.factor(data2$Y) tree &lt;- rpart(Y~.,data=data2) rpart.plot(tree) Tout est OK maintenant ! Écrire la règle de classification ainsi que la fonction de score définies par l’arbre. La règle de classification est \\[\\widehat g(x)=\\mathbf{1}_{X_1\\lt 0.44}.\\] La fonction de score est donnée par \\[\\widehat S(x)=\\widehat P(Y=1|X=x)=0.83\\mathbf{1}_{X_1\\lt 0.44}+0.07\\mathbf{1}_{X_1\\geq 0.44}.\\] Ajouter sur le graphe de la question 1 la partition définie par l’arbre. ggplot(data2)+aes(x=X1,y=X2,color=Y,shape=Y)+geom_point(size=2)+ theme_classic()+geom_vline(xintercept = 0.44,size=1,color=&quot;blue&quot;) 3.1.3 Entrée qualitative On considère les données n &lt;- 100 X &lt;- factor(rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;),n)) set.seed(1234) Y[X==&quot;A&quot;] &lt;- rbinom(sum(X==&quot;A&quot;),1,0.9) Y[X==&quot;B&quot;] &lt;- rbinom(sum(X==&quot;B&quot;),1,0.25) Y[X==&quot;C&quot;] &lt;- rbinom(sum(X==&quot;C&quot;),1,0.8) Y[X==&quot;D&quot;] &lt;- rbinom(sum(X==&quot;D&quot;),1,0.2) Y &lt;- as.factor(Y) data3 &lt;- data.frame(X,Y) Construire un arbre permettant d’expliquer \\(Y\\) par \\(X\\). tree3 &lt;- rpart(Y~.,data=data3) rpart.plot(tree3) Expliquer la manière dont l’arbre est construit dans ce cadre là. La variable étant qualitative, on ne cherche pas un seuil de coupure pour diviser un nœud en 2. On va ici considérer toutes les partitions binaires de l’ensemble \\(\\{A,B,C,D\\}\\). La meilleure partition est \\(\\{\\{A,C\\},\\{B,D\\}\\}\\). 3.2 Élagage Le procédé de coupe présenté précédemment permet de définir un très grand nombre d’arbres à partir d’un jeu de données (arbre sans coupure, avec une coupure, deux coupures…). Se pose alors la question de trouver le meilleur arbre parmi tous les arbres possibles. Une première idée serait de choisir parmi tous les arbres possibles celui qui optimise un critère de performance. Cette approche, bien que cohérente, n’est généralement pas possible à mettre en œuvre en pratique car le nombre d’arbres à considérer est souvent trop important. La méthode CART propose une procédure permettant de choisir automatiquement un arbre en 3 étapes : On construit un arbre maximal (très profond) \\(\\mathcal T_{max}\\) ; On sélectionne une suite d’arbres emboités : \\[\\mathcal T_{max}=\\mathcal T_0\\supset\\mathcal T_1\\supset\\dots\\supset \\mathcal T_K.\\] La sélection s’effectue en optimisant un critère Cout/complexité qui permet de réguler le compromis entre ajustement et complexité de l’arbre. On sélectionne un arbre dans cette sous-suite en optimisant un critère de performance. Cette approche revient à choisir un sous-arbre de l’arbre \\(\\mathcal T_\\text{max}\\), c’est-à-dire à enlever des branches à \\(T_\\text{max}\\), c’est pourquoi on parle d’élagage. 3.2.1 Élagage pour un problème de régression On considère les données Carseats du package ISLR. library(ISLR) data(Carseats) summary(Carseats) Sales CompPrice Income Min. : 0.000 Min. : 77 Min. : 21.00 1st Qu.: 5.390 1st Qu.:115 1st Qu.: 42.75 Median : 7.490 Median :125 Median : 69.00 Mean : 7.496 Mean :125 Mean : 68.66 3rd Qu.: 9.320 3rd Qu.:135 3rd Qu.: 91.00 Max. :16.270 Max. :175 Max. :120.00 Advertising Population Price Min. : 0.000 Min. : 10.0 Min. : 24.0 1st Qu.: 0.000 1st Qu.:139.0 1st Qu.:100.0 Median : 5.000 Median :272.0 Median :117.0 Mean : 6.635 Mean :264.8 Mean :115.8 3rd Qu.:12.000 3rd Qu.:398.5 3rd Qu.:131.0 Max. :29.000 Max. :509.0 Max. :191.0 ShelveLoc Age Education Urban Bad : 96 Min. :25.00 Min. :10.0 No :118 Good : 85 1st Qu.:39.75 1st Qu.:12.0 Yes:282 Medium:219 Median :54.50 Median :14.0 Mean :53.32 Mean :13.9 3rd Qu.:66.00 3rd Qu.:16.0 Max. :80.00 Max. :18.0 US No :142 Yes:258 On cherche ici à expliquer la variable quantitative Sales par les autres variables. Construire un arbre permettant de répondre au problème. tree &lt;- rpart(Sales~.,data=Carseats) rpart.plot(tree) Expliquer les sorties de la fonction printcp appliquée à l’arbre de la question précédente et calculer le dernier terme de la colonne rel error. printcp(tree) Regression tree: rpart(formula = Sales ~ ., data = Carseats) Variables actually used in tree construction: [1] Advertising Age CompPrice Income [5] Population Price ShelveLoc Root node error: 3182.3/400 = 7.9557 n= 400 CP nsplit rel error xerror xstd 1 0.250510 0 1.00000 1.00492 0.069530 2 0.105073 1 0.74949 0.75877 0.051613 3 0.051121 2 0.64442 0.68283 0.046333 4 0.045671 3 0.59330 0.64240 0.043550 5 0.033592 4 0.54763 0.60051 0.041716 6 0.024063 5 0.51403 0.58903 0.039691 7 0.023948 6 0.48997 0.59472 0.039561 8 0.022163 7 0.46602 0.58972 0.039539 9 0.016043 8 0.44386 0.58329 0.039731 10 0.014027 9 0.42782 0.57392 0.038516 11 0.013145 11 0.39976 0.57780 0.038529 12 0.012711 12 0.38662 0.58719 0.038339 13 0.012147 13 0.37391 0.58970 0.038419 14 0.011888 14 0.36176 0.58850 0.038291 15 0.010778 15 0.34987 0.58673 0.038383 16 0.010506 16 0.33909 0.57818 0.038886 17 0.010000 17 0.32859 0.57320 0.038277 On peut lire des informations sur la suite d’arbres emboîtés, cette suite est de longueur 17 ici. Dans le dernier tableau, chaque ligne représente un arbre de la suite et on a dans les colonnes : CP : le paramètre de complexité, plus il est petit plus l’arbre est profond ; nsplit : nombre de coupures de l’arbre ; rel error contient l’erreur calculée sur les données d’apprentissage. Cette erreur décroit lorsque la complexité augmente et peut être interprétée comme une erreur d’ajustement ; xerror : contient l’erreur calculée par validation croisée. Elle peut être interprétée comme une erreur de prévision ; xstd correspond à l’écart type estimé de l’erreur. Les types d’erreurs dépendent du problème considéré. Vu qu’on est ici sur un problème de régression, c’est l’erreur quadratique moyenne qui est considérée. De plus ces erreurs sont normalisées par rapport à l’erreur de l’arbre racine (sans coupure). Ainsi on retrouve l’erreur demandée avec Carseats %&gt;% mutate(fitted=predict(tree)) %&gt;% summarise(MSE=mean((fitted-Sales)^2)/mean((Sales-mean(Sales))^2)) MSE 1 0.3285866 #mean((predict(tree)-Carseats$Sales)^2)/mean((Carseats$Sales-mean(Carseats$Sales))^2) Construire une suite d’arbres plus grandes en jouant sur les paramètres cp et minsplit de la fonction rpart. Il suffit de diminuer les valeurs par défaut de ces paramètres. set.seed(123) tree1 &lt;- rpart(Sales~.,data=Carseats,cp=0.00001,minsplit=2) printcp(tree1) Regression tree: rpart(formula = Sales ~ ., data = Carseats, cp = 1e-05, minsplit = 2) Variables actually used in tree construction: [1] Advertising Age CompPrice Education [5] Income Population Price ShelveLoc [9] Urban US Root node error: 3182.3/400 = 7.9557 n= 400 CP nsplit rel error xerror xstd 1 2.5051e-01 0 1.00000000 1.00632 0.069635 2 1.0507e-01 1 0.74948961 0.75985 0.051802 3 5.1121e-02 2 0.64441706 0.67592 0.044633 4 4.5671e-02 3 0.59329646 0.67720 0.043488 5 3.3592e-02 4 0.54762521 0.64640 0.043209 6 2.4063e-02 5 0.51403284 0.64674 0.041192 7 2.3948e-02 6 0.48997005 0.63825 0.041103 8 2.2163e-02 7 0.46602225 0.63234 0.040983 9 1.6043e-02 8 0.44385897 0.61886 0.039262 10 1.4027e-02 9 0.42781645 0.61392 0.039072 11 1.3145e-02 11 0.39976237 0.61848 0.039060 12 1.2711e-02 12 0.38661699 0.62070 0.039293 13 1.2147e-02 13 0.37390609 0.62158 0.039768 14 1.1888e-02 14 0.36175900 0.62655 0.039848 15 1.0778e-02 15 0.34987122 0.61565 0.039116 16 1.0506e-02 16 0.33909277 0.62089 0.039292 17 1.0301e-02 17 0.32858663 0.62477 0.039223 18 9.8052e-03 18 0.31828518 0.62074 0.039205 19 9.5324e-03 20 0.29867475 0.62340 0.039696 20 9.3098e-03 21 0.28914234 0.61989 0.039722 21 8.6039e-03 22 0.27983257 0.62566 0.040168 22 8.5728e-03 23 0.27122871 0.61686 0.039659 23 7.7737e-03 25 0.25408305 0.62076 0.040607 24 7.4353e-03 26 0.24630936 0.61125 0.040480 25 6.2838e-03 28 0.23143882 0.59597 0.039395 26 6.1242e-03 29 0.22515504 0.60994 0.040169 27 5.6953e-03 30 0.21903085 0.59896 0.039173 28 5.5687e-03 31 0.21333555 0.60151 0.039820 29 5.4134e-03 32 0.20776686 0.60041 0.039844 30 5.1373e-03 33 0.20235343 0.58408 0.039291 31 4.9581e-03 34 0.19721608 0.58631 0.039265 32 4.8270e-03 35 0.19225798 0.58969 0.039393 33 4.5558e-03 36 0.18743102 0.59070 0.039253 34 4.5456e-03 37 0.18287525 0.58833 0.038997 35 4.3739e-03 38 0.17832965 0.58757 0.038982 36 4.3307e-03 39 0.17395578 0.58716 0.038985 37 4.2485e-03 40 0.16962503 0.58706 0.039096 38 4.0980e-03 41 0.16537650 0.58472 0.039006 39 4.0525e-03 42 0.16127847 0.58935 0.039188 40 4.0054e-03 43 0.15722596 0.58756 0.038706 41 3.6917e-03 44 0.15322052 0.60472 0.039435 42 3.6352e-03 45 0.14952883 0.60179 0.039308 43 3.5301e-03 46 0.14589367 0.60395 0.039286 44 3.5196e-03 47 0.14236356 0.60402 0.039279 45 2.8653e-03 48 0.13884396 0.59319 0.038874 46 2.8565e-03 49 0.13597868 0.58540 0.039159 47 2.8565e-03 50 0.13312217 0.58540 0.039159 48 2.7253e-03 51 0.13026571 0.58760 0.039209 49 2.6841e-03 52 0.12754044 0.58585 0.038937 50 2.6829e-03 54 0.12217220 0.58743 0.038915 51 2.6660e-03 55 0.11948928 0.58794 0.038911 52 2.4588e-03 56 0.11682326 0.58713 0.038864 53 2.3693e-03 57 0.11436443 0.57598 0.038151 54 2.3018e-03 58 0.11199508 0.57746 0.038203 55 2.2746e-03 60 0.10739152 0.58523 0.039585 56 2.2540e-03 61 0.10511688 0.58489 0.039595 57 2.1781e-03 62 0.10286290 0.58466 0.039588 58 2.1645e-03 63 0.10068483 0.58575 0.039509 59 2.0950e-03 64 0.09852033 0.58152 0.039361 60 2.0945e-03 65 0.09642538 0.58236 0.039446 61 2.0740e-03 66 0.09433084 0.58431 0.039597 62 1.8864e-03 67 0.09225680 0.57892 0.039320 63 1.8413e-03 68 0.09037038 0.58456 0.039520 64 1.7921e-03 69 0.08852905 0.58578 0.040068 65 1.7167e-03 70 0.08673697 0.58533 0.039995 66 1.6766e-03 71 0.08502031 0.58336 0.039558 67 1.6704e-03 72 0.08334367 0.58559 0.039491 68 1.6064e-03 73 0.08167332 0.58367 0.039470 69 1.6055e-03 74 0.08006697 0.58229 0.039276 70 1.5103e-03 75 0.07846149 0.58881 0.039911 71 1.4967e-03 76 0.07695120 0.58862 0.039908 72 1.4907e-03 77 0.07545453 0.59042 0.040029 73 1.4007e-03 78 0.07396387 0.60029 0.040368 74 1.4002e-03 79 0.07256317 0.60033 0.040358 75 1.3613e-03 80 0.07116301 0.60705 0.040742 76 1.3589e-03 81 0.06980172 0.61439 0.041431 77 1.3462e-03 82 0.06844282 0.61457 0.041431 78 1.3351e-03 83 0.06709659 0.61405 0.041405 79 1.3304e-03 84 0.06576144 0.61487 0.041409 80 1.3146e-03 85 0.06443102 0.61487 0.041409 81 1.2795e-03 86 0.06311644 0.61217 0.041323 82 1.2412e-03 87 0.06183696 0.61153 0.041238 83 1.2373e-03 88 0.06059575 0.61610 0.041280 84 1.2135e-03 89 0.05935843 0.61519 0.041307 85 1.2002e-03 91 0.05693148 0.61097 0.041152 86 1.1269e-03 92 0.05573126 0.61178 0.041184 87 1.0919e-03 93 0.05460435 0.60862 0.041145 88 1.0898e-03 94 0.05351243 0.60925 0.041145 89 1.0864e-03 95 0.05242260 0.60925 0.041145 90 1.0646e-03 96 0.05133621 0.60693 0.041083 91 1.0116e-03 97 0.05027156 0.60260 0.040185 92 9.5940e-04 98 0.04925996 0.60122 0.040328 93 8.9105e-04 99 0.04830056 0.60234 0.040289 94 8.8465e-04 100 0.04740951 0.60105 0.040619 95 8.7611e-04 101 0.04652486 0.60055 0.040661 96 8.5644e-04 102 0.04564875 0.60111 0.040661 97 8.4568e-04 103 0.04479231 0.60150 0.040657 98 8.3004e-04 104 0.04394663 0.60427 0.040867 99 8.0748e-04 105 0.04311659 0.60590 0.040864 100 7.9944e-04 106 0.04230912 0.60627 0.040864 101 7.5680e-04 107 0.04150968 0.61302 0.041688 102 7.4082e-04 108 0.04075288 0.61164 0.041485 103 7.4043e-04 109 0.04001206 0.61183 0.041479 104 7.3510e-04 110 0.03927163 0.61163 0.041483 105 7.0107e-04 111 0.03853653 0.61182 0.041563 106 6.9184e-04 112 0.03783546 0.60947 0.041716 107 6.7585e-04 113 0.03714362 0.60947 0.041716 108 6.7373e-04 114 0.03646776 0.60689 0.041731 109 6.7173e-04 115 0.03579403 0.60689 0.041731 110 6.6783e-04 116 0.03512230 0.60591 0.041707 111 6.6518e-04 117 0.03445448 0.60613 0.041700 112 6.6451e-04 118 0.03378929 0.60613 0.041700 113 6.0900e-04 119 0.03312478 0.60732 0.041781 114 6.0343e-04 120 0.03251578 0.61077 0.042036 115 5.9465e-04 121 0.03191235 0.61269 0.042102 116 5.8550e-04 123 0.03072304 0.61208 0.042081 117 5.8340e-04 124 0.03013754 0.61187 0.042084 118 5.6972e-04 125 0.02955414 0.61258 0.042084 119 5.6433e-04 126 0.02898442 0.61258 0.042084 120 5.6323e-04 127 0.02842009 0.61228 0.042075 121 5.4821e-04 128 0.02785686 0.60943 0.042014 122 5.4339e-04 131 0.02621222 0.60980 0.042021 123 5.1968e-04 132 0.02566882 0.60983 0.042171 124 5.0869e-04 133 0.02514915 0.60906 0.042168 125 5.0157e-04 134 0.02464045 0.60828 0.042297 126 4.7302e-04 135 0.02413889 0.61003 0.042276 127 4.6969e-04 136 0.02366587 0.60911 0.042228 128 4.6775e-04 137 0.02319618 0.61118 0.042218 129 4.6669e-04 138 0.02272842 0.61118 0.042218 130 4.5761e-04 139 0.02226174 0.60991 0.042232 131 4.5283e-04 140 0.02180413 0.60956 0.042235 132 4.5270e-04 141 0.02135130 0.61176 0.042356 133 4.5251e-04 142 0.02089861 0.61176 0.042356 134 4.4875e-04 143 0.02044610 0.61176 0.042356 135 4.4874e-04 144 0.01999735 0.61164 0.042360 136 4.4666e-04 145 0.01954861 0.61164 0.042360 137 4.3805e-04 146 0.01910194 0.61410 0.042468 138 4.2159e-04 147 0.01866389 0.61468 0.042470 139 4.1179e-04 148 0.01824230 0.61626 0.042531 140 3.8646e-04 149 0.01783051 0.61657 0.042546 141 3.6959e-04 150 0.01744404 0.61985 0.042911 142 3.3035e-04 151 0.01707446 0.62146 0.043372 143 3.0799e-04 152 0.01674411 0.62258 0.043333 144 3.0672e-04 153 0.01643612 0.62274 0.043330 145 3.0672e-04 154 0.01612940 0.62274 0.043330 146 3.0672e-04 155 0.01582268 0.62274 0.043330 147 3.0544e-04 156 0.01551596 0.62274 0.043330 148 3.0094e-04 157 0.01521052 0.62395 0.043352 149 2.9757e-04 158 0.01490958 0.62467 0.043363 150 2.8981e-04 159 0.01461201 0.62274 0.043380 151 2.8923e-04 160 0.01432220 0.62270 0.043354 152 2.8782e-04 161 0.01403296 0.62400 0.043499 153 2.8635e-04 162 0.01374515 0.62400 0.043499 154 2.8189e-04 163 0.01345879 0.62219 0.043487 155 2.8173e-04 164 0.01317690 0.62253 0.043478 156 2.6988e-04 165 0.01289517 0.62531 0.043675 157 2.6283e-04 166 0.01262530 0.62452 0.043671 158 2.5737e-04 167 0.01236246 0.62258 0.043382 159 2.5139e-04 168 0.01210509 0.62028 0.043359 160 2.5003e-04 169 0.01185370 0.61871 0.043243 161 2.3771e-04 170 0.01160367 0.61747 0.043178 162 2.3512e-04 171 0.01136596 0.61853 0.043182 163 2.2600e-04 172 0.01113084 0.61800 0.043165 164 2.1796e-04 173 0.01090483 0.61542 0.043149 165 2.1590e-04 174 0.01068688 0.61466 0.043133 166 2.1121e-04 175 0.01047098 0.61339 0.043099 167 2.0973e-04 176 0.01025977 0.61238 0.043036 168 2.0949e-04 178 0.00984031 0.61238 0.043036 169 2.0779e-04 179 0.00963081 0.61220 0.043040 170 2.0120e-04 180 0.00942302 0.61280 0.043025 171 2.0025e-04 181 0.00922182 0.61280 0.043025 172 1.9247e-04 182 0.00902157 0.61353 0.043060 173 1.8668e-04 183 0.00882910 0.61383 0.043085 174 1.7976e-04 184 0.00864242 0.61349 0.043056 175 1.6630e-04 185 0.00846266 0.61532 0.043131 176 1.6596e-04 186 0.00829637 0.61615 0.043142 177 1.6594e-04 187 0.00813041 0.61615 0.043142 178 1.6347e-04 188 0.00796447 0.61623 0.043140 179 1.6290e-04 189 0.00780100 0.61623 0.043140 180 1.5712e-04 190 0.00763810 0.61644 0.043133 181 1.5619e-04 191 0.00748098 0.61562 0.043119 182 1.5210e-04 192 0.00732479 0.61504 0.043100 183 1.4745e-04 193 0.00717270 0.61484 0.043106 184 1.4354e-04 194 0.00702525 0.61434 0.043095 185 1.3883e-04 195 0.00688171 0.61338 0.043090 186 1.3883e-04 196 0.00674288 0.61357 0.043102 187 1.3613e-04 197 0.00660405 0.61349 0.043104 188 1.3589e-04 198 0.00646792 0.61374 0.043123 189 1.3299e-04 199 0.00633203 0.61225 0.043031 190 1.3241e-04 200 0.00619904 0.61244 0.043040 191 1.3011e-04 201 0.00606664 0.61182 0.043038 192 1.2674e-04 202 0.00593652 0.61207 0.043052 193 1.2674e-04 203 0.00580978 0.61250 0.043040 194 1.2167e-04 204 0.00568304 0.61345 0.043093 195 1.2167e-04 205 0.00556136 0.61264 0.043092 196 1.2105e-04 206 0.00543969 0.61264 0.043092 197 1.1352e-04 207 0.00531864 0.61255 0.043081 198 1.0898e-04 208 0.00520512 0.61236 0.043083 199 1.0860e-04 209 0.00509614 0.61259 0.043076 200 1.0592e-04 210 0.00498754 0.61259 0.043076 201 1.0265e-04 211 0.00488162 0.61474 0.043307 202 9.6794e-05 212 0.00477896 0.61439 0.043163 203 9.5532e-05 213 0.00468217 0.61428 0.043175 204 9.4042e-05 214 0.00458664 0.61450 0.043168 205 9.1257e-05 215 0.00449260 0.61509 0.043172 206 9.0753e-05 216 0.00440134 0.61548 0.043188 207 8.9624e-05 217 0.00431059 0.61573 0.043180 208 8.8270e-05 218 0.00422096 0.61566 0.043182 209 8.7486e-05 219 0.00413269 0.61545 0.043188 210 8.3729e-05 220 0.00404521 0.61466 0.043180 211 8.1451e-05 221 0.00396148 0.61426 0.043159 212 7.9204e-05 222 0.00388003 0.61366 0.043149 213 7.7471e-05 224 0.00372162 0.61346 0.043143 214 7.6989e-05 225 0.00364415 0.61346 0.043143 215 7.4805e-05 227 0.00349017 0.61288 0.043144 216 7.2925e-05 228 0.00341536 0.61356 0.043182 217 7.2160e-05 229 0.00334244 0.61239 0.043184 218 7.1694e-05 230 0.00327028 0.61239 0.043184 219 6.9264e-05 231 0.00319859 0.61315 0.043444 220 6.8065e-05 232 0.00312932 0.61316 0.043445 221 6.8065e-05 233 0.00306126 0.61363 0.043442 222 6.7977e-05 234 0.00299319 0.61363 0.043442 223 6.6383e-05 235 0.00292522 0.61341 0.043441 224 6.6383e-05 236 0.00285883 0.61341 0.043441 225 6.6383e-05 237 0.00279245 0.61341 0.043441 226 6.6203e-05 238 0.00272607 0.61341 0.043441 227 6.5697e-05 239 0.00265986 0.61341 0.043441 228 6.5373e-05 240 0.00259417 0.61341 0.043441 229 6.4356e-05 241 0.00252879 0.61327 0.043444 230 6.3372e-05 242 0.00246444 0.61244 0.043314 231 6.2228e-05 243 0.00240107 0.61268 0.043307 232 6.2225e-05 244 0.00233884 0.61268 0.043307 233 6.0397e-05 245 0.00227661 0.61266 0.043308 234 5.8464e-05 246 0.00221622 0.61302 0.043300 235 5.8137e-05 248 0.00209929 0.61288 0.043302 236 5.4694e-05 249 0.00204115 0.61454 0.043304 237 5.2855e-05 251 0.00193176 0.61433 0.043357 238 5.1331e-05 252 0.00187891 0.61341 0.043318 239 5.1048e-05 253 0.00182758 0.61222 0.043262 240 4.9324e-05 255 0.00172548 0.61222 0.043262 241 4.9278e-05 256 0.00167616 0.61214 0.043261 242 4.9278e-05 257 0.00162688 0.61214 0.043261 243 4.9273e-05 258 0.00157760 0.61214 0.043261 244 4.5298e-05 259 0.00152833 0.61225 0.043257 245 4.3577e-05 260 0.00148303 0.61182 0.043250 246 4.3370e-05 261 0.00143945 0.61153 0.043256 247 4.2422e-05 262 0.00139608 0.61153 0.043256 248 4.0867e-05 263 0.00135366 0.61087 0.043246 249 3.9280e-05 264 0.00131279 0.61059 0.043234 250 3.7840e-05 265 0.00127351 0.61039 0.043239 251 3.7840e-05 266 0.00123567 0.61013 0.043231 252 3.7840e-05 267 0.00119783 0.61013 0.043231 253 3.6955e-05 268 0.00115999 0.61040 0.043226 254 3.5847e-05 269 0.00112304 0.61040 0.043226 255 3.5216e-05 270 0.00108719 0.61040 0.043226 256 3.4708e-05 271 0.00105197 0.61069 0.043225 257 3.4032e-05 272 0.00101727 0.61083 0.043221 258 3.3519e-05 273 0.00098323 0.61077 0.043223 259 3.3247e-05 274 0.00094971 0.61077 0.043223 260 2.9981e-05 275 0.00091647 0.61135 0.043216 261 2.9052e-05 276 0.00088649 0.61163 0.043224 262 2.7245e-05 277 0.00085744 0.61167 0.043189 263 2.5663e-05 278 0.00083019 0.61090 0.043097 264 2.5663e-05 279 0.00080453 0.61090 0.043097 265 2.2814e-05 280 0.00077886 0.61105 0.043094 266 2.2688e-05 281 0.00075605 0.61153 0.043120 267 2.2128e-05 282 0.00073336 0.61091 0.043128 268 2.1877e-05 283 0.00071123 0.61118 0.043154 269 2.1510e-05 284 0.00068936 0.61118 0.043154 270 2.0132e-05 285 0.00066785 0.61155 0.043169 271 2.0132e-05 286 0.00064772 0.61209 0.043176 272 1.8231e-05 287 0.00062758 0.61222 0.043173 273 1.8163e-05 288 0.00060935 0.61289 0.043189 274 1.7618e-05 289 0.00059119 0.61289 0.043189 275 1.7618e-05 290 0.00057357 0.61289 0.043189 276 1.7608e-05 291 0.00055595 0.61289 0.043189 277 1.7110e-05 292 0.00053834 0.61261 0.043152 278 1.5272e-05 293 0.00052123 0.61323 0.043154 279 1.5099e-05 294 0.00050596 0.61262 0.043101 280 1.4162e-05 296 0.00047576 0.61252 0.043100 281 1.4162e-05 297 0.00046160 0.61237 0.043096 282 1.4141e-05 298 0.00044744 0.61237 0.043096 283 1.4141e-05 300 0.00041916 0.61237 0.043096 284 1.3214e-05 301 0.00040502 0.61212 0.043066 285 1.3214e-05 302 0.00039180 0.61186 0.043063 286 1.3093e-05 303 0.00037859 0.61186 0.043063 287 1.2318e-05 304 0.00036550 0.61172 0.043054 288 1.2318e-05 305 0.00035318 0.61141 0.043044 289 1.1454e-05 306 0.00034086 0.61140 0.043044 290 1.1082e-05 307 0.00032941 0.61149 0.043050 291 1.0621e-05 308 0.00031832 0.61200 0.043160 292 1.0000e-05 312 0.00027584 0.61200 0.043160 On obtient ici une suite de près de 300 arbres. On remarque que l’erreur d’ajustement ne cesse de décroitre, ceci est logique vu le procédé de construction : on ajuste de mieux en mieux lorsqu’on augmente le nombre de coupures ; l’erreur de prévision décroit avant de d’augmenter à nouveau. C’est le phénomène bien connu du sur-apprentissage. Expliquer la sortie de la fonction plotcp appliquée à l’arbre de la question précédente. plotcp(tree1) On obtient un graphe qui permet de visualiser l’erreur quadratique calculée par validation croisée (erreur de prévision) en fonction du paramètre cp ou nsplit. Sélectionner le “meilleur” arbre dans la suite construite. La manière classique revient à choisir l’arbre qui a la plus petite erreur de prévision. Cela revient à aller chercher dans le tableau de la fonction printcp l’arbre qiu possède la plus petite erreur de prévision. On peut obtenir la valeur optimale de cp avec cp_opt &lt;- tree1$cptable %&gt;% as.data.frame() %&gt;% filter(xerror==min(xerror)) %&gt;% dplyr::select(CP) %&gt;% as.numeric() cp_opt [1] 0.002369349 Visualiser l’arbre choisi (utiliser la fonction prune). tree_opt &lt;- prune(tree,cp=cp_opt) rpart.plot(tree_opt) La fonction visTree du package visNetwork permet de donner une visualisation interactive de l’arbre. library(visNetwork) visTree(tree_opt) Une application Shiny est également proposée pour visualiser les arbres visTreeEditor(Carseats) On souhaite prédire les valeurs de \\(Y\\) pour de nouveaux individus à partir de l’arbre sélectionné. Pour simplifier on considèrera ces 4 individus : new_ind &lt;- Carseats %&gt;% slice(3,58,185,218) %&gt;% select(-Sales) new_ind CompPrice Income Advertising Population Price ShelveLoc 3 113 35 10 269 80 Medium 58 93 91 0 22 117 Bad 185 132 33 7 35 97 Medium 218 106 44 0 481 111 Medium Age Education Urban US 3 59 12 Yes Yes 58 75 11 Yes No 185 60 11 No Yes 218 70 14 No No Calculer les valeurs prédites. predict(tree_opt,newdata=new_ind) 3 58 185 218 7.590278 3.767200 7.590278 6.626512 Séparer les données en un échantillon d’apprentissage de taille 250 et un échantillon test de taille 150. n.train &lt;- 250 set.seed(1234) perm &lt;- sample(nrow(Carseats)) train &lt;- Carseats[perm[1:n.train],] test &lt;- Carseats[-perm[1:n.train],] On considère la suite d’arbres définie par set.seed(4321) tree &lt;- rpart(Sales~.,data=train,cp=0.000001,minsplit=2) Dans cette suite, sélectionner un arbre très simple (avec 2 ou 3 coupures) un arbre très grand l’arbre optimal (avec la procédure d’élagage classique). printcp(tree) Regression tree: rpart(formula = Sales ~ ., data = train, cp = 1e-06, minsplit = 2) Variables actually used in tree construction: [1] Advertising Age CompPrice Education [5] Income Population Price ShelveLoc [9] Urban US Root node error: 1930.1/250 = 7.7203 n= 250 CP nsplit rel error xerror xstd 1 2.1456e-01 0 1.0000e+00 1.00199 0.090218 2 9.9792e-02 1 7.8544e-01 0.86595 0.075513 3 5.5822e-02 2 6.8565e-01 0.82348 0.071817 4 5.5012e-02 3 6.2983e-01 0.76327 0.065209 5 4.7593e-02 4 5.7481e-01 0.75011 0.065969 6 3.2780e-02 5 5.2722e-01 0.69666 0.062659 7 3.2081e-02 6 4.9444e-01 0.71690 0.062718 8 2.8747e-02 7 4.6236e-01 0.71639 0.062790 9 2.7988e-02 8 4.3361e-01 0.69569 0.059348 10 1.8568e-02 9 4.0563e-01 0.70266 0.064934 11 1.8305e-02 10 3.8706e-01 0.75844 0.071881 12 1.7705e-02 11 3.6875e-01 0.75332 0.071355 13 1.6028e-02 12 3.5105e-01 0.77667 0.075496 14 1.4152e-02 13 3.3502e-01 0.76655 0.074752 15 1.4119e-02 14 3.2087e-01 0.80299 0.082942 16 1.1545e-02 15 3.0675e-01 0.82136 0.082988 17 1.1033e-02 16 2.9520e-01 0.80368 0.081778 18 1.0407e-02 17 2.8417e-01 0.80205 0.081219 19 9.6380e-03 18 2.7377e-01 0.80105 0.081149 20 9.4448e-03 19 2.6413e-01 0.79823 0.081147 21 9.2825e-03 21 2.4524e-01 0.79320 0.081143 22 8.7958e-03 22 2.3596e-01 0.76562 0.078130 23 8.7574e-03 23 2.2716e-01 0.76141 0.078054 24 7.9616e-03 24 2.1840e-01 0.76444 0.077936 25 7.0728e-03 25 2.1044e-01 0.77626 0.077783 26 7.0288e-03 27 1.9630e-01 0.77706 0.078603 27 6.7205e-03 28 1.8927e-01 0.77012 0.077833 28 6.5421e-03 29 1.8255e-01 0.76052 0.075687 29 6.4728e-03 30 1.7600e-01 0.77588 0.075711 30 5.7670e-03 31 1.6953e-01 0.77776 0.075870 31 5.0693e-03 32 1.6376e-01 0.79694 0.077306 32 4.9069e-03 34 1.5363e-01 0.78951 0.077248 33 4.7845e-03 35 1.4872e-01 0.78954 0.077227 34 4.7623e-03 36 1.4393e-01 0.79479 0.078655 35 4.7423e-03 38 1.3441e-01 0.79479 0.078655 36 4.3579e-03 39 1.2967e-01 0.78334 0.078557 37 4.3530e-03 40 1.2531e-01 0.78187 0.078607 38 4.1413e-03 41 1.2096e-01 0.78700 0.078606 39 4.0455e-03 42 1.1681e-01 0.78351 0.078553 40 3.9302e-03 43 1.1277e-01 0.76858 0.075026 41 3.8957e-03 45 1.0491e-01 0.76858 0.075026 42 3.8803e-03 46 1.0101e-01 0.77054 0.075020 43 3.8596e-03 47 9.7133e-02 0.77054 0.075020 44 3.5520e-03 48 9.3273e-02 0.76480 0.074222 45 3.5181e-03 49 8.9721e-02 0.75251 0.074226 46 3.4216e-03 50 8.6203e-02 0.75336 0.074228 47 3.1866e-03 51 8.2782e-02 0.75473 0.074308 48 3.1193e-03 52 7.9595e-02 0.75621 0.074297 49 3.0949e-03 53 7.6476e-02 0.75747 0.074346 50 2.8538e-03 54 7.3381e-02 0.76454 0.074387 51 2.7245e-03 55 7.0527e-02 0.75920 0.074369 52 2.6778e-03 56 6.7802e-02 0.75964 0.074366 53 2.2840e-03 57 6.5125e-02 0.75580 0.074519 54 2.1373e-03 58 6.2841e-02 0.74584 0.074550 55 2.1338e-03 59 6.0703e-02 0.73895 0.074268 56 1.9958e-03 60 5.8570e-02 0.73653 0.074241 57 1.9324e-03 61 5.6574e-02 0.73663 0.073524 58 1.8577e-03 62 5.4641e-02 0.73454 0.073556 59 1.7446e-03 63 5.2784e-02 0.73777 0.073803 60 1.7300e-03 64 5.1039e-02 0.73668 0.073751 61 1.7199e-03 65 4.9309e-02 0.73848 0.073725 62 1.6642e-03 67 4.5869e-02 0.73425 0.073646 63 1.5818e-03 68 4.4205e-02 0.72999 0.073374 64 1.4176e-03 69 4.2623e-02 0.74065 0.074144 65 1.2535e-03 70 4.1205e-02 0.73915 0.074086 66 1.2528e-03 71 3.9952e-02 0.73727 0.074038 67 1.2241e-03 72 3.8699e-02 0.73623 0.074052 68 1.1710e-03 73 3.7475e-02 0.74071 0.074539 69 1.0861e-03 74 3.6304e-02 0.73962 0.074496 70 1.0751e-03 75 3.5218e-02 0.73945 0.074716 71 1.0619e-03 76 3.4143e-02 0.73833 0.074740 72 1.0396e-03 77 3.3081e-02 0.73833 0.074740 73 1.0031e-03 78 3.2041e-02 0.73390 0.073412 74 9.8653e-04 79 3.1038e-02 0.73390 0.073412 75 9.7982e-04 80 3.0052e-02 0.73362 0.073421 76 9.6068e-04 81 2.9072e-02 0.73336 0.073431 77 9.0157e-04 82 2.8111e-02 0.73346 0.073239 78 8.5140e-04 83 2.7210e-02 0.74355 0.073608 79 8.2985e-04 84 2.6358e-02 0.74289 0.073577 80 7.6578e-04 85 2.5529e-02 0.75270 0.074401 81 7.5439e-04 87 2.3997e-02 0.75068 0.071516 82 7.1170e-04 88 2.3243e-02 0.74411 0.071166 83 6.8926e-04 89 2.2531e-02 0.74202 0.071059 84 6.7218e-04 90 2.1842e-02 0.73282 0.070730 85 6.7025e-04 91 2.1169e-02 0.73262 0.070738 86 6.6158e-04 92 2.0499e-02 0.73262 0.070738 87 6.3418e-04 93 1.9838e-02 0.73337 0.070711 88 5.8450e-04 94 1.9203e-02 0.73500 0.071237 89 5.6893e-04 95 1.8619e-02 0.73223 0.070717 90 5.5273e-04 96 1.8050e-02 0.73659 0.070702 91 5.4467e-04 97 1.7497e-02 0.73718 0.070776 92 5.4467e-04 98 1.6953e-02 0.73718 0.070776 93 5.3718e-04 99 1.6408e-02 0.73718 0.070776 94 5.2351e-04 100 1.5871e-02 0.73718 0.070776 95 5.0781e-04 101 1.5347e-02 0.73857 0.071077 96 4.9273e-04 102 1.4839e-02 0.73897 0.071303 97 4.7591e-04 103 1.4347e-02 0.73723 0.071291 98 4.7283e-04 104 1.3871e-02 0.73289 0.071245 99 4.7167e-04 105 1.3398e-02 0.73289 0.071245 100 4.6792e-04 106 1.2926e-02 0.73367 0.071229 101 4.4890e-04 107 1.2458e-02 0.73271 0.071251 102 4.4497e-04 108 1.2009e-02 0.73500 0.071336 103 4.3662e-04 109 1.1565e-02 0.73626 0.071314 104 4.0289e-04 110 1.1128e-02 0.73827 0.071318 105 3.9917e-04 111 1.0725e-02 0.73714 0.071225 106 3.5812e-04 112 1.0326e-02 0.73757 0.071280 107 3.4136e-04 113 9.9677e-03 0.73935 0.071322 108 3.4092e-04 114 9.6264e-03 0.73935 0.071322 109 3.1735e-04 115 9.2854e-03 0.73935 0.071322 110 2.9187e-04 116 8.9681e-03 0.74282 0.072752 111 2.8919e-04 117 8.6762e-03 0.74209 0.072720 112 2.6331e-04 118 8.3870e-03 0.74465 0.072682 113 2.5547e-04 119 8.1237e-03 0.74431 0.072695 114 2.5495e-04 120 7.8683e-03 0.74576 0.072656 115 2.3380e-04 121 7.6133e-03 0.74654 0.072644 116 2.2890e-04 122 7.3795e-03 0.74668 0.072724 117 2.2642e-04 123 7.1506e-03 0.74690 0.072727 118 2.0984e-04 124 6.9242e-03 0.74265 0.072544 119 2.0200e-04 125 6.7143e-03 0.74425 0.072512 120 1.9608e-04 126 6.5123e-03 0.74161 0.072178 121 1.8717e-04 127 6.3163e-03 0.74511 0.072464 122 1.8717e-04 128 6.1291e-03 0.74799 0.072449 123 1.8580e-04 129 5.9419e-03 0.74770 0.072459 124 1.8476e-04 130 5.7561e-03 0.74770 0.072459 125 1.7793e-04 131 5.5714e-03 0.74719 0.072478 126 1.7444e-04 132 5.3934e-03 0.75014 0.072719 127 1.7244e-04 133 5.2190e-03 0.75150 0.072759 128 1.5738e-04 134 5.0466e-03 0.75072 0.072755 129 1.5505e-04 135 4.8892e-03 0.75222 0.072736 130 1.5275e-04 136 4.7341e-03 0.75193 0.072741 131 1.4819e-04 137 4.5814e-03 0.75415 0.072836 132 1.4370e-04 138 4.4332e-03 0.75422 0.072833 133 1.4148e-04 139 4.2895e-03 0.75413 0.072836 134 1.3805e-04 140 4.1480e-03 0.75413 0.072836 135 1.3430e-04 141 4.0100e-03 0.74921 0.071582 136 1.2773e-04 142 3.8757e-03 0.74657 0.071549 137 1.2334e-04 143 3.7479e-03 0.74713 0.071529 138 1.2312e-04 144 3.6246e-03 0.74737 0.071512 139 1.2024e-04 145 3.5015e-03 0.74737 0.071512 140 1.1979e-04 146 3.3812e-03 0.74737 0.071512 141 1.1629e-04 147 3.2615e-03 0.74785 0.071532 142 1.1285e-04 148 3.1452e-03 0.74861 0.071529 143 1.1222e-04 149 3.0323e-03 0.74823 0.071534 144 1.1026e-04 150 2.9201e-03 0.74823 0.071534 145 1.0832e-04 151 2.8098e-03 0.74823 0.071534 146 1.0640e-04 152 2.7015e-03 0.74763 0.071529 147 1.0640e-04 153 2.5951e-03 0.74763 0.071529 148 1.0611e-04 154 2.4887e-03 0.74763 0.071529 149 1.0260e-04 155 2.3826e-03 0.74763 0.071529 150 9.6395e-05 156 2.2800e-03 0.74796 0.071519 151 9.1612e-05 157 2.1836e-03 0.74794 0.071510 152 9.1612e-05 158 2.0920e-03 0.74575 0.071262 153 9.0178e-05 159 2.0004e-03 0.74575 0.071262 154 8.8088e-05 160 1.9102e-03 0.74505 0.071262 155 8.2225e-05 161 1.8221e-03 0.74505 0.071262 156 8.1241e-05 162 1.7399e-03 0.74505 0.071262 157 7.8365e-05 163 1.6587e-03 0.74505 0.071262 158 7.7933e-05 164 1.5803e-03 0.74444 0.071272 159 7.6301e-05 165 1.5024e-03 0.74439 0.071302 160 7.2769e-05 166 1.4261e-03 0.74487 0.071275 161 6.7381e-05 167 1.3533e-03 0.74537 0.071259 162 6.4264e-05 168 1.2859e-03 0.74567 0.071240 163 6.2200e-05 169 1.2216e-03 0.74575 0.071237 164 6.2200e-05 170 1.1594e-03 0.74484 0.071204 165 5.9104e-05 172 1.0350e-03 0.74682 0.071255 166 5.7226e-05 173 9.7594e-04 0.74664 0.071254 167 5.2975e-05 174 9.1872e-04 0.74630 0.071256 168 5.0154e-05 175 8.6574e-04 0.74784 0.071591 169 4.5138e-05 176 8.1559e-04 0.74775 0.071593 170 4.3548e-05 177 7.7045e-04 0.74783 0.071590 171 3.7343e-05 178 7.2690e-04 0.74733 0.071588 172 3.3574e-05 179 6.8956e-04 0.74901 0.071885 173 3.2132e-05 181 6.2241e-04 0.74965 0.071873 174 3.1933e-05 182 5.9028e-04 0.74965 0.071873 175 3.1735e-05 183 5.5835e-04 0.74965 0.071873 176 2.8211e-05 184 5.2661e-04 0.74871 0.071893 177 2.6528e-05 186 4.7019e-04 0.74910 0.071893 178 2.4895e-05 187 4.4366e-04 0.74849 0.071891 179 2.3315e-05 189 3.9387e-04 0.74884 0.071879 180 2.2406e-05 190 3.7055e-04 0.74863 0.071879 181 2.1787e-05 191 3.4815e-04 0.74883 0.071893 182 2.0310e-05 192 3.2636e-04 0.74905 0.071891 183 1.8885e-05 193 3.0605e-04 0.74848 0.071856 184 1.7512e-05 194 2.8717e-04 0.74842 0.071854 185 1.7486e-05 195 2.6965e-04 0.74764 0.071829 186 1.6191e-05 196 2.5217e-04 0.74786 0.071823 187 1.5967e-05 198 2.1978e-04 0.74741 0.071800 188 1.4922e-05 199 2.0382e-04 0.74741 0.071800 189 1.3816e-05 200 1.8890e-04 0.74746 0.071784 190 1.3704e-05 201 1.7508e-04 0.74738 0.071787 191 1.3704e-05 202 1.6138e-04 0.74738 0.071787 192 1.3134e-05 203 1.4767e-04 0.74738 0.071787 193 1.2538e-05 204 1.3454e-04 0.74713 0.071769 194 1.2469e-05 205 1.2200e-04 0.74686 0.071769 195 9.3520e-06 206 1.0953e-04 0.74739 0.071770 196 9.3520e-06 207 1.0018e-04 0.74683 0.071766 197 8.3935e-06 208 9.0826e-05 0.74686 0.071762 198 7.4868e-06 210 7.4039e-05 0.74709 0.071770 199 6.2951e-06 211 6.6552e-05 0.74743 0.071763 200 5.0775e-06 212 6.0257e-05 0.74757 0.071758 201 4.9739e-06 213 5.5179e-05 0.74752 0.071756 202 4.9739e-06 214 5.0205e-05 0.74752 0.071756 203 4.7731e-06 215 4.5232e-05 0.74752 0.071756 204 4.1967e-06 217 3.5685e-05 0.74745 0.071757 205 4.1492e-06 218 3.1488e-05 0.74741 0.071759 206 3.7304e-06 219 2.7339e-05 0.74701 0.071761 207 3.1346e-06 220 2.3609e-05 0.74663 0.071759 208 2.4956e-06 221 2.0474e-05 0.74656 0.071748 209 2.2106e-06 222 1.7979e-05 0.74660 0.071747 210 2.2106e-06 223 1.5768e-05 0.74670 0.071746 211 1.6580e-06 224 1.3557e-05 0.74670 0.071746 212 1.4594e-06 226 1.0241e-05 0.74634 0.071706 213 1.2694e-06 227 8.7821e-06 0.74652 0.071710 214 1.2694e-06 229 6.2433e-06 0.74652 0.071710 215 1.0000e-06 230 4.9739e-06 0.74652 0.071710 simple.tree &lt;- prune(tree,cp=0.1) large.tree &lt;- prune(tree,cp=1e-6) #cp_opt &lt;- tree$cptable[which.min(tree$cptable[,&quot;xerror&quot;]),&quot;CP&quot;] cp_opt &lt;- tree$cptable %&gt;% as.data.frame() %&gt;% filter(xerror==min(xerror)) %&gt;% dplyr::select(CP) %&gt;% as.numeric() opt.tree &lt;- prune(tree,cp=cp_opt) Calculer l’erreur quadratique de ces 3 arbres en utilisant l’échantillon test. Pour chaque arbre \\(T\\) on calcule \\[\\frac{1}{n_\\text{test}}\\sum_{i\\in \\text{test}}(Y_i-T(X_i))^2.\\] On définit une table qui regroupe les prédictions des 3 arbres sur l’échantillon test : data.prev &lt;- data.frame(simple=predict(simple.tree,newdata = test), large=predict(large.tree,newdata = test), opt=predict(opt.tree,newdata = test), obs=test$Sale) On en déduit les erreurs quadratique data.prev %&gt;% summarise_at(1:3,~mean((obs-.)^2)) simple large opt 1 5.800361 5.43738 4.469369 L’arbre sélectionné a ici la plus petite erreur. Refaire la comparaison avec une validation croisée 10 blocs. On créé tout d’abord les blocs. library(caret) K &lt;- 10 set.seed(1234) kfolds &lt;- createFolds(1:nrow(Carseats),k=K) On fait la validation croisée. prev &lt;- matrix(0,nrow=nrow(Carseats),ncol=3) %&gt;% as.data.frame() names(prev) &lt;- c(&quot;simple&quot;,&quot;large&quot;,&quot;opt&quot;) for (j in 1:K){ train &lt;- Carseats[-kfolds[[j]],] test &lt;- Carseats[kfolds[[j]],] tree &lt;- rpart(Sales~.,data=train,minsplit=2,cp=1e-9) simple &lt;- prune(tree,cp=tree$cptable[2,1]) large &lt;- prune(tree,cp=1e-9) cp_opt &lt;- tree$cptable %&gt;% as.data.frame() %&gt;% filter(xerror==min(xerror)) %&gt;% dplyr::select(CP) %&gt;% as.numeric() opt &lt;- prune(tree,cp=cp_opt) prev[kfolds[[j]],1] &lt;- predict(simple,newdata=test) prev[kfolds[[j]],2] &lt;- predict(large,newdata=test) prev[kfolds[[j]],3] &lt;- predict(opt,newdata=test) } prev %&gt;% mutate(obs=Carseats$Sales) %&gt;% summarize_at(1:3,~mean((obs-.)^2)) simple large opt 1 6.003064 4.79406 4.556404 3.2.2 Élagage en classification binaire et matrice de coût On considère ici les mêmes données que précédemment mais on cherche à expliquer une version binaire de la variable Sales. Cette nouvelle variable, appelée High prend pour valeurs No si Sales est inférieur ou égal à 8, Yes sinon. On travaillera donc avec le jeu data1 défini ci-dessous. High &lt;- ifelse(Carseats$Sales&lt;=8,&quot;No&quot;,&quot;Yes&quot;) data1 &lt;- Carseats %&gt;% dplyr::select(-Sales) %&gt;% mutate(High) Construire un arbre permettant d’expliquer High par les autres variables (sans Sales évidemment !) et expliquer les principales différences par rapport à la partie précédente précédente. set.seed(321) tree &lt;- rpart(High~.,data=data1) rpart.plot(tree) L’arbre construit est un arbre de classification. Le procédé de découpe des noeuds est différent : il utilise l’impureté de Gini au lieu de la variance. Expliquer l’option parms dans la commande : tree1 &lt;- rpart(High~.,data=data1,parms=list(split=&quot;information&quot;)) tree1$parms $prior 1 2 0.59 0.41 $loss [,1] [,2] [1,] 0 1 [2,] 1 0 $split [1] 2 On change de fonction d’impureté (information au lieu de Gini). Expliquer les sorties de la fonction printcp sur le premier arbre construit et retrouver la valeur du dernier terme de la colonne rel error. printcp(tree) Classification tree: rpart(formula = High ~ ., data = data1) Variables actually used in tree construction: [1] Advertising Age CompPrice Income [5] Price ShelveLoc Root node error: 164/400 = 0.41 n= 400 CP nsplit rel error xerror xstd 1 0.286585 0 1.00000 1.00000 0.059980 2 0.109756 1 0.71341 0.71341 0.055477 3 0.045732 2 0.60366 0.66463 0.054298 4 0.036585 4 0.51220 0.64634 0.053821 5 0.027439 5 0.47561 0.60976 0.052806 6 0.024390 7 0.42073 0.58537 0.052083 7 0.012195 8 0.39634 0.56707 0.051515 8 0.010000 10 0.37195 0.53659 0.050518 On peut lire des informations sur la suite d’arbres emboîtés, cette suite est de longueur 17 ici. Dans le dernier tableau, chaque ligne représente un arbre de la suite et on a dans les colonnes : CP : le paramètre de complexité, plus il est petit plus l’arbre est profond ; nsplit : nombre de coupures de l’arbre ; rel error contient l’erreur calculée sur les données d’apprentissage. Cette erreur décroit lorsque la complexité augmente et peut être interprétée comme une erreur d’ajustement ; xerror : contient l’erreur calculée par validation croisée. Elle peut être interprétée comme une erreur de prévision ; xstd correspond à l’écart type estimé de l’erreur. Les types d’erreurs dépendent du problème considéré. Vu qu’on est ici sur un problème de classification, c’est l’erreur de classification qui est considérée. De plus ces erreurs sont normalisées par rapport à l’erreur de l’arbre racine (sans coupure). Ainsi on retrouve l’erreur demandée avec data1 %&gt;% mutate(fitted=predict(tree,type=&quot;class&quot;)) %&gt;% summarise(MC=mean(fitted!=High)/mean(High==&quot;Yes&quot;)) MC 1 0.3719512 #mean(predict(arbre,type=&quot;class&quot;)!=donnees$High)/mean(donnees$High==&quot;Yes&quot;) Sélectionner un arbre optimal dans la suite. tree1 &lt;- rpart(High~.,data=data1,cp=0.000001,minsplit=2) plotcp(tree1) cp_opt &lt;- tree1$cptable %&gt;% as.data.frame() %&gt;% slice(which.min(xerror)) %&gt;% dplyr::select(CP) %&gt;% as.numeric() tree_sel &lt;- prune(tree1,cp=cp_opt) rpart.plot(tree_sel) On considère la suite d’arbres tree2 &lt;- rpart(High~.,data=data1,parms=list(loss=matrix(c(0,5,1,0),ncol=2)), cp=0.01,minsplit=2) Expliquer les sorties des commandes suivantes. On pourra notamment calculer le dernier terme de la colonne rel error de la table cptable. tree2$parms $prior 1 2 0.59 0.41 $loss [,1] [,2] [1,] 0 1 [2,] 5 0 $split [1] 1 printcp(tree2) Classification tree: rpart(formula = High ~ ., data = data1, parms = list(loss = matrix(c(0, 5, 1, 0), ncol = 2)), cp = 0.01, minsplit = 2) Variables actually used in tree construction: [1] Advertising Age CompPrice Education [5] Income Population Price ShelveLoc Root node error: 236/400 = 0.59 n= 400 CP nsplit rel error xerror xstd 1 0.101695 0 1.00000 5.0000 0.20840 2 0.050847 2 0.79661 3.7119 0.20834 3 0.036017 3 0.74576 3.1653 0.20197 4 0.035311 5 0.67373 2.9449 0.19818 5 0.025424 9 0.50847 2.4915 0.18872 6 0.016949 11 0.45763 2.3559 0.18485 7 0.015537 16 0.37288 2.0339 0.17491 8 0.014831 21 0.28814 2.0339 0.17491 9 0.010593 23 0.25847 1.8983 0.16982 10 0.010000 25 0.23729 1.8263 0.16664 Le critère est ici modifié, on utilise une erreur de classification pondérée pour choisir l’arbre. On rappelle que l’erreur de classification est définie par \\[L(g)=P(g(X)\\neq Y)=E[\\alpha_11_{g(X)=0,Y=1}+\\alpha_21_{g(X)=1,Y=0}]\\] avec \\(\\alpha_1=\\alpha_2=1\\). Cette erreur donne donc le même poids aux deux erreurs possible (prédire 1 à tort ou prédire 0 à tort). Utiliser cette erreur revient donc à supposer qu’elles ont la même importance pour le problème considéré. Ce n’est bien entendu pas toujours le cas en pratique. La matrice loss contient les valeurs de \\(\\alpha_1\\) et \\(\\alpha_2\\) et modifier ces valeurs permettra de donner des poids différents à ces deux erreurs. Avec cette nouvelle commande, on donne un poids de 5 pour une erreur et de 1 pour l’autre. On obtient le terme demandé avec prev &lt;- predict(tree2,type=&quot;class&quot;) conf &lt;- table(data1$High,prev) conf prev No Yes No 185 51 Yes 1 163 loss &lt;- tree2$parms$loss (conf[1,2]*loss[1,2]+conf[2,1]*loss[2,1])/nrow(data1)/mean(data1$High==&quot;No&quot;) [1] 0.2372881 Comparer les valeurs ajustées par les deux arbres considérés. summary(predict(tree_sel,type=&quot;class&quot;)) No Yes 240 160 summary(predict(tree2,type=&quot;class&quot;)) No Yes 186 214 Il y a plus de Yes prédits dans le second arbre. Cela vient des changements dans la matrice loss : la perte pour prédire No au lieu de Yes est de 5 pour le second arbre. Cela signifie bien détecter les Yes est plus important pour cet arbre, c’est donc tout à fait normal qu’il prédise plus souvent Yes que le premier. Cette stratégie de changer la matrice de coût peut se révéler intéressante dans le cas de données déséquilibrées : une modalité de la cible sous-représentée par rapport à l’autre. En effet, pour de tels problèmes il est souvent très important de bien détecter la modalité sous-représentée. On pourra donc donner un poids plus fort lorsqu’on détecte mal cette modalité. Références "],
["agregation.html", "Chapitre 4 Agrégation : forêts aléatoires et gradient boosting 4.1 Forêts aléatoires 4.2 Gradient boosting", " Chapitre 4 Agrégation : forêts aléatoires et gradient boosting Les méthodes par arbres présentées précédemment sont des algorithmes qui possèdent tout un tas de qualités (facile à mettre en œuvre, interprétable…). Ce sont néanmoins rarement les algorithmes qui se révèlent les plus performants. Les méthodes d’agrégation d’arbres présentées dans cette partie sont souvent beaucoup plus pertinentes, notamment en terme de qualité de prédiction. Elles consistent à construire un très grand nombre d’arbres “simples” : \\(g_1,\\dots,g_B\\) et à les agréger en faisant la moyenne : \\[\\frac{1}{B}\\sum_{k=1}^Bg_k(x).\\] Les forêts aléatoires (Breiman 2001) et le gradient boosting (Friedman 2001) utilisent ce procédé d’agrégation. 4.1 Forêts aléatoires L’algorithme des forêts aléatoires consiste à construire des arbres sur des échantillons bootstrap et à les agréger. Il peut s’écrire de la façon suivante : Entrées : \\(x\\in\\mathbb R^d\\) l’observation à prévoir, \\(\\mathcal D_n\\) l’échantillon ; \\(B\\) nombre d’arbres ; \\(n_{max}\\) nombre max d’observations par nœud \\(m\\in\\{1,\\dots,d\\}\\) le nombre de variables candidates pour découper un nœud. Algorithme : pour \\(k=1,\\dots,B\\) : Tirer un échantillon bootstrap dans \\(\\mathcal D_n\\) Construire un arbre CART sur cet échantillon bootstrap, chaque coupure est sélectionnée en minimisant la fonction de coût de CART sur un ensemble de \\(m\\) variables choisies au hasard parmi les \\(d\\). On note \\(T(.,\\theta_k,\\mathcal D_n)\\) l’arbre construit. Sortie : l’estimateur \\(T_B(x)=\\frac{1}{B}\\sum_{k=1}^BT(x,\\theta_k,\\mathcal D_n)\\). Cet algorithme peut être utilisé sur R avec la fonction randomForest du package randomForest. Nous la présentons à travers l’exemple du jeu de données spam du package kernlab. library(kernlab) data(spam) set.seed(1234) spam &lt;- spam[sample(nrow(spam)),] Le problème est d’expliquer la variable binaire type par les autres. A l’aide de la fonction randomForest du package randomForest, ajuster une forêt aléatoire pour répondre au problème posé. library(randomForest) rf1 &lt;- randomForest(type~.,data=spam) Appliquer la fonction plot à l’objet construit avec randomForest et expliquer le graphe obtenu. A quoi peut servir ce graphe en pratique ? plot(rf1) Ce graphe permet de visualiser l’erreur de classication ainsi que les taux de faux positifs et faux négatifs calculés par Out Of Bag en fonction du nombre d’arbres de la forêt. Ce graphe peut être utilisé pour voir si l’algorithme a bien “convergé”. Si ce n’est pas le cas, il faut construire une forêt avec plus d’abres. Construire la forêt avec mtry=1 et comparer ses performances avec celle construite précédemment. rf2 &lt;- randomForest(type~.,data=spam,mtry=1) rf1 Call: randomForest(formula = type ~ ., data = spam) Type of random forest: classification Number of trees: 500 No. of variables tried at each split: 7 OOB estimate of error rate: 4.56% Confusion matrix: nonspam spam class.error nonspam 2711 77 0.02761836 spam 133 1680 0.07335907 rf2 Call: randomForest(formula = type ~ ., data = spam, mtry = 1) Type of random forest: classification Number of trees: 500 No. of variables tried at each split: 1 OOB estimate of error rate: 7.89% Confusion matrix: nonspam spam class.error nonspam 2729 59 0.02116212 spam 304 1509 0.16767788 La forêt rf1 est plus performante en terme d’erreur de classification OOB. Utiliser la fonction train du package caret pour choisir le paramètre mtry dans la grille seq(1,30,by=5). library(caret) grille.mtry &lt;- data.frame(mtry=seq(1,30,by=5)) ctrl &lt;- trainControl(method=&quot;oob&quot;) library(doParallel) ## pour paralléliser cl &lt;- makePSOCKcluster(4) registerDoParallel(cl) set.seed(12345) sel.mtry &lt;- train(type~.,data=spam,method=&quot;rf&quot;,trControl=ctrl,tuneGrid=grille.mtry) on.exit(stopCluster(cl)) On choisit sel.mtry$bestTune mtry 2 6 Construire la forêt avec le paramètre mtry sélectionné. Calculer l’importance des variables et représenter ces importance à l’aide d’un diagramme en barres. rf3 &lt;- randomForest(type~.,data=spam,mtry=unlist(sel.mtry$bestTune),importance=TRUE) Imp &lt;- randomForest::importance(rf3,type=1) %&gt;% as.data.frame() %&gt;% mutate(variable=names(spam)[-58]) %&gt;% arrange(desc(MeanDecreaseAccuracy)) head(Imp) MeanDecreaseAccuracy variable 1 47.58430 charExclamation 2 40.83001 remove 3 40.79968 charDollar 4 40.39225 capitalAve 5 37.18721 free 6 36.17332 edu ggplot(Imp) + aes(x=reorder(variable,MeanDecreaseAccuracy),y=MeanDecreaseAccuracy)+geom_bar(stat=&quot;identity&quot;)+coord_flip()+xlab(&quot;&quot;)+theme_classic() La fonction vip du package vip permet de faire le diagramme en barres plus facilement library(vip) vip(rf3) La fonction ranger du package ranger permet également de calculer des forêts aléatoires. Comparer les temps de calcul de cette fonction avec randomForest library(ranger) system.time(rf4 &lt;- ranger(type~.,data=spam)) user system elapsed 2.117 0.019 0.673 system.time(rf5 &lt;- randomForest(type~.,data=spam)) user system elapsed 8.823 0.086 8.920 Le temps de calcul est plus rapide avec ranger. Ce package permet une implémentation efficace des forêts aléatoires pour des données de grande dimension. on peut touver plus d’information ici. 4.2 Gradient boosting Les algorithmes de gradient boosting permettent de minimiser des pertes empiriques de la forme \\[\\frac{1}{n}\\sum_{i=1}^n\\ell(y_i,f(x_i)).\\] où \\(\\ell:\\mathbb R\\times\\mathbb R\\to\\mathbb R\\) est une fonction de coût convexe en son second argument. Il existe plusieurs type d’algorithmes boosting. Un des plus connus et utilisés a été proposé par Friedman (2001), c’est la version que nous étudions dans cette partie. Cette approche propose de chercher la meilleure combinaison linéaire d’arbres binaires, c’est-à-dire que l’on recherche \\(g(x)=\\sum_{m=1}^M\\alpha_mh_m(x)\\) qui minimise \\[\\mathcal R_n(g)=\\frac{1}{n}\\sum_{i=1}^n\\ell(y_i,g(x_i)).\\] Optimiser sur toutes les combinaisons d’arbres binaires se révélant souvent trop compliqué, Friedman (2001) utilise une descente de gradient pour construire la combinaison d’abres de façon récursive. L’algorithme est le suivant : Entrées : \\(d_n=(x_1,y_1),\\dots,(x_n,y_n)\\) l’échantillon, \\(\\lambda\\) un paramètre de régularisation tel que \\(0&lt;\\lambda\\leq 1\\). \\(M\\in\\mathbb N\\) le nombre d’itérations. paramètres de l’arbre (nombre de coupures…) Itérations : Initialisation : \\(g_0(.)=\\mathop{\\mathrm{argmin}}_c \\frac{1}{n}\\sum_{i=1}^n \\ell(y_i,c)\\) Pour \\(m=1\\) à \\(M\\) : Calculer l’opposé du gradient \\(-\\frac{\\partial}{\\partial g(x_i)}\\ell(y_i,g(x_i))\\) et l’évaluer aux points \\(g_{m-1}(x_i)\\) : \\[U_i=-\\frac{\\partial}{\\partial g(x_i)}\\ell(y_i,g(x_i)) _{\\Big |g(x_i)=g_{m-1}(x_i)},\\quad i=1,\\dots,n.\\] Ajuster un arbre sur l’échantillon \\((x_1,U_1),\\dots,(x_n,U_n)\\), on le note \\(h_m\\). Mise à jour : \\(g_m(x)=g_{m-1}(x)+\\lambda h_m(x)\\). Sortie : la suite \\((g_m(x))_m\\). Sur R On peut utiliser différents packages pour faire du gradient boosting. Nous utilisons ici le package gbm (Ridgeway 2006). 4.2.1 Un exemple simple en régression On considère un jeu de données \\((x_i,y_i),i=1,\\dots,200\\) issu d’un modèle de régression \\[y_i=m(x_i)+\\varepsilon_i\\] où la vraie fonction de régression est la fonction sinus (mais on va faire comme si on ne le savait pas). x &lt;- seq(-2*pi,2*pi,by=0.01) y &lt;- sin(x) set.seed(1234) X &lt;- runif(200,-2*pi,2*pi) Y &lt;- sin(X)+rnorm(200,sd=0.2) df1 &lt;- data.frame(X,Y) df2 &lt;- data.frame(X=x,Y=y) p1 &lt;- ggplot(df1)+aes(x=X,y=Y)+geom_point()+geom_line(data=df2,size=1)+xlab(&quot;&quot;)+ylab(&quot;&quot;) p1 Rappeler ce que siginifie le \\(L_2\\)-boosting. Il s’agit de l’algorithme de gradient boosting présenté ci-dessus appliqué à la fonction de perte \\[\\ell(y,f(x))=\\frac{1}{2}(y-f(x))^2.\\] A l’aide de la fonction gbm du package gbm construire un algorithme de \\(L_2\\)-boosting. On utilisera 500000 itérations et gardera les autres valeurs par défaut de paramètres. library(gbm) L2boost &lt;- gbm(Y~.,data=df1,n.trees = 500000,distribution=&quot;gaussian&quot;,bag.fraction = 1) Visualiser l’estimateur à la première itération. On pourra faire un predict avec l’option n.trees. prev1 &lt;- predict(L2boost,newdata=df2,n.trees=1) df3 &lt;- df2 %&gt;% rename(vraie=Y) %&gt;% mutate(`M=1`=prev1) df4 &lt;- df3 %&gt;% pivot_longer(-X,names_to=&quot;courbes&quot;,values_to=&quot;prev&quot;) ggplot(df4)+aes(x=X,y=prev,color=courbes)+geom_line(size=1) On remarque que l’estimateur est un arbre avec une seule coupure. Faire de même pour les itérations 1000 et 500000. prev1000 &lt;- predict(L2boost,newdata=df2,n.trees=1000) prev500000 &lt;- predict(L2boost,newdata=df2,n.trees=500000) df31 &lt;- df3 %&gt;% mutate(`M=1000`=prev1000,`M=500000`=prev500000) df41 &lt;- df31 %&gt;% pivot_longer(-X,names_to=&quot;courbes&quot;,values_to=&quot;prev&quot;) ggplot(df41)+aes(x=X,y=prev,color=courbes)+geom_line(size=1) On surajuste lorsque le nombre d’itérations est trop important. Sélectionner le nombre d’itérations par la procédure de votre choix. On propose de faire une validation hold out. C’est assez facile avec gbm il suffit de renseigner l’option train.fraction de gbm. #parallel:::setDefaultClusterOptions(setup_strategy = &quot;sequential&quot;) L2boost.sel &lt;- gbm(Y~.,data=df1,n.trees = 10000,distribution=&quot;gaussian&quot;, bag.fraction = 1,train.fraction=0.75) gbm.perf(L2boost.sel) [1] 4787 4.2.2 Adaboost et logitboost pour la classification binaire. On considère le jeu de données spam du package kernlab. library(kernlab) data(spam) set.seed(1234) spam &lt;- spam[sample(nrow(spam)),] Exécuter la commande model_ada1 &lt;- gbm(type~.,data=spam,distribution=&quot;adaboost&quot;,interaction.depth=2,shrinkage=0.05,n.trees=500) Proposer une correction permettant de faire fonctionner l’algorithme. Il est nécessaire que la variable qualitative à expliquer soit codée 0-1 pour adaboost. spam1 &lt;- spam spam1$type &lt;- as.numeric(spam1$type)-1 set.seed(1234) model_ada1 &lt;- gbm(type~.,data=spam1,distribution=&quot;adaboost&quot;,interaction.depth=2,shrinkage=0.05,n.trees=500) Expliciter le modèle ajusté par la commande précédente. L’algorithme gbm est une descente de gradient qui minimise la fonction de perte \\[\\frac{1}{n}\\sum_{i=1}^n \\ell(y_i,g(x_i)).\\] Dans le cas de adaboost on utilise la perte exponentielle : \\(\\ell(y,g(x))=\\exp(-yg(x))\\). Effectuer un summary du modèle ajusté. Expliquer la sortie. summary(model_ada1) var rel.inf charExclamation charExclamation 20.04035224 charDollar charDollar 17.51535261 remove remove 11.51692621 free free 7.49397637 hp hp 6.25654932 capitalLong capitalLong 5.42905223 capitalAve capitalAve 4.69521299 your your 4.23371585 george george 2.50300727 edu edu 2.19692796 our our 1.99655393 money money 1.79063219 email email 1.51773292 capitalTotal capitalTotal 1.43872496 internet internet 1.12579132 receive receive 0.97001932 will will 0.94015881 you you 0.89915372 business business 0.84418397 re re 0.82959153 num1999 num1999 0.80016393 num650 num650 0.79468746 meeting meeting 0.69494729 num000 num000 0.56448978 charRoundbracket charRoundbracket 0.39921437 report report 0.38621968 charSemicolon charSemicolon 0.29835251 credit credit 0.27841575 over over 0.27064075 order order 0.26017226 mail mail 0.22398163 technology technology 0.10340435 hpl hpl 0.10151723 original original 0.09615196 font font 0.09539134 make make 0.08995855 project project 0.07970985 all all 0.05392468 people people 0.05359692 address address 0.04690996 parts parts 0.04260362 conference conference 0.02037549 num85 num85 0.01155488 num3d num3d 0.00000000 addresses addresses 0.00000000 lab lab 0.00000000 labs labs 0.00000000 telnet telnet 0.00000000 num857 num857 0.00000000 data data 0.00000000 num415 num415 0.00000000 pm pm 0.00000000 direct direct 0.00000000 cs cs 0.00000000 table table 0.00000000 charSquarebracket charSquarebracket 0.00000000 charHash charHash 0.00000000 On obtient un indicateur qui permet de mesurer l’importance des variable dans la construction de la méthode. Utiliser la fonction vip du package vip pour retrouver ce sorties. library(vip) vip(model_ada1,num_features = 20L) Sélectionner le nombre d’itérations pour l’algorithme adaboost en faisant de la validation croisée 5 blocs. model_ada2 &lt;- gbm(type~.,data=spam1,distribution=&quot;adaboost&quot;,interaction.depth=2,bag.fraction=1,cv.folds = 5,n.trees=500) gbm.perf(model_ada2) [1] 233 Faire la même procédure en changeant la valeur du paramètre shrinkage. Interpréter. model_ada3 &lt;- gbm(type~.,data=spam1,distribution=&quot;adaboost&quot;,interaction.depth=2,bag.fraction=1,cv.folds = 5,n.trees=500,shrinkage=0.05) gbm.perf(model_ada3) [1] 370 model_ada4 &lt;- gbm(type~.,data=spam1,distribution=&quot;adaboost&quot;,interaction.depth=2,bag.fraction=1,cv.folds = 5,n.trees=500,shrinkage=0.5) gbm.perf(model_ada4) [1] 36 Le nombre d’itérations optimal augmente lorsque shrinkage diminue. C’est logique car ce dernier paramètre contrôle la vitesse de descente de gradient : plus il est grand, plus on minimise vite et moins on itère. Il faut néanmoins veiller à ne pas le prendre trop petit pour avoir un estimateur stable. Ici, 0.05 semble être une bonne valeur. Expliquer la différence entre adaboost et logitboost et précisez comment on peut mettre en œuvre ce dernier algorithme. La seule différence se situe au niveau de la fonction de perte, adaboost utilise \\[\\exp(-yg(x))\\] tandis que logitboost utilise \\[\\log(1+\\exp(-2yg(x)))\\] Avec gbm il faudra utiliser l’option distribution=“bernoulli” pour faire du logitboost, par exemple : set.seed(4321) logitboost &lt;- gbm(type~.,data=spam1,distribution=&quot;bernoulli&quot;,interaction.depth=2,bag.fraction=1,cv.folds = 5,n.trees=500,shrinkage=0.4) gbm.perf(logitboost) [1] 288 Références "],
["deep.html", "Chapitre 5 Réseaux de neurones avec Keras", " Chapitre 5 Réseaux de neurones avec Keras Nous présentons ici une introduction au réseau de neurones à l’aide du package keras. On pourra trouver une documentation complète ainsi qu’un très bon tutoriel aux adresses suivantes https://keras.rstudio.com et https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/. On commence par charger la librairie library(keras) #install_keras() 1 seule fois sur la machine On va utiliser des réseaux de neurones pour le jeu de données spam où le problème est d’expliquer la variable binaires typepar les 57 autres variables du jeu de données : library(kernlab) data(spam) spamX &lt;- as.matrix(spam[,-58]) #spamY &lt;- to_categorical(as.numeric(spam$type)-1, 2) spamY &lt;- as.numeric(spam$type)-1 On sépare les données en un échantillon d’apprentissage et un échantillon test set.seed(5678) perm &lt;- sample(4601,3000) appX &lt;- spamX[perm,] appY &lt;- spamY[perm] validX &lt;- spamX[-perm,] validY &lt;- spamY[-perm] A l’aide des données d’apprentissage, entrainer un perceptron simple avec une fonction d’activation sigmoïde. On utilisera 30 epochs et des batchs de taille 5. On définit tout d’abord la structure du réseau, 1 seule couche ici de 1 neurone : percep.sig &lt;- keras_model_sequential() percep.sig %&gt;% layer_dense(units=1,input_shape = 57,activation=&quot;sigmoid&quot;) summary(percep.sig) Model: &quot;sequential&quot; ____________________________________________________________ Layer (type) Output Shape Param # ============================================================ dense (Dense) (None, 1) 58 ============================================================ Total params: 58 Trainable params: 58 Non-trainable params: 0 ____________________________________________________________ On donne ensuite la fonction de perte, l’algorithme d’optimisation ainsi que le critère pour mesurer la performance du réseau : percep.sig %&gt;% compile( loss=&quot;binary_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=&quot;accuracy&quot; ) On donne enfin dans fit les paramètres qui permettent d’entrainer le modèle (taille des batchs, nombre d’epochs…) p.sig &lt;- percep.sig %&gt;% fit( x=appX, y=appY, epochs=30, batch_size=5, validation_split=0.2, verbose=0 ) La fonction plot permet de visualiser la perte et la performance en fonction du nombre d’epochs : plot(p.sig) Faire de même avec la fonction d’activation softmax. On utilisera pour cela 2 neurones avec une sortie \\(Y\\) possédant la forme suivante. spamY1 &lt;- to_categorical(as.numeric(spam$type)-1, 2) appY1 &lt;- spamY1[perm,] validY1 &lt;- spamY1[-perm,] percep.soft &lt;- keras_model_sequential() percep.soft %&gt;% layer_dense(units=2,input_shape = 57,activation=&quot;softmax&quot;) summary(percep.soft) Model: &quot;sequential_1&quot; ____________________________________________________________ Layer (type) Output Shape Param # ============================================================ dense_1 (Dense) (None, 2) 116 ============================================================ Total params: 116 Trainable params: 116 Non-trainable params: 0 ____________________________________________________________ percep.soft %&gt;% compile( loss=&quot;binary_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=&quot;accuracy&quot; ) p.soft &lt;- percep.soft %&gt;% fit( x=appX, y=appY1, epochs=30, batch_size=1, validation_split=0.2, verbose=0 ) plot(p.soft) Comparer les performances des deux perceptrons sur les données de validation à l’aide de la fonction evaluate. percep.sig %&gt;% evaluate(validX,validY) loss accuracy 27.7081203 0.4097439 percep.soft %&gt;% evaluate(validX,validY1) loss accuracy 5.9358535 0.6002498 Construire un ou deux réseaux avec deux couches cachées. On pourra faire varier les nombre de neurones dans ces couches. Comparer les performances des réseaux construits. On propose tout d’abord 2 couches cachées composées de 100 neurones : mod2c &lt;- keras_model_sequential() mod2c %&gt;% layer_dense(units=100,activation=&quot;softmax&quot;) %&gt;% layer_dense(units=100,activation=&quot;softmax&quot;) %&gt;% layer_dense(units = 1,activation = &quot;sigmoid&quot;) mod2c %&gt;% compile( loss=&quot;binary_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=&quot;accuracy&quot; ) mod2c %&gt;% fit( x=appX, y=appY, epochs=100, batch_size=3, validation_split=0.2, verbose=0 ) mod2c %&gt;% evaluate(validX,validY) loss accuracy 0.6920030 0.6008744 On propose ici 50 neurones pour la première couche cachée et 30 neurones pour la seconde. On ajoute de plus un dropout dans la première couche cachée (permet généralement d’éviter le sur-apprentissage, mais pas forcément utile ici). mod2cd &lt;- keras_model_sequential() mod2cd %&gt;% layer_dropout(0.7) %&gt;% layer_dense(units=50,activation=&quot;softmax&quot;) %&gt;% layer_dense(units=30,activation=&quot;softmax&quot;) %&gt;% layer_dense(units = 1,activation = &quot;sigmoid&quot;) mod2cd %&gt;% compile( loss=&quot;binary_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=&quot;accuracy&quot; ) mod2cd %&gt;% fit( x=appX, y=appY, epochs=150, batch_size=5, validation_split=0.2, verbose=0 ) On évalue la performance sur les données test : mod2cd %&gt;% evaluate(validX,validY) loss accuracy 0.6922378 0.6008744 "],
["références.html", "Références", " Références "]
]
