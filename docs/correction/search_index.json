[["index.html", "Machine learning Présentation", " Machine learning Laurent Rouvière 2020-11-20 Présentation Ce tutoriel présente une introduction au machine learning avec R. On pourra trouver : les supports de cours associés à ce tutoriel ainsi que les données utilisées à l’adresse suivante https://lrouviere.github.io/machine_learning/ ; le tutoriel sans les correction à l’url https://lrouviere.github.io/TUTO_ML/ le tutoriel avec les corrigés (à certains moment) à l’url https://lrouviere.github.io/TUTO_ML/correction/. Il est recommandé d’utiliser mozilla firefox pour lire le tutoriel. Les thèmes suivants sont abordés : Estimation du risque, présentation du package caret ; SVM, cas séparable, non séparable et astuce du noyau ; Arbres, notamment l’algorithme CART ; Agrégation d’arbres, forêts aléatoires et gradient boosting ; Réseaux de neurones et introduction au deep learning, perceptron multicouches avec keras. Il existe de nombreuses références sur le machine learning, la plus connue étant certainement Hastie, Tibshirani, and Friedman (2009), disponible en ligne à l’url https://web.stanford.edu/~hastie/ElemStatLearn/. On pourra également consulter Boehmke and Greenwell (2019) qui propose une présentation très claire des algorithmes machine learning avec R. Cet ouvrage est également disponible en ligne à l’url https://bradleyboehmke.github.io/HOML/. Références "],["caret.html", "Chapitre 1 Estimation du risque avec caret 1.1 Notion de risque en apprentissage supervisé 1.2 La validation croisée 1.3 Le package caret 1.4 Compléments", " Chapitre 1 Estimation du risque avec caret 1.1 Notion de risque en apprentissage supervisé L’apprentissage supervisé consiste à expliquer ou prédire une sortie \\(y\\in\\mathcal Y\\) par des entrées \\(x\\in\\mathcal X\\) (le plus souvent \\(\\mathcal X=\\mathbb R^p\\)). Cela revient à trouver un algorithme ou machine représenté par une fonction \\[f:\\mathcal X\\to\\mathcal Y\\] qui à une nouvelle observation \\(x\\) associe la prévision \\(f(x)\\). Bien entendu le problème consiste à chercher le meilleur algorithme pour le cas d’intérêt. Cette notion nécessite de meilleur algorithme la définition de critères que l’on va chercher à optimiser. Les critères sont le plus souvent définis à partir du fonction de perte \\[\\begin{align*} \\ell:\\mathcal Y \\times\\mathcal Y &amp; \\mapsto \\mathbb R^+ \\\\ (y,y^\\prime) &amp; \\to\\ell(y,y^\\prime) \\end{align*}\\] où \\(\\ell(y,y^\\prime)\\) représentera l’erreur (ou la perte) pour la prévision \\(y^\\prime\\) par rapport à l’observation \\(y\\). Si on représente le phénomène d’intérêt par un couple aléatoire \\((X,Y)\\) à valeurs dans \\(\\mathcal X\\times\\mathcal Y\\), on mesurera la performance d’un algorithme \\(f\\) par son risque \\[\\mathcal R(f)=\\mathbf E[\\ell(Y,f(X))].\\] Trouver le meilleur algorithme revient alors à trouver \\(f\\) qui minimise \\(\\mathcal R(f)\\). Bien entendu, ce cadre possède une utilité limitée en pratique puisqu’on ne connaît jamais la loi de \\((X,Y)\\), on ne pourra donc jamais calculé le vrai risque d’un algorithme \\(f\\). Tout le problème va donc être de trouver l’algorithme qui a le plus petit risque à partir de \\(n\\) observations \\((x_1,y_1),\\dots,(x_n,y_n)\\). Nous verrons dans les chapitres suivants plusieurs façons de construire des algorithmes mais, dans tous les cas, un algorithme est représenté par une fonction \\[f_n:\\mathcal X\\times(\\mathcal X\\times\\mathcal Y)^n\\to\\mathcal Y\\] qui, pour une nouvelle donnée \\(x\\), renverra la prévision \\(f_n(x)\\) calculée à partir de l’échantillon qui vit dans \\((\\mathcal X\\times\\mathcal Y)^n\\). Dès lors la question qui se pose est de calculer (ou plutôt d’estimer) le risque (inconnu) \\(\\mathcal R(f_n)\\) d’un algorithme \\(f_n\\). Les techniques classiques reposent sur des algorithmes de type validation croisée. Nous les mettons en œuvre dans cette partie pour un algorithme simple : les \\(k\\) plus proches voisins. On commencera par programmer ces techniques “à la main” puis on utilisera le package caret qui permet de calculer des risques pour quasiment tous les algorithmes que l’on retrouver en apprentissage supervisé. 1.2 La validation croisée On cherche à expliquer une variable binaire \\(Y\\) par deux variables quantitatives \\(X_1\\) et \\(X_2\\) à l’aide du jeu de données suivant n &lt;- 2000 set.seed(12345) X1 &lt;- runif(n) X2 &lt;- runif(n) set.seed(9012) R1 &lt;- X1&lt;=0.25 R2 &lt;- (X1&gt;0.25 &amp; X2&gt;=0.75) R3 &lt;- (X1&gt;0.25 &amp; X2&lt;0.75) Y &lt;- rep(0,n) Y[R1] &lt;- rbinom(sum(R1),1,0.25) Y[R2] &lt;- rbinom(sum(R2),1,0.25) Y[R3] &lt;- rbinom(sum(R3),1,0.75) donnees &lt;- data.frame(X1,X2,Y) donnees$Y &lt;- as.factor(donnees$Y) ggplot(donnees)+aes(x=X1,y=X2,color=Y)+geom_point() On considère la perte indicatrice : \\(\\ell(y,y^\\prime)=\\mathbf 1_{y\\neq y^\\prime}\\), le risque d’un algorithme \\(f\\) est donc \\[\\mathcal R(f)=\\mathbf E[\\mathbf 1_{Y\\neq f(X)}]=\\mathbf P(Y\\neq f(X)),\\] il est appelé probabilité d’erreur ou erreur de classification. Séparer le jeu de données en un échantillon d’apprentissage dapp de taille 1500 et un échantillon test dtest de taille 500. set.seed(234) indapp &lt;- sample(nrow(donnees),1500) dapp &lt;- donnees[indapp,] dtest &lt;- donnees[-indapp,] On considère la règle de classification des \\(k\\) plus proches voisins. Pour un entier \\(k\\) plus petit que \\(n\\) et un nouvel individu \\(x\\), cette règle affecte à \\(x\\) le label majoritaire des \\(k\\) plus proches voisins de \\(x\\). Sur R on utilise la fonction knn du package class. On peut par exemple obtenir les prévisions des individus de l’échantillon test de la règle des 3 plus proches voisins avec library(class) knn3 &lt;- knn(dapp[,1:2],dtest[,1:2],cl=dapp$Y,k=3) Calculer l’erreur de classification de la règle des 3 plus proches voisins sur les données test (procédure validation hold out). mean(knn3!=dtest$Y) [1] 0.338 Expliquer la fonction knn.cv. Cette fonction permet, pour la règle des plus proches voisins, de prédire le groupe de chaque individu par validation croisée leave-one-out : \\[\\widehat y_i=g_{k,i}(x_i),\\quad i=1,\\dots,n\\] où \\(g_{k,i}\\) désigne la règle de \\(k\\) plus proche voisins construites à partir de l’échantillon amputé de la \\(i\\)ème observation. Calculer l’erreur de classification de la règle des 3 plus proches voisins par validation croisée leave-one-out. prev_cv &lt;- knn.cv(donnees[,-3],cl=donnees$Y,k=3) On peut alors estimer l’erreur de la règle des 10 ppv par \\[\\frac{1}{n}\\sum_{i=1}^n1_{g_{k,i}(x_i)\\neq y_i}.\\] mean(prev_cv!=donnees$Y) [1] 0.334 On considère le vecteur de plus proches voisins suivant : K_cand &lt;- seq(1,500,by=20) Sélectionner une valeur de \\(k\\) dans ce vecteur à l’aide d’une validation hold out et d’un leave-one-out : On calcule l’erreur de classification par validation hold out pour chaque valeur de \\(k\\) : err.ho &lt;- rep(0,length(K_cand)) for (i in 1:length(K_cand)){ knni &lt;- knn(dapp[,1:2],dtest[,1:2],cl=dapp$Y,k=K_cand[i]) err.ho[i] &lt;- mean(knni!=dtest$Y) } Puis on choisit la valeur de \\(k\\) pour laquelle l’erreur est minimale. K_cand[which.min(err.ho)] [1] 41 On de même chose avec la validation croisée leave-one-out : err.loo &lt;- rep(0,length(K_cand)) for (i in 1:length(K_cand)){ knni &lt;- knn.cv(donnees[,-3],cl=donnees$Y,k=K_cand[i]) err.loo[i] &lt;- mean(knni!=donnees$Y) } K_cand[which.min(err.loo)] Faire la même chose à l’aide d’une validation croisée 10 blocs. On pourra construire les blocs avec set.seed(2345) blocs &lt;- caret::createFolds(1:nrow(donnees),10,returnTrain = TRUE) err.cv &lt;- rep(0,length(K_cand)) prev &lt;- donnees$Y for (i in 1:length(K_cand)){ for (j in 1:length(blocs)){ train &lt;- donnees[blocs[[j]],] test &lt;- donnees[-blocs[[j]],] prev[-blocs[[j]]] &lt;- knn(train[,1:2],test[,1:2],cl=train$Y,k=K_cand[i]) } err.cv[i] &lt;- mean(prev!=donnees$Y) } K_cand[which.min(err.cv)] 1.3 Le package caret Dans la partie précédente, nous avons utiliser des méthodes de validation croisée pour sélectionner le nombre de voisins dans l’algorithme des plus proches voisins. L’approche revenait à estimer un risque pour une grille de valeurs candidates de \\(k\\) choisir la valeur de \\(k\\) qui minimise le risque estimé. Cette pratique est courante en machine learning : on la retrouve fréquemment pour calibrer les algorithmes. Le protocole est toujours le même, pour un méthode donnée il faut spécifier : une grille de valeurs pour les paramètres un risque un algorithme pour estimer le risque. Le package caret permet d’appliquer ce protocole pour plus de 200 algorithmes machine learning. On pourra trouver une documentation complète à cette url http://topepo.github.io/caret/index.html. Deux fonctions sont à utiliser : traincontrol qui permettra notamment de spécifier l’algorithme pour estimer le risque ainsi que les paramètres de cet algorithme ; train dans laquelle on renseignera les données, la grille de candidats… On reprend les données de la partie précédente. Expliquer les sorties des commandes library(caret) set.seed(321) ctrl1 &lt;- trainControl(method=&quot;LGOCV&quot;,number=1) KK &lt;- data.frame(k=K_cand) caret.ho &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl1,tuneGrid=KK) caret.ho k-Nearest Neighbors 2000 samples 2 predictor 2 classes: &#39;0&#39;, &#39;1&#39; No pre-processing Resampling: Repeated Train/Test Splits Estimated (1 reps, 75%) Summary of sample sizes: 1500 Resampling results across tuning parameters: k Accuracy Kappa 1 0.602 0.1956346 21 0.690 0.3649415 41 0.694 0.3736696 61 0.706 0.3992546 81 0.700 0.3867338 101 0.712 0.4122641 121 0.700 0.3882944 141 0.706 0.4017971 161 0.700 0.3903629 181 0.702 0.3941710 201 0.700 0.3898471 221 0.696 0.3806637 241 0.692 0.3714491 261 0.698 0.3829078 281 0.692 0.3693074 301 0.696 0.3764358 321 0.682 0.3474407 341 0.682 0.3468831 361 0.678 0.3352601 381 0.672 0.3214167 401 0.668 0.3113633 421 0.666 0.3057172 441 0.658 0.2853800 461 0.658 0.2841354 481 0.654 0.2732314 Accuracy was used to select the optimal model using the largest value. The final value used for the model was k = 101. plot(caret.ho) On obtient ici l’accuracy (1 moins l’erreur de classification) pour chaque valeur de \\(k\\) calculé par validation hold out. Cette technique a été précisée dans la fonction trainControl via l’option method=“LGOCV”. Un autre indicateur est calculé : le kappa de Cohen. Cet indicateur peut se révéler pertinent en présence de données déséquilibrées, on pourra trouver de l’information sur cet indicateur dans ce document https://lrouviere.github.io/INP-HB/cours_don_des.pdf En modifiant les paramètres du code précédent, retrouver les résultats de la validation hold out de la partie précédente. On pourra utiliser l’option index dans la fonction trainControl. ctrl2 &lt;- trainControl(method=&quot;LGOCV&quot;,number=1,index=list(indapp)) caret.ho2 &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl2,tuneGrid=KK) On retrouve bien la même valeur de \\(k\\). caret.ho2$bestTune k 3 41 Utiliser caret pour sélectionner \\(k\\) par validation croisée leave-one-out. ctrl3 &lt;- trainControl(method=&quot;LOOCV&quot;,number=1) caret.loo &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl3,tuneGrid=KK) caret.loo$bestTune On remarque que le temps de calcul ets beaucoup plus long qu’avec la fonction knn.cv. Cela vient du fait que train recalcule l’algorithme des kppv \\(n\\) fois tandis que knn.cv utilise une astuce matricielle pour faire la validation croisée leave-one-out. Heureusement, on a quand même le même résultat : caret.loo$bestTune k 7 121 Faire de même pour la validation croisée 10 blocs en gardant les mêmes blocs que dans la partie précédente. ctrl4 &lt;- trainControl(method=&quot;cv&quot;,index=blocs) caret.cv &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl4,tuneGrid=KK) Là encore, on retrouve bien la même valeur : caret.cv$bestTune k 6 101 1.4 Compléments 1.4.1 Calcul parallèle Les validations croisées peuvent se révéler coûteuses en temps de calcul. On utilise souvent des techniques de parallélisation pour améliorer les performances computationnelles. Ces techniques sont relativement facile à mettre en œuvre avec caret, on peut par exemple utiliser la librairie doParallel pour utiliser plusieurs cœurs de la machine. On compare les temps de calculs pour une même validation croisée 10 blocs exécutée avec 1 cœur et 4 cœurs : library(doParallel) ctrl4 &lt;- trainControl(method=&quot;cv&quot;,index=blocs) cl &lt;- makePSOCKcluster(1) registerDoParallel(cl) temps1 &lt;- system.time(ee3 &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl4,tuneGrid=KK)) stopCluster(cl) cl &lt;- makePSOCKcluster(4) registerDoParallel(cl) temps4 &lt;- system.time(ee3 &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl4,tuneGrid=KK)) stopCluster(cl) On compare ces deux temps de calcul temps1 user system elapsed 12.985 0.062 13.124 temps4 user system elapsed 0.625 0.018 5.935 Sans surprise, l’exécution est beaucoup plus rapide avec 4 cœurs. 1.4.2 Répéter les méthodes de rééchantillonnage Les méthodes d’estimation du risque présentées dans cette partie (hold out, validation croisée) sont basées sur du rééchantillonnage. Elles peuvent se révéler sensible à la manière de couper l’échantillon. C’est pourquoi il est recommandé de les répéter plusieurs fois et de moyenner les erreurs sur les répétitions. Ces répétitions sont très faciles à mettre en œuvre avec caret, par exemple pour la validation hold out on utilise l’option number: ctrl &lt;- trainControl(method=&quot;LGOCV&quot;,number=5) caret.ho.rep &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl,tuneGrid=KK) la validation croisée on utilise les options repeatedcv et repeats : ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;,repeats=5) caret.ho.rep &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl,tuneGrid=KK) 1.4.3 Modifier le risque Enfin nous avons uniquement considéré l’erreur de classification. Il est bien entendu possible d’utiliser d’autres risques pour évaluer les performances. C’est l’option metric de la fonction train qui permet généralement de spécifier le risque, si on est par exemple intéressé par l’aire sur la courbe ROC (AUC) on fera : donnees1 &lt;- donnees names(donnees1)[3] &lt;- &quot;Class&quot; levels(donnees1$Class) &lt;- c(&quot;G0&quot;,&quot;G1&quot;) ctrl &lt;- trainControl(method=&quot;LGOCV&quot;,number=1,classProbs=TRUE,summary=twoClassSummary) caret.auc &lt;- train(Class~.,data=donnees1,method=&quot;knn&quot;,trControl=ctrl,tuneGrid=KK,metric=&quot;ROC&quot;) On obtient ici pour chaque valeur de \\(k\\), l’AUC ainsi que les sensibilité et spécificité : caret.auc k-Nearest Neighbors 2000 samples 2 predictor 2 classes: &#39;G0&#39;, &#39;G1&#39; No pre-processing Resampling: Repeated Train/Test Splits Estimated (1 reps, 75%) Summary of sample sizes: 1500 Resampling results across tuning parameters: k ROC Sens Spec 1 0.6338315 0.5937500 0.6739130 21 0.7488031 0.6473214 0.8152174 41 0.7599072 0.6696429 0.8115942 61 0.7554590 0.6785714 0.8188406 81 0.7586698 0.6875000 0.8224638 101 0.7628025 0.6785714 0.8333333 121 0.7603196 0.6830357 0.8333333 141 0.7605703 0.6830357 0.8297101 161 0.7625194 0.6875000 0.8224638 181 0.7616945 0.6875000 0.8188406 201 0.7609990 0.6830357 0.8188406 221 0.7582411 0.6696429 0.8188406 241 0.7567854 0.6607143 0.8224638 261 0.7563406 0.6473214 0.8188406 281 0.7546260 0.6383929 0.8297101 301 0.7530328 0.6294643 0.8333333 321 0.7554914 0.6205357 0.8297101 341 0.7530166 0.6205357 0.8369565 361 0.7518925 0.5848214 0.8405797 381 0.7500970 0.5357143 0.8550725 401 0.7472179 0.5133929 0.8586957 421 0.7472907 0.4866071 0.8695652 441 0.7432550 0.4732143 0.8768116 461 0.7429720 0.4598214 0.8840580 481 0.7404487 0.4241071 0.8876812 ROC was used to select the optimal model using the largest value. The final value used for the model was k = 101. Et on choisira la valeur de \\(k\\) qui maximise l’AUC : caret.auc$bestTune k 6 101 "],["SVM.html", "Chapitre 2 Support Vector Machine (SVM) 2.1 Cas séparable 2.2 Cas non séparable 2.3 L’astuce du noyau 2.4 Support vector régression 2.5 SVM sur les données spam 2.6 Exercices", " Chapitre 2 Support Vector Machine (SVM) Etant donnée un échantillon \\((x_1,y_1),\\dots,(x_n,y_n)\\) où les \\(x_i\\) sont à valeurs dans \\(\\mathbb R^p\\) et les \\(y_i\\) sont binaires à valeurs dans \\(\\{-1,1\\}\\), l’approche SVM cherche le meilleur hyperplan en terme de séparation des données. Globalement on veut que les 1 se trouvent d’un coté de l’hyperplan et les -1 de l’autre. Dans cette partie on propose d’étudier la mise en œuvre de cet algorithme tout d’abord dans le cas idéal où les données sont séparables puis dans le cas plus réel où elles ne le sont pas. Nous verrons ensuite comment introduire de la non linéarité ne utilisant l’astuce du noyau. 2.1 Cas séparable Le cas séparable est le cas facile : il correspond à la situation où il existe effectivement un (même plusieurs) hyperplan(s) qui sépare(nt) parfaitement les 1 des -1. Il ne se produit quasiment jamais en pratique mais il convient de l’étudier pour comprendre comment est construit l’algorithme. Dans ce cas on cherche l’hyperplan d’équation \\(\\langle w,x\\rangle+b=w^tx+b=0\\) tel que la marge (qui peut être vue comme la distance entre les observations les plus proches de l’hyperplan et l’hyperplan) soit maximale. Mathématiquement le problème se réécrit comme un problème d’optimisation sous contraintes : \\[\\begin{equation} \\min_{w,b}\\frac{1}{2}\\|w\\|^2 \\tag{2.1} \\end{equation}\\] \\[\\text{sous les contraintes } y_i(w^tx_i+b)\\geq 1,\\ i=1,\\dots,n.\\] La solution s’obtient de façon classique en résolvant le problème dual et elle s’écrit comme une combinaison linéaire des \\(x_i\\) \\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\] De plus, les conditions KKT impliquent que pour tout \\(i=1,\\dots,n\\): \\(\\alpha_i^\\star=0\\) ou \\(y_i(x_i^tw+b)-1=0.\\) Ces conditions impliquent que \\(w^\\star\\) s’écrit comme une combinaison linéaire de quelques points, appelés vecteurs supports qui se trouvent sur la marge. Nous proposons maintenant de retrouver ces points et de tracer la marge sur un exemple simple. On considère le nuage de points suivant : n &lt;- 20 set.seed(123) X1 &lt;- scale(runif(n)) set.seed(567) X2 &lt;- scale(runif(n)) Y &lt;- rep(-1,n) Y[X1&gt;X2] &lt;- 1 Y &lt;- as.factor(Y) donnees &lt;- data.frame(X1=X1,X2=X2,Y=Y) p &lt;- ggplot(donnees)+aes(x=X2,y=X1,color=Y)+geom_point() p La fonction svm du package e1071 permet d’ajuster une SVM : library(e1071) mod.svm &lt;- svm(Y~.,data=donnees,kernel=&quot;linear&quot;,cost=10000000000) Récupérer les vecteurs supports et visualiser les sur le graphe (en utilisant une autre couleur par exemple). On les affectera à un data.frame dont les 2 premières colonnes représenteront les valeurs de \\(X_1\\) et \\(X_2\\) des vecteurs supports. Les vecteurs supports se trouvent dans la sortie index de la fonction svm : ind.svm &lt;- mod.svm$index sv &lt;- donnees %&gt;% slice(ind.svm) sv X1 X2 Y 1 -1.61179777 -0.6599042 -1 2 0.06962369 0.7140262 -1 3 -0.31095135 -0.5332139 1 p1 &lt;- p+geom_point(data=sv,aes(x=X2,y=X1),color=&quot;blue&quot;,size=2) On peut ainsi représenter la marge en traçant les droites qui passent par ces points. sv1 &lt;- sv[,2:1] b &lt;- (sv1[1,2]-sv1[2,2])/(sv1[1,1]-sv1[2,1]) a &lt;- sv1[1,2]-b*sv1[1,1] a1 &lt;- sv1[3,2]-b*sv1[3,1] p1+geom_abline(intercept = c(a,a1),slope=b,col=&quot;blue&quot;,size=1) Retrouver ce graphe à l’aide de la fonction plot. plot(mod.svm,data=donnees,grid=250) Rappeler la règle de décision associée à la méthode SVM. Donner les estimations des paramètres de la règle de décision sur cet exemple. On pourra notamment regarder la sortie coef de la fonction svm. Une fois \\(w^\\star\\) et \\(b^\\star\\) obtenus, la règle s’écrit \\[g(x)=1_{\\langle w^\\star,x\\rangle+b^\\star\\leq 0}-1_{\\langle w^\\star,x\\rangle+b^\\star&gt;0}.\\] L’objet mod.svm$coefs contient les coefficients \\(\\alpha_i^\\star y_i\\) pour chaque vecteur support. On peut ainsi récupérer l’équation de l’hyperplan et faire la prévision avec w &lt;- apply(mod.svm$coefs*donnees[mod.svm$index,1:2],2,sum) #ou w &lt;- t(mod.svm$coefs) %*% mod.svm$SV b &lt;- -mod.svm$rho b [1] -0.4035113 L’hyperplan séparateur a donc pour équation : \\[-1.74x_1+2.12x_2-0.40=0.\\] On dispose d’un nouvel individu \\(x=(-0.5,0.5)\\). Expliquer comment on peut prédire son groupe. Il suffit de calculer \\(\\langle w^\\star,x\\rangle+b\\) et de prédire en fonction du signe de cette valeur : newX &lt;- data.frame(X1=-0.5,X2=0.5) sum(w*newX)+b [1] 1.537053 On prédira le groupe -1 pour ce nouvel individu. Retrouver les résultats de la question précédente à l’aide de la fonction predict. On pourra utiliser l’option decision.values = TRUE. predict(mod.svm,newX,decision.values = TRUE) 1 -1 attr(,&quot;decision.values&quot;) -1/1 1 1.537053 Levels: -1 1 Plus cette valeur est élevée, plus on est loin de l’hyperplan. On peut donc l’interpréter comme un score. Obtenir les probabilités prédites à l’aide de la fonction predict. On pourra utiliser probability=TRUE dans la fonction svm. mod.svm1 &lt;- svm(Y~.,data=donnees,kernel=&quot;linear&quot;,cost=10000000000,probability=TRUE) predict(mod.svm1,newX,decision.values=TRUE,probability=TRUE) 1 -1 attr(,&quot;decision.values&quot;) -1/1 1 1.537053 attr(,&quot;probabilities&quot;) -1 1 1 0.8294474 0.1705526 Levels: -1 1 Comme souvent, il est possible d’obtenir une estimation des probabilités d’être dans les groupes -1 et 1 à partir du score, il “suffit” de ramener ce score sur l’échelle \\([0,1]\\) avec des transformations de type logit par exemple. Pour la svm, ces probabilités sont obtenues en ajustant un modèle logistique sur les scores \\(S(x)\\) : \\[P(Y=1|X=x)=\\frac{1}{1+\\exp(aS(x)+b)}.\\] On peut retrouver ces probabilités avec : score.newX &lt;- sum(w*newX)+b 1/(1+exp(-(mod.svm1$probB+mod.svm1$probA*score.newX))) [1] 0.1705526 2.2 Cas non séparable Dans la vraie vie, les groupes ne sont généralement pas séparables et il n’existe donc pas de solution au problème (2.1). On va donc autoriser certains points à être : mal classés et/ou bien classés mais à l’intérieur de la marge. Mathématiquement, cela revient à introduire des variables ressorts (slacks variables) \\(\\xi_1,\\dots,\\xi_n\\) positives telles que : \\(\\xi_i\\in [0,1]\\Longrightarrow\\) \\(i\\) bien classé mais dans la région définie par la marge ; \\(\\xi_i&gt;1 \\Longrightarrow\\) \\(i\\) mal classé. Le problème d’optimisation est alors de minimiser en \\((w,b,\\xi)\\) \\[\\frac{1}{2}\\|w\\|^2 +C\\sum_{i=1}^n\\xi_i\\] \\[\\textrm{sous les contraintes } \\left\\{ \\begin{array}{l} y_i(w^tx_i+b)\\geq 1 -\\xi_i \\\\ \\xi_i\\geq 0, i=1,\\dots,n. \\end{array}\\right.\\] Le paramètre \\(C&gt;0\\) est à calibrer et on remarque que le cas séparable correspond à \\(C\\to +\\infty\\). Les solutions de ce nouveau problème d’optimisation s’obtiennent de la même façon que dans le cas séparable, en particulier \\(w^\\star\\) s’écrite toujours comme une combinaison linéaire \\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\] de vecteurs supports sauf qu’on distingue deux types de vecteurs supports (\\(\\alpha_i^\\star&gt;0\\)): ceux sur la frontière définie par la marge : \\(\\xi_i^\\star=0\\) ; ceux en dehors : \\(\\xi_i^\\star&gt;0\\) et \\(\\alpha_i^\\star=C\\). Le choix de \\(C\\) est crucial : ce paramètre régule le compromis biais/variance de la svm : \\(C\\searrow\\): la marge est privilégiée et les \\(\\xi_i\\nearrow\\) \\(\\Longrightarrow\\) beaucoup d’observations dans la marge ou mal classées (et donc beaucoup de vecteurs supports). \\(C\\nearrow\\Longrightarrow\\) \\(\\xi_i\\searrow\\) donc moins d’observations mal classées \\(\\Longrightarrow\\) meilleur ajustement mais petite marge \\(\\Longrightarrow\\) risque de surajustement. On choisit généralement ce paramètre à l’aide des techiques présentées dans le chapitre 1 : choix d’une grille de valeurs de \\(C\\) et d’un critère ; choix d’une méthode de ré-échantillonnage pour estimer le critère ; choix de la valeur de \\(C\\) qui minimise le critère estimé. On considère le jeu de données df3 définie ci-dessous. n &lt;- 1000 set.seed(1234) df &lt;- as.data.frame(matrix(runif(2*n),ncol=2)) df1 &lt;- df %&gt;% filter(V1&lt;=V2)%&gt;% mutate(Y=rbinom(nrow(.),1,0.95)) df2 &lt;- df %&gt;% filter(V1&gt;V2)%&gt;% mutate(Y=rbinom(nrow(.),1,0.05)) df3 &lt;- bind_rows(df1,df2) %&gt;% mutate(Y=as.factor(Y)) ggplot(df3)+aes(x=V2,y=V1,color=Y)+geom_point()+ scale_color_manual(values=c(&quot;#FFFFC8&quot;, &quot;#7D0025&quot;))+ theme(panel.background = element_rect(fill = &quot;#BFD5E3&quot;, colour = &quot;#6D9EC1&quot;,size = 2, linetype = &quot;solid&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Ajuster 3 svm en considérant comme valeur de \\(C\\) : 0.000001, 0.1 et 5. On pourra utiliser l’option cost. mod.svm1 &lt;- svm(Y~.,data=df3,kernel=&quot;linear&quot;,cost=0.000001) mod.svm2 &lt;- svm(Y~.,data=df3,kernel=&quot;linear&quot;,cost=0.1) mod.svm3 &lt;- svm(Y~.,data=df3,kernel=&quot;linear&quot;,cost=5) Calculer les nombres de vecteurs supports pour chaque valeur de \\(C\\). mod.svm1$nSV [1] 469 469 mod.svm2$nSV [1] 178 178 mod.svm3$nSV [1] 150 150 Visualiser les 3 svm obtenues. Interpréter. plot(mod.svm1,data=df3,grid=250) plot(mod.svm2,data=df3,grid=250) plot(mod.svm3,data=df3,grid=250) Pour \\(C\\) petit, toutes les observations sont classées 0, la marge est grande et le nombre de vecteurs supports importants. On remarque que ces deux quantités diminuent lorsque \\(C\\) augmente. 2.3 L’astuce du noyau Les SVM présentées précédemment font l’hypothèse que les groupes sont linéairement séparables, ce qui n’est bien entendu pas toujours le cas en pratique. L’astuce du noyau permet de mettre de la non linéarité, elle consiste à : plonger les données dans un nouvel espace appelé espace de représentation ou feature space ; appliquer une svm linéaire dans ce nouvel espace. Le terme astuce vient du fait que ce procédé ne nécessite pas de connaître explicitement ce nouvel espace : pour résoudre le problème d’optimisation dans le feature space on a juste besoin de connaître le noyau associé au feature space. D’un point de vu formel un noyau est une fonction \\[K:\\mathcal X\\times\\mathcal X\\to\\mathbb R\\] dont les propriétés sont proches d’un produit scalaire. Il existe donc tout un tas de noyau avec lesquels on peut faire des SVM, par exemple Linéaire (sur \\(\\mathbb R^d\\)) : \\(K(x,x&#39;)=x^tx&#39;\\). Polynomial (sur \\(\\mathbb R^d\\)) : \\(K(x,x&#39;)=(x^tx&#39;+1)^d\\). Gaussien (Gaussian radial basis function ou RBF) (sur \\(\\mathbb R^d\\)) \\[K(x,x&#39;)=\\exp\\left(-\\frac{\\|x-x&#39;\\|}{2\\sigma^2}\\right).\\] Laplace (sur \\(\\mathbb R\\)) : \\(K(x,x&#39;)=\\exp(-\\gamma|x-x&#39;|)\\). Noyau min (sur \\(\\mathbb R^+\\)) : \\(K(x,x&#39;)=\\min(x,x&#39;)\\). … Bien entendu, en pratique tout le problème va consister à trouver le bon noyau ! On considère le jeu de données suivant où le problème est d’expliquer \\(Y\\) par \\(V1\\) et \\(V2\\). n &lt;- 500 set.seed(13) X &lt;- matrix(runif(n*2,-2,2),ncol=2) %&gt;% as.data.frame() Y &lt;- rep(0,n) cond &lt;- (X$V1^2+X$V2^2)&lt;=2.8 Y[cond] &lt;- rbinom(sum(cond),1,0.9) Y[!cond] &lt;- rbinom(sum(!cond),1,0.1) df &lt;- X %&gt;% mutate(Y=as.factor(Y)) ggplot(df)+aes(x=V2,y=V1,color=Y)+geom_point()+theme_classic() Ajuster une svm linéaire et visualiser l’hyperplan séparateur. Que remarquez-vous ? mod.svm0 &lt;- svm(Y~.,data=df,kernel=&quot;linear&quot;,cost=1) plot(mod.svm0,df,grid=250) La svm linéaire ne permet pas de séparer les groupes (on pouvait s’y attendre). Exécuter la commande suivante et commenter la sortie. mod.svm1 &lt;- svm(Y~.,data=df,kernel=&quot;radial&quot;,gamma=1,cost=1) plot(mod.svm1,df,grid=250) Le noyau radial permet de mettre en évidence une séparation non linéaire. Faire varier les paramètres gamma et cost. Interpréter (on pourra notamment étudier l’évolution du nombre de vecteurs supports en fonction du paramètre cost). mod.svm2 &lt;- svm(Y~.,data=df,kernel=&quot;radial&quot;,gamma=1,cost=0.0001) mod.svm3 &lt;- svm(Y~.,data=df,kernel=&quot;radial&quot;,gamma=1,cost=1) mod.svm4 &lt;- svm(Y~.,data=df,kernel=&quot;radial&quot;,gamma=1,cost=100000) plot(mod.svm2,df,grid=250) plot(mod.svm3,df,grid=250) plot(mod.svm4,df,grid=250) mod.svm2$nSV [1] 244 244 mod.svm3$nSV [1] 114 114 mod.svm4$nSV [1] 78 77 Le nombre de vecteurs supports diminue lorsque \\(C\\) augmente. Une forte valeur de \\(C\\) autorise moins d’observations à être dans la marge, elle a donc tendance à diminuer (risque de surapprentissage). Sélectionner automatiquement ces paramètres. On pourra utiliser la fonction tune en faisant varier C dans c(0.1,1,10,100,1000) et gamma dans c(0.5,1,2,3,4). set.seed(1234) tune.out &lt;- tune(svm,Y~.,data=df,kernel=&quot;radial&quot;, ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4))) summary(tune.out) Parameter tuning of &#39;svm&#39;: - sampling method: 10-fold cross validation - best parameters: cost gamma 10 0.5 - best performance: 0.108 - Detailed performance results: cost gamma error dispersion 1 1e-01 0.5 0.182 0.04565572 2 1e+00 0.5 0.148 0.03155243 3 1e+01 0.5 0.108 0.03425395 4 1e+02 0.5 0.116 0.03373096 5 1e+03 0.5 0.112 0.03425395 6 1e-01 1.0 0.184 0.04402020 7 1e+00 1.0 0.120 0.03651484 8 1e+01 1.0 0.120 0.03126944 9 1e+02 1.0 0.112 0.03155243 10 1e+03 1.0 0.120 0.03887301 11 1e-01 2.0 0.170 0.04136558 12 1e+00 2.0 0.124 0.02458545 13 1e+01 2.0 0.122 0.03457681 14 1e+02 2.0 0.124 0.03502380 15 1e+03 2.0 0.142 0.03705851 16 1e-01 3.0 0.160 0.03651484 17 1e+00 3.0 0.124 0.02458545 18 1e+01 3.0 0.126 0.03134042 19 1e+02 3.0 0.132 0.04022161 20 1e+03 3.0 0.166 0.03272783 21 1e-01 4.0 0.154 0.03777124 22 1e+00 4.0 0.124 0.02458545 23 1e+01 4.0 0.126 0.03134042 24 1e+02 4.0 0.138 0.04467164 25 1e+03 4.0 0.190 0.05754226 La sélection est faite en minimisant l’erreur de classification par validation croisée 10 blocs. Faire de même avec caret, on utilisera method=“svmRadial” et prob.model=TRUE. C &lt;- c(0.001,0.01,1,10,100,1000) sigma &lt;- c(0.5,1,2,3,4) gr &lt;- expand.grid(C=C,sigma=sigma) ctrl &lt;- trainControl(method=&quot;cv&quot;) res.caret1 &lt;- train(Y~.,data=df,method=&quot;svmRadial&quot;,trControl=ctrl,tuneGrid=gr,prob.model=TRUE) res.caret1 Support Vector Machines with Radial Basis Function Kernel 500 samples 2 predictor 2 classes: &#39;0&#39;, &#39;1&#39; No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 449, 450, 451, 450, 449, 450, ... Resampling results across tuning parameters: C sigma Accuracy Kappa 1e-03 0.5 0.8359976 0.6734429 1e-03 1.0 0.8439584 0.6890006 1e-03 2.0 0.8398359 0.6806352 1e-03 3.0 0.8578816 0.7164180 1e-03 4.0 0.8577623 0.7162786 1e-02 0.5 0.8400384 0.6813511 1e-02 1.0 0.8419584 0.6851382 1e-02 2.0 0.8458784 0.6926478 1e-02 3.0 0.8518407 0.7044624 1e-02 4.0 0.8577623 0.7162786 1e+00 0.5 0.8676871 0.7347434 1e+00 1.0 0.8857719 0.7713024 1e+00 2.0 0.8838127 0.7672737 1e+00 3.0 0.8798519 0.7594972 1e+00 4.0 0.8838928 0.7675507 1e+01 0.5 0.8798095 0.7596483 1e+01 1.0 0.8818111 0.7634823 1e+01 2.0 0.8838928 0.7676862 1e+01 3.0 0.8778503 0.7554248 1e+01 4.0 0.8720480 0.7438434 1e+02 0.5 0.8818111 0.7635668 1e+02 1.0 0.8839320 0.7677447 1e+02 2.0 0.8680480 0.7359331 1e+02 3.0 0.8517231 0.7031601 1e+02 4.0 0.8377591 0.6749363 1e+03 0.5 0.8760088 0.7521217 1e+03 1.0 0.8760496 0.7520939 1e+03 2.0 0.8498816 0.6998254 1e+03 3.0 0.8297967 0.6590033 1e+03 4.0 0.8100352 0.6192088 Accuracy was used to select the optimal model using the largest value. The final values used for the model were sigma = 1 and C = 1. On peut également répéter plusieurs fois la validation croisée pour stabiliser les résultats (on parallélise avec doParallel) : library(doParallel) ## pour paralléliser cl &lt;- makePSOCKcluster(4) registerDoParallel(cl) set.seed(12345) ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;,number=10,repeats=5) res.caret2 &lt;- train(Y~.,data=df,method=&quot;svmRadial&quot;,trControl=ctrl,tuneGrid=gr,prob.model=TRUE) on.exit(stopCluster(cl)) res.caret2 res.caret2 Support Vector Machines with Radial Basis Function Kernel 500 samples 2 predictor 2 classes: &#39;0&#39;, &#39;1&#39; No pre-processing Resampling: Cross-Validated (10 fold, repeated 5 times) Summary of sample sizes: 450, 449, 450, 451, 450, 449, ... Resampling results across tuning parameters: C sigma Accuracy Kappa 1e-03 0.5 0.8222641 0.6462662 1e-03 1.0 0.8458598 0.6928018 1e-03 2.0 0.8507000 0.7022920 1e-03 3.0 0.8555163 0.7118454 1e-03 4.0 0.8607898 0.7222353 1e-02 0.5 0.8242566 0.6502516 1e-02 1.0 0.8462519 0.6935931 1e-02 2.0 0.8499238 0.7007974 1e-02 3.0 0.8543078 0.7094622 1e-02 4.0 0.8639097 0.7284572 1e+00 0.5 0.8640626 0.7275460 1e+00 1.0 0.8839933 0.7678344 1e+00 2.0 0.8843858 0.7685479 1e+00 3.0 0.8823531 0.7644392 1e+00 4.0 0.8799766 0.7596759 1e+01 0.5 0.8848178 0.7696785 1e+01 1.0 0.8803851 0.7606211 1e+01 2.0 0.8775757 0.7549666 1e+01 3.0 0.8751989 0.7501291 1e+01 4.0 0.8727989 0.7453460 1e+02 0.5 0.8815531 0.7631204 1e+02 1.0 0.8751107 0.7501217 1e+02 2.0 0.8743443 0.7484731 1e+02 3.0 0.8615653 0.7229593 1e+02 4.0 0.8507228 0.7011391 1e+03 0.5 0.8803600 0.7607328 1e+03 1.0 0.8731277 0.7462531 1e+03 2.0 0.8499715 0.7000011 1e+03 3.0 0.8319834 0.6640949 1e+03 4.0 0.8089513 0.6174340 Accuracy was used to select the optimal model using the largest value. The final values used for the model were sigma = 0.5 and C = 10. Visualiser la règle sélectionnée. caret utilise la fonction ksvm du package kernlab. Ce package propose un choix plus large pour les noyaux. Par conséquent, si on souhaite visualiser la svm sélectionnée par caret, il est préférable d’utiliser cette fonction. library(kernlab) C.opt &lt;- res.caret2$bestTune$C sigma.opt &lt;- res.caret2$bestTune$sigma svm.sel &lt;- ksvm(Y~.,data=df,kernel=&quot;rbfdot&quot;,kpar=list(sigma=sigma.opt),C=C.opt) plot(svm.sel,data=df) 2.4 Support vector régression Dans un contexte de régression (lorsque \\(y_i\\in\\mathbb R\\)), on ne recherche plus la l’hyperplan qui va séparer au mieux. On va dans ce cas là cherche à approcher au mieux les valeurs de \\(y_i\\). Cela revient à chercher \\(w\\in\\mathbb R^p\\) et \\(b\\in\\mathbb R\\) tels que \\[|\\langle w,x_i\\rangle+b-y_i|\\leq \\varepsilon\\] avec \\(\\varepsilon&gt;0\\) petit à choisir par l’utilisateur. Par analogie avec la SVM binaire, on va ainsi chercher \\((w,b)\\) qui minimisent \\[\\frac{1}{2}\\|w\\|^2\\] \\[\\textrm{sous les contraintes } |y_i-\\langle w,x_i\\rangle-b|\\leq \\varepsilon,\\ i=1,\\dots,n,\\] Les contraintes impliquent que toute les observations doivent se définir dans une marge ou bande de taille \\(2\\varepsilon\\). Cette hypothèse peut amener l’utilisateur à utiliser des valeurs de \\(\\varepsilon\\) très grandes et empêcher la solution de bien ajuster le nuage de points. Pour pallier à cela, on introduit, comme dans le cas de la SVM binaire, des variables ressorts qui vont autoriser certaines observations à se situer en dehors de la marge. Le problème revient alors à trouver \\((w,b,\\xi,\\xi^\\star)\\) qui minimise \\[\\frac{1}{2}\\|w\\|^2+C\\sum_{i=1}^n(\\xi_i+\\xi_i^\\star)\\] \\[\\textrm{sous les contraintes } \\left\\{ \\begin{array}{l} y_i-\\langle w,x_i\\rangle-b\\leq \\varepsilon+\\xi_i,\\ i=1,\\dots,n,\\\\ \\langle w,x_i\\rangle+b-y_i\\leq \\varepsilon+\\xi_i^\\star,\\ i=1,\\dots,n \\\\ \\xi_i\\geq 0,\\xi_i^\\star\\geq 0,\\ i=1,\\dots,n \\end{array}\\right. \\] Les solutions s’obtiennent exactement de la même façon que dans le cas binaire. On montre notamment que \\(w^\\star\\) s’écrit comme une combinaison linéaire de vecteurs supports : \\[w^\\star=\\sum_{i=1}^n(\\alpha_i^\\star-\\alpha_i)x_i.\\] Les vecteurs supports sont les observations vérifiant \\(\\alpha_i^\\star-\\alpha_i\\neq 0\\). Ici encore il faudra calibrer le paramètre \\(C\\) et on pourra utiliser l’astuce du noyau. On considère le nuage de points \\((x_i,y_i),i=1,\\dots,n\\) définie ci-dessous: set.seed(321) n &lt;- 30 X &lt;- runif(n) eps &lt;- rnorm(n,0,0.2) Y &lt;- 1+X+eps df &lt;- data.frame(X,Y) p1 &lt;- ggplot(df)+aes(x=X,y=Y)+geom_point() p1 On souhaite faire une SVR permettant de prédire \\(Y\\) par \\(X\\). On peut l’obtenir sur R toujours avec la fonction svm de e1071: svr1 &lt;- svm(Y~.,data=df,kernel=&quot;linear&quot;,epsilon=0.5,cost=100,scale=FALSE) On choisit ici exceptionnellement de ne pas réduire les \\(X\\). Écrire une fonction R qui, à partir d’un objet svm, calcule l’équation de la droite de la SVR. Cette fonction pourra également tracer cette doite ainsi que la marge. droite_svr &lt;- function(svr,df){ SV &lt;- df %&gt;% slice(svr$index) w &lt;- sum(svr$coefs*SV[,1]) b &lt;- -svr$rho p &lt;- ggplot(df) + aes(x=X,y=Y)+geom_point()+ geom_point(data=SV,color=&quot;red&quot;)+ geom_abline(slope=w,intercept=c(b))+ geom_abline(slope=w,intercept=c(b-svr$epsilon,b+svr$epsilon),color=&quot;red&quot;) return(list(w=w,b=b,graph=p)) } svr11 &lt;- droite_svr(svr1,df) svr11$graph Comparer la SVR précédente avec celle utilisant epsilon=0.7. svr2 &lt;- svm(Y~.,data=df,kernel=&quot;linear&quot;,epsilon=0.7,cost=100,scale=FALSE) svr22 &lt;- droite_svr(svr2,df) svr22$graph La droite traverse moins bien le nuage de points pour cette valeur de \\(\\varepsilon\\) qui semble trop grande. On ajoute le point de coordonnées \\((0.05,3)\\) aux données. Discuter de la SVR pour ce nouveau jeu de données en utilisant plusieurs valeurs pour C et epsilon. df1 &lt;- df %&gt;% bind_rows(data.frame(X=0.05,Y=3)) On commence par faire grandir la valeur de epsilon pour que toutes les observations soient dans la marge. eps3 &lt;- 0.5*(3-min(df1$Y))-0.01 svr3 &lt;- svm(Y~.,data=df1,kernel=&quot;linear&quot;,epsilon=eps3,cost=100,scale=FALSE) svr33 &lt;- droite_svr(svr3,df1) svr33$graph Toutes les observations sont bien dans la marge mais la règle n’est clairement pas pertinente. Il est préférable d’autoriser certaines observations à se situer en dehors de la marge, par exemple : svr4 &lt;- svm(Y~.,data=df1,kernel=&quot;linear&quot;,epsilon=0.5,cost=100,scale=FALSE) svr44 &lt;- droite_svr(svr4,df1) svr44$graph 2.5 SVM sur les données spam On considère le jeu de données spam où le problème est d’expliquer la variable type par les autres. data(spam) summary(spam$type) nonspam spam 2788 1813 On veut comparer plusieurs svm en utilisant le package kernlab. On pourra trouver un descriptif du package à cette adresse https://www.jstatsoft.org/article/view/v011i09. Utiliser la fonction ksvm pour faire une svm linéaire et une svm à noyau gaussien. On prendra comme paramètre 1 pour C et pour le paramètre du noyau gaussien. La svm linéaire correspond au noyau polynomial avec des valeurs de paramètres particulières : svm.lin &lt;- ksvm(type~.,data=spam,kernel=&quot;polydot&quot;,C=1,kpar=list(degree=1,scale=1,offset=0)) Pour le noyau gaussien, il suffit d’utiliser l’option kernel=“rbfdot” : svm.gauss &lt;- ksvm(type~.,data=spam,kernel=&quot;rbfdot&quot;,C=1,kpar=list(sigma=1)) Évaluer la performance des 2 svm précédentes en calculant l’erreur de classification par validation croisée 5 blocs. Comparer ces deux algorithmes. Il suffit d’utiliser l’option cross dans ksvm. svm.lin &lt;- ksvm(type~.,data=spam,kernel=&quot;polydot&quot;,C=1, kpar=list(degree=1,scale=1,offset=0),cross=5) svm.gauss &lt;- ksvm(type~.,data=spam,kernel=&quot;rbfdot&quot;,C=1, kpar=list(sigma=1),cross=5) svm.lin Support Vector Machine object of class &quot;ksvm&quot; SV type: C-svc (classification) parameter : cost C = 1 Polynomial kernel function. Hyperparameters : degree = 1 scale = 1 offset = 0 Number of Support Vectors : 942 Objective Function Value : -881.4942 Training error : 0.067377 Cross validation error : 0.071942 svm.gauss Support Vector Machine object of class &quot;ksvm&quot; SV type: C-svc (classification) parameter : cost C = 1 Gaussian Radial Basis kernel function. Hyperparameter : sigma = 1 Number of Support Vectors : 3881 Objective Function Value : -1457.267 Training error : 0.005868 Cross validation error : 0.199957 On remarque que l’erreur de classification est plus faible pour la svm linéaire. De plus, il y a un gros écart entre l’erreur de prévision et l’erreur d’ajustement pour le noyau gaussien, il est fort possible que l’on soit en sur-apprentissage avec ces valeurs de paramètres. Refaire la svm à noyau gaussien avec l’option kpar='automatic'. Expliquer. svm.gauss &lt;- ksvm(type~.,data=spam,kernel=&quot;rbfdot&quot;,C=1,kpar=&#39;automatic&#39;,cross=5) svm.gauss Support Vector Machine object of class &quot;ksvm&quot; SV type: C-svc (classification) parameter : cost C = 1 Gaussian Radial Basis kernel function. Hyperparameter : sigma = 0.0298786979455573 Number of Support Vectors : 1412 Objective Function Value : -807.0795 Training error : 0.045642 Cross validation error : 0.068461 Le paramètre du noyau est ici calibré à partir d’une heuristique. La valeur choisie semble pertinente puisque l’erreur de prévision a diminué et est maintenant proche de l’erreur d’ajustement. On s’intéresse maintenant à l’AUC. À partir de validation croisée, sélectionner un noyau (linéaire ou gaussien) ainsi que des valeurs de paramètres associés au noyau, sans oublier le paramètre C. On pourra utiliser le package caret et comparer le résultat obtenu à celui d’une forêt aléatoire. Il faut tout d’abord définir des grilles. On peut consulter la page https://topepo.github.io/caret/available-models.html pour identifier les identifiants des paramètres. On fait les choix suivants : C &lt;- c(0.01,0.1,1,10) degree &lt;- c(1,2,3) scale &lt;- 1 gr.lin &lt;- expand.grid(C=C,degree=degree,scale=scale) sigma &lt;- c(0.001,0.01,0.05,0.2,1) gr.gauss &lt;- expand.grid(C=C,sigma=sigma) spam1 &lt;- spam names(spam1)[ncol(spam)] &lt;- &quot;Class&quot; levels(spam1$Class) &lt;- c(&quot;G0&quot;,&quot;G1&quot;) ctrl &lt;- trainControl(method=&quot;cv&quot;,classProbs=TRUE,summary=twoClassSummary) cl &lt;- makePSOCKcluster(4) registerDoParallel(cl) set.seed(12345) lin.caret &lt;- train(Class~.,data=spam1,method=&quot;svmPoly&quot;,trControl=ctrl, tuneGrid=gr.lin,prob.model=TRUE,metric=&quot;ROC&quot;) gauss.caret &lt;- train(Class~.,data=spam1,method=&quot;svmRadial&quot;,trControl=ctrl, tuneGrid=gr.gauss,prob.model=TRUE,metric=&quot;ROC&quot;) on.exit(stopCluster(cl)) On obtient les erreurs suivantes : lin.caret Support Vector Machines with Polynomial Kernel 4601 samples 57 predictor 2 classes: &#39;G0&#39;, &#39;G1&#39; No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 4141, 4141, 4141, 4142, 4141, 4141, ... Resampling results across tuning parameters: C degree ROC Sens Spec 0.01 1 0.9666560 0.9523014 0.8731316 0.01 2 0.9664432 0.9594814 0.8764495 0.01 3 0.9521303 0.9701243 0.7221480 0.10 1 0.9704662 0.9547169 0.8813900 0.10 2 0.9550894 0.9569686 0.8588155 0.10 3 0.9461588 0.9605620 0.7144308 1.00 1 0.9717055 0.9576842 0.8836045 1.00 2 0.9432383 0.9565154 0.7448356 1.00 3 0.9362428 0.9687605 0.4081459 10.00 1 0.9719611 0.9569648 0.8902313 10.00 2 0.9428121 0.9569796 0.7255441 10.00 3 0.9295425 0.9753488 0.3673047 Tuning parameter &#39;scale&#39; was held constant at a value of 1 ROC was used to select the optimal model using the largest value. The final values used for the model were degree = 1, scale = 1 and C = 10. gauss.caret Support Vector Machines with Radial Basis Function Kernel 4601 samples 57 predictor 2 classes: &#39;G0&#39;, &#39;G1&#39; No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 4141, 4141, 4141, 4140, 4141, 4142, ... Resampling results across tuning parameters: C sigma ROC Sens Spec 0.01 0.001 0.9421579 0.8575772 0.9100965 0.01 0.010 0.9497898 0.8468116 0.9205634 0.01 0.050 0.9382280 0.8687050 0.8935371 0.01 0.200 0.9400169 0.7754442 0.9503612 0.01 1.000 0.9438293 0.9906771 0.5730678 0.10 0.001 0.9472419 0.9257484 0.8770050 0.10 0.010 0.9630276 0.9497808 0.8742305 0.10 0.050 0.9581472 0.9472641 0.8438953 0.10 0.200 0.9445127 0.9235966 0.8378696 0.10 1.000 0.9441026 0.9913940 0.5708609 1.00 0.001 0.9619480 0.9465537 0.8753445 1.00 0.010 0.9735428 0.9583868 0.8952007 1.00 0.050 0.9732358 0.9562389 0.8896910 1.00 0.200 0.9604489 0.9637774 0.7915366 1.00 1.000 0.9443214 0.9939055 0.5543136 10.00 0.001 0.9699282 0.9558766 0.8929937 10.00 0.010 0.9795221 0.9609010 0.9029264 10.00 0.050 0.9711805 0.9548052 0.8896940 10.00 0.200 0.9594052 0.9630579 0.7876723 10.00 1.000 0.9425738 0.9881695 0.5543106 ROC was used to select the optimal model using the largest value. The final values used for the model were sigma = 0.01 and C = 10. et les valeurs de paramètres sélectionnés pour chaque SVM : lin.caret$bestTune degree scale C 10 1 1 10 gauss.caret$bestTune sigma C 17 0.01 10 On remarque que les AUC associés à ces deux jeux de paramètres sont très proches. On compare à une forêt aléatoire avec les paramètres par défaut : gr.foret &lt;- expand.grid(data.frame(mtry=7)) set.seed(123) foret.caret &lt;- train(Class~.,data=spam1,method=&quot;rf&quot;,trControl=ctrl, tuneGrid=gr.foret,metric=&quot;ROC&quot;) foret.caret Random Forest 4601 samples 57 predictor 2 classes: &#39;G0&#39;, &#39;G1&#39; No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 4141, 4140, 4141, 4141, 4140, 4141, ... Resampling results: ROC Sens Spec 0.9870745 0.971312 0.9260701 Tuning parameter &#39;mtry&#39; was held constant at a value of 7 La forêt aléatoire est (légèrement) plus pertinente en terme d’AUC. 2.6 Exercices Exercice 2.1 (Résolution du problème d’optimisation dans le cas séparable) On considère \\(n\\) observations \\((x_1,y_1),\\dots,(x_n,y_n)\\) telles que \\((x_i,y_i)\\in\\mathbb R^p\\times\\{-1,1\\}\\). On cherche à expliquer la variable \\(Y\\) par \\(X\\). On considère l’algorithme SVM et on se place dans le cas où les données sont séparables. Soit \\(\\mathcal H\\) un hyperplan séparateur d’équation \\(\\langle w,x\\rangle+b=0\\) où \\(w\\in\\mathbb R^p,b\\in\\mathbb R\\). Exprimer la distance entre \\(x_i,i=1,\\dots,n\\) et \\(\\mathcal H\\) en fonction de \\(w\\) et \\(b\\). Soit \\(x_0\\in\\mathcal H\\). La solution correspond à la norme du projeté orthogonal de \\(x-x_0\\) sur \\(\\mathcal H\\), elle est donc colinéaire à \\(w\\) (car \\(w\\) est normal à \\(\\mathcal H\\)) et s’écrit \\[\\frac{\\langle x-x_0,w\\rangle}{\\|w\\|^2}w=\\frac{\\langle x,w\\rangle}{\\|w\\|^2}w-\\frac{\\langle x_0,w\\rangle}{\\|w\\|^2}w,\\] Comme \\(\\langle x_0,w\\rangle=-b\\), on déduit que, si \\(\\|w\\|=1\\), alors \\[d_{\\mathcal H}(x)=\\frac{|\\langle w,x\\rangle+b|}{\\|w\\|}=|\\langle w,x\\rangle+b|=(\\langle w,x\\rangle+b)y\\] si \\(y=\\text{signe}(\\langle w,x\\rangle+b)\\). Expliquer la logique du problème d’optimisation \\[\\max_{w,b,\\|w\\|=1}M\\] \\[\\textrm{sous les contraintes } y_i(\\langle w,x_i\\rangle+b)\\geq M,\\ i=1,\\dots,n.\\] Si \\((w,b)\\) est un hyperplan séparateur sa marge vaut \\[\\min_{i=1,\\dots,n}y_i(\\langle w,x_i\\rangle+b).\\] Le problème proposé revient donc à chercher l’hyperplan : qui sépare les groupes ; tel que la distance entre les observations et lui soit maximale. Montrer que ce problème peut se réécrire \\[\\min_{w,b}\\frac{1}{2}\\|w\\|^2\\] \\[\\textrm{sous les contraintes } y_i(\\langle w,x_i\\rangle+b)\\geq 1,\\ i=1,\\dots,n.\\] Il suffit de poser comme contrainte \\(M=1/\\|w\\|\\). On rappelle que pour la minimisation d’une fonction \\(h:\\mathbb R^p\\to\\mathbb R\\) sous contraintes affines \\(g_i(u)\\geq 0,i=1,\\dots,n\\), le Lagrangien s’écrit \\[L(u,\\alpha)=h(u)-\\sum_{i=1}^n\\alpha_ig_i(u).\\] Si on désigne par \\(u_\\alpha=\\mathop{\\mathrm{argmin}}_uL(u,\\alpha)\\), la fonction duale est alors donnée par \\[\\theta(\\alpha)=L(u_\\alpha,\\alpha)=\\min_{u\\in\\mathbb R^p}L(u,\\alpha),\\] et le problème dual consiste à maximiser \\(\\theta(\\alpha)\\) sous les contraintes \\(\\alpha_i\\geq 0\\). En désignant par \\(\\alpha^\\star\\) la solution de ce problème, on déduit la solution du problème primal \\(u^\\star=u_{\\alpha^\\star}\\). Les conditions de Karush-Kuhn-Tucker sont données par \\(\\alpha_i^\\star\\geq 0\\). \\(g_i(u_{\\alpha^\\star})\\geq 0\\). \\(\\alpha_i^\\star g_i(u_{\\alpha^\\star})=0\\). Écrire le Lagrangien du problème considéré et en déduire une expression de \\(w\\) en fonction des \\(\\alpha_i\\) et des observations. Le lagrangien s’écrit \\[L(w,b;\\alpha)=\\frac{1}{2}\\|w\\|^2-\\sum_{i=1}^n\\alpha_i[y_i(\\langle w,x_i\\rangle+b)-1].\\] On a alors \\[\\frac{\\partial L(w,b;\\alpha)}{\\partial w}=w-\\sum_{i=1}^n\\alpha_iy_ix_i=0\\] et \\[\\frac{\\partial L(w,b;\\alpha)}{\\partial b}=-\\sum_{i=1}^n\\alpha_iy_i=0.\\] D’où \\(w_\\alpha=\\sum_{i=1}^n\\alpha_iy_ix_i\\). Écrire la fonction duale. La fonction duale s’écrit \\[\\begin{align*} \\theta(\\alpha)=L(w_\\alpha,b_\\alpha;\\alpha)= &amp;\\ \\frac{1}{2}\\langle \\sum_i\\alpha_iy_ix_i,\\sum_j\\alpha_jy_jx_j\\rangle-\\sum_i\\alpha_iy_i\\langle \\sum_j\\alpha_jy_jx_j,x_i\\rangle-\\sum_i\\alpha_iy_ib+\\sum_i\\alpha_i \\\\ = &amp;\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_j\\langle x_i,x_j\\rangle \\end{align*}\\] On note \\(\\alpha_i^*\\) les valeurs de \\(\\alpha_i\\) qui maximisent \\(\\theta(\\alpha)\\). Écrire les conditions KKT et en déduire les solutions \\(w^\\star\\) et \\(b^\\star\\). On peut déjà écrire \\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\] Les conditions KKT sont pour tout \\(i=1,\\dots,n\\) : \\[\\alpha_i^\\star\\geq 0 \\quad\\text{et}\\quad \\alpha_i^\\star[y_i(\\langle w^\\star,x_i\\rangle+b^\\star)-1]=0.\\] On obtient \\(b^\\star\\) en résolvant \\[\\alpha_i^\\star[y_i(\\langle w^\\star,x_i\\rangle+b^\\star)-1]=0\\] pour un \\(\\alpha_i^\\star\\) non nul. Interpréter les conditions KKT. Les \\(x_i\\) tels que \\(\\alpha_i^\\star&gt;0\\) vérifient \\[y_i(\\langle w^\\star,x_i\\rangle+b^\\star)=1.\\] Ils se situent donc sur la frontière définissant la marge maximale. Ce sont les vecteurs supports. Exercice 2.2 (Règle svm à partir de sorties R) On considère \\(n\\) observations \\((x_1,y_1),\\dots,(x_n,y_n)\\) telles que \\((x_i,y_i)\\in\\mathbb R^3\\times\\{-1,1\\}\\). On cherche à expliquer la variable \\(Y\\) par \\(X=(X_1,X_2,X_3)\\). On considère l’algorithme SVM et on se place dans le cas où les données sont séparables. On rappelle que cet algorithme consiste à chercher une droite d’équation \\(w^tx+b=0\\) où \\((w,b)\\in\\mathbb R^3\\times\\mathbb R\\) sont solutions du problème d’optimisation (problème primal) \\[\\min_{w,b}\\frac{1}{2}\\|w\\|^2\\] \\[\\textrm{sous les contraintes } y_i(w^tx_i+b)\\geq 1,\\ i=1,\\dots,n.\\] On désigne par \\(\\alpha_i^\\star,i=1,\\dots,n\\), les solutions du problème dual et par \\((w^\\star,b^\\star)\\) les solutions du problème ci-dessus. Donner la formule permettant de calculer \\(w^\\star\\) en fonction des \\(\\alpha_i^\\star\\). \\(w^\\star\\) se calcule selon \\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\] Les \\(\\alpha_i^\\star\\) étant nuls pour les vecteurs non supports, il suffit de sommer sur les vecteur supports. Expliquer comment on classe un nouveau point \\(x\\in\\mathbb R^3\\) par la méthode svm. Une fois \\(w^\\star\\) et \\(b^\\star\\) obtenus, la règle s’écrit \\[g(x)=1_{\\langle w^\\star,x\\rangle+b^\\star\\leq 0}-1_{\\langle w^\\star,x\\rangle+b^\\star&gt;0}.\\] Les données se trouvent dans un dataframe df. On exécute set.seed(1234) n &lt;- 100 X &lt;- data.frame(X1=runif(n),X2=runif(n),X3=runif(n)) X &lt;- data.frame(X1=scale(runif(n)),X2=scale(runif(n)),X3=scale(runif(n))) Y &lt;- rep(-1,100) Y[X[,1]&lt;X[,2]] &lt;- 1 #Y &lt;- (apply(X,1,sum)&lt;=0) %&gt;% as.numeric() %&gt;% as.factor() df &lt;- data.frame(X,Y=as.factor(Y)) mod.svm &lt;- svm(Y~.,data=df,kernel=&quot;linear&quot;,cost=10000000000) et on obtient df[mod.svm$index,] X1 X2 X3 Y 51 -1.1 -1.0 -1.0 1 92 0.7 0.8 1.1 1 31 0.7 0.5 -1.0 -1 37 -0.5 -0.6 0.3 -1 mod.svm$coefs [,1] [1,] 59 [2,] 49 [3,] -30 [4,] -79 mod.svm$rho [1] -0.5 Calculer les valeurs de \\(w^\\star\\) et \\(b^\\star\\). En déduire la règle de classification. \\(b^\\star\\) est l’opposé de mod.svm$rho. Pour \\(w^\\star\\) il suffit d’appliquer la formule et on trouve X1 X2 X3 -12.1 12.6 1.2 On dispose d’une nouvelle observation \\(x=(1,-0.5,-1)\\). Dans quel groupe (-1 ou 1) l’algorithme affecte cette nouvelle donnée ? On calcule la combinaison linéaire \\(\\langle w^\\star,x\\rangle+b^\\star\\) : newX &lt;- data.frame(X1=1,X2=-0.5,X3=-1) sum(w*newX)+b [1] -19.1 On affectera donc la nouvelle donnée au groupe -1. "],["arbres.html", "Chapitre 3 Arbres 3.1 Coupures CART en fonction de la nature des variables 3.2 Élagage", " Chapitre 3 Arbres Les méthodes par arbres sont des algorithmes où la prévision s’effectue à partir de moyennes locales. Plus précisément, étant donné un échantillon \\((x_1,y_1)\\dots,(x_n,y_n)\\), l’approche consiste à : construire une partition de l’espace de variables explicatives (\\(\\mathbb R^p\\)) ; prédire la sortie d’une nouvelle observation \\(x\\) en faisant : la moyenne des \\(y_i\\) pour les \\(x_i\\) qui sont dans la même classe que \\(x\\) si on est en régression ; un vote à la majorité parmi les \\(y_i\\) tels que les \\(x_i\\) qui sont dans la même classe que \\(x\\) si on est en classification. Bien entendu toute la difficulté est de trouver la “bonne partition” pour le problème d’intérêt. Il existe un grand nombre d’algorithmes qui permettent de trouver une partition. Le plus connu est l’algorithme CART (Breiman et al. 1984) où la partition est construite par divisions successives au moyen d’hyperplan orthogonaux aux axes de \\(\\mathbb R^p\\). L’algorithme est récursif : il va à chaque étape séparer un groupe d’observations (nœuds) en deux groupes (nœuds fils) en cherchant la meilleure variable et le meilleur seuil de coupure. Ce choix s’effectue à partir d’un critère d’impureté : la meilleure coupure est celle pour laquelle l’impureté des 2 nœuds fils sera minimale. Nous étudions cet algorithme dans cette partie. 3.1 Coupures CART en fonction de la nature des variables Une partition CART s’obtient en séparant les observations en 2 selon une coupure parallèle aux axes puis en itérant ce procédé de séparation binaire sur les deux groupes… Par conséquent la première question à se poser est : pour un ensemble de données \\((x_1,y_1),\\dots,(x_n,y_n)\\) fixé, comment obtenir la meilleure coupure ? Comme souvent ce sont les données qui vont répondre à cette question. La sélection de la meilleur coupure s’effectue en introduisant une fonction d’impureté \\(\\mathcal I\\) qui va mesurer le degrés d’hétérogénéité d’un nœud \\(\\mathcal N\\). Cette fonction prendra de grandes valeurs pour les nœuds hétérogènes (les valeurs de \\(Y\\) diffèrent à l’intérieur du nœud) ; faibles valeurs pour les nœuds homogènes (les valeurs de \\(Y\\) sont proches à l’intérieur du nœud). On utilise souvent comme fonction d’impureté : la variance en régression \\[\\mathcal I(\\mathcal N)=\\frac{1}{|\\mathcal N|}\\sum_{i:x_i\\in\\mathcal N}(y_i-\\overline{y}_\\mathcal N)^2,\\] où \\(\\overline{y} _\\mathcal N\\) désigne la moyenne des \\(y_i\\) dans \\(\\mathcal N\\). l’impureté de Gini en classification binaire \\[\\mathcal I(\\mathcal N)=2p(\\mathcal N)(1-p(\\mathcal N))\\] où \\(p(\\mathcal N)\\) représente la proportion de 1 dans \\(\\mathcal N\\). Les coupures considérées par l’algorithme CART sont des hyperplans orthogonaux aux axes de \\(\\mathbb R^p\\), choisir une coupure revient donc à choisir une variable \\(j\\) parmi les \\(p\\) variables explicatives et un seuil \\(s\\) dans \\(\\mathbb R\\). On peut donc représenter une coupure par un couple \\((j,s)\\). Une fois l’impureté définie, on choisira la coupure \\((j,s)\\) qui maximise le gain d’impureté entre le noeud père et ses deux noeuds fils : \\[\\Delta(\\mathcal I)=\\mathbf P(\\mathcal N)\\mathcal I(\\mathcal N)-(\\mathbf P(\\mathcal N_1(j,s))\\mathcal I(\\mathcal N_1(j,s))+\\mathbf P(\\mathcal N_2(j,s))\\mathcal I(\\mathcal N_2(j,s))\\] où * \\(\\mathcal N_1(j,s)\\) et \\(\\mathcal N_2(j,s)\\) sont les 2 nœuds fils de \\(\\mathcal N\\) engendrés par la coupure \\((j,s)\\) ; * \\(\\mathbf P(\\mathcal N)\\) représente la proportion d’observations dans le nœud \\(\\mathcal N\\). 3.1.1 Arbres de régression On considère le jeu de données suivant où le problème est d’expliquer la variable quantitative \\(Y\\) par la variable quantitative \\(X\\). n &lt;- 50 set.seed(1234) X &lt;- runif(n) set.seed(5678) Y &lt;- 1*X*(X&lt;=0.6)+(-1*X+3.2)*(X&gt;0.6)+rnorm(n,sd=0.1) data1 &lt;- data.frame(X,Y) ggplot(data1)+aes(x=X,y=Y)+geom_point() A l’aide de la fonction rpart du package rpart, construire un arbre permettant d’expliquer \\(Y\\) par \\(X\\). library(rpart) Visualiser l’arbre à l’aide des fonctions prp et rpart.plot du package rpart.plot. Écrire l’estimateur associé à l’arbre. Ajouter sur le graphe de la question 1 la partition définie par l’arbre ainsi que les valeurs prédites. 3.1.2 Arbres de classification On considère les données suivantes où le problème est d’expliquer la variable binaire \\(Y\\) par deux variables quantitatives \\(X_1\\) et \\(X_2\\). n &lt;- 50 set.seed(12345) X1 &lt;- runif(n) set.seed(5678) X2 &lt;- runif(n) Y &lt;- rep(0,n) set.seed(54321) Y[X1&lt;=0.45] &lt;- rbinom(sum(X1&lt;=0.45),1,0.85) set.seed(52432) Y[X1&gt;0.45] &lt;- rbinom(sum(X1&gt;0.45),1,0.15) data2 &lt;- data.frame(X1,X2,Y) ggplot(data2)+aes(x=X1,y=X2,color=Y)+geom_point(size=2)+scale_x_continuous(name=&quot;&quot;)+ scale_y_continuous(name=&quot;&quot;)+theme_classic() Construire un arbre permettant d’expliquer \\(Y\\) par \\(X_1\\) et \\(X_2\\). Représenter l’arbre et identifier l’éventuel problème. Écrire la règle de classification ainsi que la fonction de score définies par l’arbre. Ajouter sur le graphe de la question 1 la partition définie par l’arbre. 3.1.3 Entrée qualitative On considère les données n &lt;- 100 X &lt;- factor(rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;),n)) set.seed(1234) Y[X==&quot;A&quot;] &lt;- rbinom(sum(X==&quot;A&quot;),1,0.9) Y[X==&quot;B&quot;] &lt;- rbinom(sum(X==&quot;B&quot;),1,0.25) Y[X==&quot;C&quot;] &lt;- rbinom(sum(X==&quot;C&quot;),1,0.8) Y[X==&quot;D&quot;] &lt;- rbinom(sum(X==&quot;D&quot;),1,0.2) Y &lt;- as.factor(Y) data3 &lt;- data.frame(X,Y) Construire un arbre permettant d’expliquer \\(Y\\) par \\(X\\). Expliquer la manière dont l’arbre est construit dans ce cadre là. 3.2 Élagage Le procédé de coupe présenté précédemment permet de définir un très grand nombre d’arbres à partir d’un jeu de données (arbre sans coupure, avec une coupure, deux coupures…). Se pose alors la question de trouver le meilleur arbre parmi tous les arbres possibles. Une première idée serait de choisir parmi tous les arbres possibles celui qui optimise un critère de performance. Cette approche, bien que cohérente, n’est généralement pas possible à mettre en œuvre en pratique car le nombre d’arbres à considérer est souvent trop important. La méthode CART propose une procédure permettant de choisir automatiquement un arbre en 3 étapes : On construit un arbre maximal (très profond) \\(\\mathcal T_{max}\\) ; On sélectionne une suite d’arbres emboités : \\[\\mathcal T_{max}=\\mathcal T_0\\supset\\mathcal T_1\\supset\\dots\\supset \\mathcal T_K.\\] La sélection s’effectue en optimisant un critère Cout/complexité qui permet de réguler le compromis entre ajustement et complexité de l’arbre. On sélectionne un arbre dans cette sous-suite en optimisant un critère de performance. Cette approche revient à choisir un sous-arbre de l’arbre \\(\\mathcal T_\\text{max}\\), c’est-à-dire à enlever des branches à \\(T_\\text{max}\\), c’est pourquoi on parle d’élagage. 3.2.1 Élagage pour un problème de régression On considère les données Carseats du package ISLR. library(ISLR) data(Carseats) summary(Carseats) Sales CompPrice Income Min. : 0.000 Min. : 77 Min. : 21.00 1st Qu.: 5.390 1st Qu.:115 1st Qu.: 42.75 Median : 7.490 Median :125 Median : 69.00 Mean : 7.496 Mean :125 Mean : 68.66 3rd Qu.: 9.320 3rd Qu.:135 3rd Qu.: 91.00 Max. :16.270 Max. :175 Max. :120.00 Advertising Population Price Min. : 0.000 Min. : 10.0 Min. : 24.0 1st Qu.: 0.000 1st Qu.:139.0 1st Qu.:100.0 Median : 5.000 Median :272.0 Median :117.0 Mean : 6.635 Mean :264.8 Mean :115.8 3rd Qu.:12.000 3rd Qu.:398.5 3rd Qu.:131.0 Max. :29.000 Max. :509.0 Max. :191.0 ShelveLoc Age Education Urban Bad : 96 Min. :25.00 Min. :10.0 No :118 Good : 85 1st Qu.:39.75 1st Qu.:12.0 Yes:282 Medium:219 Median :54.50 Median :14.0 Mean :53.32 Mean :13.9 3rd Qu.:66.00 3rd Qu.:16.0 Max. :80.00 Max. :18.0 US No :142 Yes:258 On cherche ici à expliquer la variable quantitative Sales par les autres variables. Construire un arbre permettant de répondre au problème. Expliquer les sorties de la fonction printcp appliquée à l’arbre de la question précédente et calculer le dernier terme de la colonne rel error. Construire une suite d’arbres plus grandes en jouant sur les paramètres cp et minsplit de la fonction rpart. Expliquer la sortie de la fonction plotcp appliquée à l’arbre de la question précédente. Sélectionner le “meilleur” arbre dans la suite construite. Visualiser l’arbre choisi (utiliser la fonction prune). On souhaite prédire les valeurs de \\(Y\\) pour de nouveaux individus à partir de l’arbre sélectionné. Pour simplifier on considèrera ces 4 individus : new_ind &lt;- Carseats %&gt;% slice(3,58,185,218) %&gt;% select(-Sales) new_ind CompPrice Income Advertising Population Price ShelveLoc 3 113 35 10 269 80 Medium 58 93 91 0 22 117 Bad 185 132 33 7 35 97 Medium 218 106 44 0 481 111 Medium Age Education Urban US 3 59 12 Yes Yes 58 75 11 Yes No 185 60 11 No Yes 218 70 14 No No Calculer les valeurs prédites. Séparer les données en un échantillon d’apprentissage de taille 250 et un échantillon test de taille 150. On considère la suite d’arbres définie par set.seed(4321) tree &lt;- rpart(Sales~.,data=train,cp=0.000001,minsplit=2) Dans cette suite, sélectionner un arbre très simple (avec 2 ou 3 coupures) un arbre très grand l’arbre optimal (avec la procédure d’élagage classique). Calculer l’erreur quadratique de ces 3 arbres en utilisant l’échantillon test. Refaire la comparaison avec une validation croisée 10 blocs. 3.2.2 Élagage en classification binaire et matrice de coût On considère ici les mêmes données que précédemment mais on cherche à expliquer une version binaire de la variable Sales. Cette nouvelle variable, appelée High prend pour valeurs No si Sales est inférieur ou égal à 8, Yes sinon. On travaillera donc avec le jeu data1 défini ci-dessous. High &lt;- ifelse(Carseats$Sales&lt;=8,&quot;No&quot;,&quot;Yes&quot;) data1 &lt;- Carseats %&gt;% dplyr::select(-Sales) %&gt;% mutate(High) Construire un arbre permettant d’expliquer High par les autres variables (sans Sales évidemment !) et expliquer les principales différences par rapport à la partie précédente précédente. Expliquer l’option parms dans la commande : tree1 &lt;- rpart(High~.,data=data1,parms=list(split=&quot;information&quot;)) tree1$parms $prior 1 2 0.59 0.41 $loss [,1] [,2] [1,] 0 1 [2,] 1 0 $split [1] 2 Expliquer les sorties de la fonction printcp sur le premier arbre construit et retrouver la valeur du dernier terme de la colonne rel error. Sélectionner un arbre optimal dans la suite. On considère la suite d’arbres tree2 &lt;- rpart(High~.,data=data1,parms=list(loss=matrix(c(0,5,1,0),ncol=2)), cp=0.01,minsplit=2) Expliquer les sorties des commandes suivantes. On pourra notamment calculer le dernier terme de la colonne rel error de la table cptable. tree2$parms $prior 1 2 0.59 0.41 $loss [,1] [,2] [1,] 0 1 [2,] 5 0 $split [1] 1 printcp(tree2) Classification tree: rpart(formula = High ~ ., data = data1, parms = list(loss = matrix(c(0, 5, 1, 0), ncol = 2)), cp = 0.01, minsplit = 2) Variables actually used in tree construction: [1] Advertising Age CompPrice Education [5] Income Population Price ShelveLoc Root node error: 236/400 = 0.59 n= 400 CP nsplit rel error xerror xstd 1 0.101695 0 1.00000 5.0000 0.20840 2 0.050847 2 0.79661 3.8136 0.20909 3 0.036017 3 0.74576 3.2034 0.20176 4 0.035311 5 0.67373 3.1271 0.20038 5 0.025424 9 0.50847 2.6144 0.19069 6 0.016949 11 0.45763 2.3475 0.18307 7 0.015537 16 0.37288 2.1992 0.17905 8 0.014831 21 0.28814 2.1992 0.17905 9 0.010593 23 0.25847 2.0466 0.17367 10 0.010000 25 0.23729 2.0297 0.17292 Comparer les valeurs ajustées par les deux arbres considérés. Références "],["agregation.html", "Chapitre 4 Agrégation : forêts aléatoires et gradient boosting 4.1 Forêts aléatoires 4.2 Gradient boosting", " Chapitre 4 Agrégation : forêts aléatoires et gradient boosting Les méthodes par arbres présentées précédemment sont des algorithmes qui possèdent tout un tas de qualités (facile à mettre en œuvre, interprétable…). Ce sont néanmoins rarement les algorithmes qui se révèlent les plus performants. Les méthodes d’agrégation d’arbres présentées dans cette partie sont souvent beaucoup plus pertinentes, notamment en terme de qualité de prédiction. Elles consistent à construire un très grand nombre d’arbres “simples” : \\(g_1,\\dots,g_B\\) et à les agréger en faisant la moyenne : \\[\\frac{1}{B}\\sum_{k=1}^Bg_k(x).\\] Les forêts aléatoires (Breiman 2001) et le gradient boosting (Friedman 2001) utilisent ce procédé d’agrégation. 4.1 Forêts aléatoires L’algorithme des forêts aléatoires consiste à construire des arbres sur des échantillons bootstrap et à les agréger. Il peut s’écrire de la façon suivante : Entrées : \\(x\\in\\mathbb R^d\\) l’observation à prévoir, \\(\\mathcal D_n\\) l’échantillon ; \\(B\\) nombre d’arbres ; \\(n_{max}\\) nombre max d’observations par nœud \\(m\\in\\{1,\\dots,d\\}\\) le nombre de variables candidates pour découper un nœud. Algorithme : pour \\(k=1,\\dots,B\\) : Tirer un échantillon bootstrap dans \\(\\mathcal D_n\\) Construire un arbre CART sur cet échantillon bootstrap, chaque coupure est sélectionnée en minimisant la fonction de coût de CART sur un ensemble de \\(m\\) variables choisies au hasard parmi les \\(d\\). On note \\(T(.,\\theta_k,\\mathcal D_n)\\) l’arbre construit. Sortie : l’estimateur \\(T_B(x)=\\frac{1}{B}\\sum_{k=1}^BT(x,\\theta_k,\\mathcal D_n)\\). Cet algorithme peut être utilisé sur R avec la fonction randomForest du package randomForest. Nous la présentons à travers l’exemple du jeu de données spam du package kernlab. library(kernlab) data(spam) set.seed(1234) spam &lt;- spam[sample(nrow(spam)),] Le problème est d’expliquer la variable binaire type par les autres. A l’aide de la fonction randomForest du package randomForest, ajuster une forêt aléatoire pour répondre au problème posé. Appliquer la fonction plot à l’objet construit avec randomForest et expliquer le graphe obtenu. A quoi peut servir ce graphe en pratique ? Construire la forêt avec mtry=1 et comparer ses performances avec celle construite précédemment. Utiliser la fonction train du package caret pour choisir le paramètre mtry dans la grille seq(1,30,by=5). Construire la forêt avec le paramètre mtry sélectionné. Calculer l’importance des variables et représenter ces importance à l’aide d’un diagramme en barres. La fonction ranger du package ranger permet également de calculer des forêts aléatoires. Comparer les temps de calcul de cette fonction avec randomForest 4.2 Gradient boosting Les algorithmes de gradient boosting permettent de minimiser des pertes empiriques de la forme \\[\\frac{1}{n}\\sum_{i=1}^n\\ell(y_i,f(x_i)).\\] où \\(\\ell:\\mathbb R\\times\\mathbb R\\to\\mathbb R\\) est une fonction de coût convexe en son second argument. Il existe plusieurs type d’algorithmes boosting. Un des plus connus et utilisés a été proposé par Friedman (2001), c’est la version que nous étudions dans cette partie. Cette approche propose de chercher la meilleure combinaison linéaire d’arbres binaires, c’est-à-dire que l’on recherche \\(g(x)=\\sum_{m=1}^M\\alpha_mh_m(x)\\) qui minimise \\[\\mathcal R_n(g)=\\frac{1}{n}\\sum_{i=1}^n\\ell(y_i,g(x_i)).\\] Optimiser sur toutes les combinaisons d’arbres binaires se révélant souvent trop compliqué, Friedman (2001) utilise une descente de gradient pour construire la combinaison d’abres de façon récursive. L’algorithme est le suivant : Entrées : \\(d_n=(x_1,y_1),\\dots,(x_n,y_n)\\) l’échantillon, \\(\\lambda\\) un paramètre de régularisation tel que \\(0&lt;\\lambda\\leq 1\\). \\(M\\in\\mathbb N\\) le nombre d’itérations. paramètres de l’arbre (nombre de coupures…) Itérations : Initialisation : \\(g_0(.)=\\mathop{\\mathrm{argmin}}_c \\frac{1}{n}\\sum_{i=1}^n \\ell(y_i,c)\\) Pour \\(m=1\\) à \\(M\\) : Calculer l’opposé du gradient \\(-\\frac{\\partial}{\\partial g(x_i)}\\ell(y_i,g(x_i))\\) et l’évaluer aux points \\(g_{m-1}(x_i)\\) : \\[U_i=-\\frac{\\partial}{\\partial g(x_i)}\\ell(y_i,g(x_i)) _{\\Big |g(x_i)=g_{m-1}(x_i)},\\quad i=1,\\dots,n.\\] Ajuster un arbre sur l’échantillon \\((x_1,U_1),\\dots,(x_n,U_n)\\), on le note \\(h_m\\). Mise à jour : \\(g_m(x)=g_{m-1}(x)+\\lambda h_m(x)\\). Sortie : la suite \\((g_m(x))_m\\). Sur R On peut utiliser différents packages pour faire du gradient boosting. Nous utilisons ici le package gbm (Ridgeway 2006). 4.2.1 Un exemple simple en régression On considère un jeu de données \\((x_i,y_i),i=1,\\dots,200\\) issu d’un modèle de régression \\[y_i=m(x_i)+\\varepsilon_i\\] où la vraie fonction de régression est la fonction sinus (mais on va faire comme si on ne le savait pas). x &lt;- seq(-2*pi,2*pi,by=0.01) y &lt;- sin(x) set.seed(1234) X &lt;- runif(200,-2*pi,2*pi) Y &lt;- sin(X)+rnorm(200,sd=0.2) df1 &lt;- data.frame(X,Y) df2 &lt;- data.frame(X=x,Y=y) p1 &lt;- ggplot(df1)+aes(x=X,y=Y)+geom_point()+geom_line(data=df2,size=1)+xlab(&quot;&quot;)+ylab(&quot;&quot;) p1 Rappeler ce que siginifie le \\(L_2\\)-boosting. A l’aide de la fonction gbm du package gbm construire un algorithme de \\(L_2\\)-boosting. On utilisera 500000 itérations et gardera les autres valeurs par défaut de paramètres. Visualiser l’estimateur à la première itération. On pourra faire un predict avec l’option n.trees. Faire de même pour les itérations 1000 et 500000. Sélectionner le nombre d’itérations par la procédure de votre choix. 4.2.2 Adaboost et logitboost pour la classification binaire. On considère le jeu de données spam du package kernlab. library(kernlab) data(spam) set.seed(1234) spam &lt;- spam[sample(nrow(spam)),] Exécuter la commande model_ada1 &lt;- gbm(type~.,data=spam,distribution=&quot;adaboost&quot;,interaction.depth=2, shrinkage=0.05,n.trees=500) Proposer une correction permettant de faire fonctionner l’algorithme. Expliciter le modèle ajusté par la commande précédente. Effectuer un summary du modèle ajusté. Expliquer la sortie. Utiliser la fonction vip du package vip pour retrouver ce sorties. Sélectionner le nombre d’itérations pour l’algorithme adaboost en faisant de la validation croisée 5 blocs. Faire la même procédure en changeant la valeur du paramètre shrinkage. Interpréter. Expliquer la différence entre adaboost et logitboost et précisez comment on peut mettre en œuvre ce dernier algorithme. 4.2.3 Exercices Rappeler la fonction de risque adaboost. Montrer que le risque est minimum en \\[f^\\star(x)=\\frac{1}{2}\\log\\frac{\\mathbf P(Y=1|X=x)}{\\mathbf P(Y=-1|X=x)}.\\] Mêmes questions pour le risque logitboost. Références "],["deep.html", "Chapitre 5 Réseaux de neurones avec Keras", " Chapitre 5 Réseaux de neurones avec Keras Nous présentons ici une introduction au réseau de neurones à l’aide du package keras. On pourra trouver une documentation complète ainsi qu’un très bon tutoriel aux adresses suivantes https://keras.rstudio.com et https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/. On commence par charger la librairie library(keras) #install_keras() 1 seule fois sur la machine On va utiliser des réseaux de neurones pour le jeu de données spam où le problème est d’expliquer la variable binaires typepar les 57 autres variables du jeu de données : library(kernlab) data(spam) spamX &lt;- as.matrix(spam[,-58]) #spamY &lt;- to_categorical(as.numeric(spam$type)-1, 2) spamY &lt;- as.numeric(spam$type)-1 On sépare les données en un échantillon d’apprentissage et un échantillon test set.seed(5678) perm &lt;- sample(4601,3000) appX &lt;- spamX[perm,] appY &lt;- spamY[perm] validX &lt;- spamX[-perm,] validY &lt;- spamY[-perm] A l’aide des données d’apprentissage, entrainer un perceptron simple avec une fonction d’activation sigmoïde. On utilisera 30 epochs et des batchs de taille 5. #Définition du modèle percep.sig &lt;- keras_model_sequential() percep.sig %&gt;% layer_dense(units=...,input_shape = ...,activation=&quot;...&quot;) summary(percep.sig) percep.sig %&gt;% compile( loss=&quot;binary_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=&quot;accuracy&quot; ) #Entrainement p.sig &lt;- percep.sig %&gt;% fit( x=..., y=..., epochs=..., batch_size=..., validation_split=..., verbose=0 ) Faire de même avec la fonction d’activation softmax. On utilisera pour cela 2 neurones avec une sortie \\(Y\\) possédant la forme suivante. spamY1 &lt;- to_categorical(as.numeric(spam$type)-1, 2) appY1 &lt;- spamY1[perm,] validY1 &lt;- spamY1[-perm,] Comparer les performances des deux perceptrons sur les données de validation à l’aide de la fonction evaluate. Construire un ou deux réseaux avec deux couches cachées. On pourra faire varier les nombre de neurones dans ces couches. Comparer les performances des réseaux construits. "],["dondes.html", "Chapitre 6 Données déséquilibrées 6.1 Critères de performance pour données déséquilibrées 6.2 Ré-équilibrage 6.3 Exercices supplémantaires", " Chapitre 6 Données déséquilibrées On parle de données déséquilibrées lorsque les deux modalités de la variable cible \\(Y\\) ne sont pas représentées de façon égale dans l’échantillon, ou plus précisément lorsqu’une des deux modalités est fortement majoritaire. Ce contexte est fréquemment rencontré en pratique, on peut citer les cas de détection de fraudes (peu de fraudeurs), de la présence d’une maladie rare (peu de patients atteints), du risque de crédit (peu de mauvais payeurs)… Les algorithmes standards peuvent être mis en difficultés et de nouvelles stratégies doivent être élaborées. Les stratégies classiques permettant de répondre à ce problème consistent à utiliser des critères de performance adaptés au déséquilibre ; ré-échantilloner les données pour se rapprocher d’une situation d’équilibre. Nous présentons ces stratégies à travers quelques exercices. 6.1 Critères de performance pour données déséquilibrées La notion de risque en machine learning est capitale puisque c’est à partir de l’estimation de ces risques que l’on calibre des algorithmes et que l’on choisit un algorithme de prévision. En présence de données déséquilibré, il convient de choisir un risque adapté. En effet, il est le plus souvent important de parvenir à bien identifier des individus de la classe minoritaire. Des critères tels que l’accuracy ou l’erreur de classification ne sont pas pertinents pour ce cadre. On va privilégier des critères comme le balanced accuracy \\[\\text{Bal Acc}=\\frac{1}{2}\\mathbf P(g(X)=1|Y=1)+\\frac{1}{2}\\mathbf P(g(X)=-1|Y=-1)=\\frac{\\text{TPR+TNR}}{2}.\\] le \\(F_1\\)-score \\[F_1=2\\,\\frac{\\text{Precision }\\times\\text{Recall}}{\\text{Precision }+\\text{Recall}},\\] avec \\[\\text{Precision}=\\mathbf P(Y=1|g(X)=1)\\quad\\text{et}\\quad\\text{Recall}=\\mathbf P(g(X)=1|Y=1).\\] le kappa de Cohen \\[\\kappa=\\frac{\\mathbf P(a)-\\mathbf P(e)}{1-\\mathbf P(e)}\\] où \\(\\mathbf P(a)\\) représente l’accuraci et \\(\\mathbf P(e)\\) l’accuracy sous une hypothèse d’indépendance. la courbe ROC et l’AUC… Comme d’habitude, ces critères sont inconnus et doivent être estimés par des méthodes de ré-échantillonnage de type validation croisée. Exercice 6.1 (Calculer des critères) Générer un vecteur d’observations Y de taille 500 selon une loi de Bernoulli de paramètre 0.05. Générer un vecteur de prévisions P1 de taille 500 selon une loi de Bernoulli de paramètre 0.01. Générer un vecteur de prévision P2 de taille 500 tel que \\[\\mathcal L(P2|Y=0)=\\mathcal B(0.10)\\quad\\text{et}\\quad \\mathcal L(P2|Y=1)=\\mathcal B(0.85).\\] Dresser les tables de contingence de P1 et P2 à l’aide de table. Commenter. Pour P2, calculer, avec les fonctions usuelles de R, l’accuracy, le recall et la précision. En déduire le F1-score. Même question pour le \\(\\kappa\\) de Cohen. Retrouver ces indicateurs à l’aide de la fonction confusionMatrix de caret puis comparer les prévisions P1 et P2. 6.2 Ré-équilibrage En complément du choix d’un critère pertinent, il peut être intéressant de tenter de ré-équilibrer l’échantillon pour aider les algorithmes à mieux détecter les individus de la classe minoritaire. Les méthodes classiques consistent à créer de nouvelles observations de la classe minoritaire (oversampling) et/ou supprimer des individus de la classe minoritaire (undersampling). Exercice 6.2 (Quelques algorithmes de ré-équilibrage) On considère le jeu de données df ci-dessous où on cherche à prédire Y par X1 et X2. n &lt;- 2000 set.seed(1234) X1 &lt;- runif(n) set.seed(5678) X2 &lt;- runif(n) set.seed(9012) R1 &lt;- X1&lt;=0.25 R2 &lt;- (X1&gt;0.25 &amp; X2&gt;=0.75) R3 &lt;- (X1&gt;0.25 &amp; X2&lt;0.75) Y &lt;- rep(0,n) Y[R1] &lt;- rbinom(sum(R1),1,0.75) Y[R2] &lt;- rbinom(sum(R2),1,0.75) Y[R3] &lt;- rbinom(sum(R3),1,0.25) df1 &lt;- data.frame(X1,X2,Y) df1$Y &lt;- factor(df1$Y) indDY1 &lt;- which(df1$Y==1) df1.1 &lt;- df1[-indDY1[1:650],] df1.2 &lt;- df1.1[sample(nrow(df1.1),1000),] df &lt;- df1.2[sample(nrow(df1.2),100),] rownames(df) &lt;- NULL p1 &lt;- ggplot(df)+aes(x=X1,y=X2,color=Y)+geom_point() p1 On a ici 4 fois plus d’observations dans le groupe 0. summary(df$Y) 0 1 80 20 On commence par faire du oversampling avec la fonction RandOverClassif. Effectuer le ré-échantillonnage et expliquer. library(UBL) over1 &lt;- RandOverClassif(Y~.,dat = df) summary(over1$Y) 0 1 80 80 Corriger les paramètres de la fonction de manière à avoir 80 observations dans le groupe 0 et 60 dans le groupe 1. On s’intéresse maintenant à l’algorithme SMOTE Exécuter la fonction SmoteClassif avec k=3 et les les paramètres par défaut Visualiser les observations smote. Corriger les paramètres de la fonction de manière à avoir 80 observations dans le groupe 0 et 60 dans le groupe 1. On souhaite maintenant ré-équilibrer par random undersampling. Utiliser la fonction RandUnderClassif pour effectuer un tel ré-équilibrage. Ici encore on pourra faire varier les paramètres. On passe maintenant à l’algorithme Tomek. Sans utiliser la fonction TomekClassif identifier les paires d’observations qui ont un lien de Tomek. On pourra utiliser la fonction nng du package cccd. Retrouver ces paires à l’aide de la fonction Tomek LinK. Visualiser les observations supprimées. On prendra soin d’expliquer l’option rem de TomekClassif. Exercice 6.2 (Comparaison de méthodes de ré-équilibrage) On considère 3 jeux de données df1, df2 et df3. n &lt;- 2000 set.seed(12345) X1 &lt;- runif(n) set.seed(5678) X2 &lt;- runif(n) set.seed(9012) R1 &lt;- X1&lt;=0.25 R2 &lt;- (X1&gt;0.25 &amp; X2&gt;=0.75) R3 &lt;- (X1&gt;0.25 &amp; X2&lt;0.75) Y &lt;- rep(0,n) Y[R1] &lt;- rbinom(sum(R1),1,0.75) Y[R2] &lt;- rbinom(sum(R2),1,0.75) Y[R3] &lt;- rbinom(sum(R3),1,0.25) df1 &lt;- data.frame(X1,X2,Y) df1$Y &lt;- factor(df1$Y) indDY1 &lt;- which(df1$Y==1) df2 &lt;- df1[-indDY1[1:400],] df3 &lt;- df1[-indDY1[1:700],] df1 &lt;- df1[sample(nrow(df1),1000),] df2 &lt;- df2[sample(nrow(df2),1000),] df3 &lt;- df3[sample(nrow(df3),1000),] Comparer la distribution de Y pour ces trois jeux de données et visualiser les observations. On sépare ces 3 échantillons en un échantillon d’apprentissage et un échantillon test. set.seed(123) a1 &lt;- createDataPartition(1:nrow(df1),p=2/3) a2 &lt;- createDataPartition(1:nrow(df2),p=2/3) a3 &lt;- createDataPartition(1:nrow(df3),p=2/3) train1 &lt;- df1[a1$Resample1,] train2 &lt;- df2[a2$Resample1,] train3 &lt;- df3[a3$Resample1,] test1 &lt;- df1[-a1$Resample1,] test2 &lt;- df2[-a2$Resample1,] test3 &lt;- df3[-a3$Resample1,] Ajuster une forêt aléatoire sur les 3 échantillon d’apprentissage, calculer les labels prédits sur les échantillons tests et estimer les différents indicateurs vus en cours à l’aide de confusionMatrix. On considère uniquement l’échantillon df3. Refaire l’analyse précédente en utilisant des techniques de ré-échantillonnage. 6.3 Exercices supplémantaires Exercice 6.3 (Echantillonnage rétrospectif) Dans le cadre de l’échantillonnage rétrospectif pour le modèle logistique vu en cours, démontrer la propriété qui lie le modèle logistique initial au modèle ré-équilibré. Exercice 6.4 (Echantillonnage rétrospectif) Une étude cas/témoins est réalisée pour mesurer l’effet du tabac sur la pathologie. Pour ce faire, on choisit \\(n_1=250\\) patients atteints de la pathologie (cas) et \\(n_0=250\\) patients sains (témoins). Les résultats de l’étude sont présentés ci-dessous Fumeur Non fumeur Non malade 48 202 Malade 208 42 A partir des données obtenues, estimer à l’aide d’un modèle logistique la probabilité d’être atteint pour un fumeur, puis pour un non fumeur. Comment interpréter ces deux probabilités ? Est-ce qu’elles estiment la probabilité d’être atteint pour un individu quelconque dans la population ? Des études précédentes ont montré que cinq individus sur mille sont atteints par la pathologie dans la population entière. En utilisant la propriété de l’exercice précédent, en déduire les probabilités d’être atteint pour un fumeur et un non fumeur dans la population. "],["comp-algo.html", "Chapitre 7 Comparaison d’algorithmes", " Chapitre 7 Comparaison d’algorithmes Les chapitres précédents ont présenté plusieurs algorithmes permettant de répondre à un problème posé, le plus souvent de classification supervisée. Se pose bien entendu la question de choisir un unique algorithme. Etant donné un échantillon \\(\\mathcal D_n=\\{(x_1,y_1),\\dots,(x_n,y_y)\\}\\) on rappelle qu’un algorithme de prévision est une fonction \\[g:\\mathcal X\\times(\\mathcal X\\times \\mathcal Y)^n\\to\\mathcal Y\\] qui, à une nouvelle observation \\(x\\in\\mathcal X\\) renverra la prévision \\(g(x,\\mathcal D_n)\\) calculée à partir de l’échantillon \\(\\mathcal D_n\\). Cette fonction \\(g\\) peut contenir tout un tas d’étapes comme : la gestion des données manquantes une procédure de choix de variables une méthode pour ré-équilibrer les données des procédures pour calibrer des paramètres (qui peuvent éventuellement inclure des validations croisées) … Le machine learning se focalisant sur la capacité d’un algorithme à bien prédire, les stratégies classiques pour choisir un algorithme vont (une fois de plus) consister à évaluer le pouvoir prédictif de chaque algorithme. Il n’y a rien de bien nouveau puisque cela va reposer sur les techniques présentées aux chapitres 1 : choisir un ou plusieurs critères (erreur de classification, AUC, \\(F_1\\)-score…) choisir une procédure de ré-échantillonnage pour estimer ce critère (validation hold-out, validation croisée, OOB…). Nous proposons de développer une stratégie pour choisir un algorithme sur le jeu de données Internet Advertisements Data Set disponible sur cette page https://archive.ics.uci.edu/ml/datasets/internet+advertisements. Le problème est d’identifier la présence d’une image publicitaire sur des pages webs. Il comporte ad.data &lt;- read.table(&quot;data/ad_data.txt&quot;,header=FALSE,sep=&quot;,&quot;,dec=&quot;.&quot;,na.strings = &quot;?&quot;,strip.white = TRUE) dim(ad.data) [1] 3279 1559 Ce jeu de données contient 1558 variables explicatives, ces variables contiennent différentes caractériques de la page web (voir le site où sont présentées les données pour plus d’information). La dernière variable est la variable à expliquer, elle vaut ad. si présence d’une publicité, nonad. sinon. names(ad.data)[ncol(ad.data)] &lt;- &quot;Y&quot; ad.data$Y &lt;- as.factor(ad.data$Y) summary(ad.data$Y) ad. nonad. 459 2820 Ce jeu de données contient des données manquantes. sum(is.na(ad.data)) [1] 2729 On remarque que : 920 lignes 4 colonnes ont au moins une valeur manquante. apply(is.na(ad.data),1,any) %&gt;% sum() [1] 920 var.na &lt;- apply(is.na(ad.data),2,any) names(ad.data)[var.na] [1] &quot;V1&quot; &quot;V2&quot; &quot;V3&quot; &quot;V4&quot; On choisit de retirer ces 4 variables de l’analyse (il faudrait peut-être réfléchir un peu plus…). ad.data1 &lt;- ad.data[,var.na==FALSE] dim(ad.data1) [1] 3279 1555 sum(is.na(ad.data1)) [1] 0 On se retrouve donc en présence de 3279 individus et 1554 variables explicatives. On construit la matrice des X et le vecteur des Y qui sont nécessaires pour certaines fonctions comme glmnet : X.ad &lt;- model.matrix(Y~.,data=ad.data1)[,-1] Y.ad &lt;- ad.data1$Y et on transforme la variable cible en 0-1 pour utiliser gbm: ad.data2 &lt;- ad.data1 %&gt;% mutate(Y=recode(Y,&quot;ad.&quot;=0,&quot;nonad.&quot;=1)) On souhaite comparer les algorithmes présentés précédemment. Ils nécessitent les packages suivants library(e1071) library(caret) library(rpart) library(glmnet) library(ranger) library(gbm) On commence tout d’abord par représenter un algorithme par une fonction R qui admettra en entrée un jeu de données et renverra une unique prévision pour de nouveaux individus. On illustre ces fonctions pour prédire ce nouvel individu. newX &lt;- ad.data1[1000,] newX.X &lt;- matrix(X.ad[1000,],nrow=1) On stockera les prévisions dans l’objet suivant prev &lt;- tibble(algo=c(&quot;SVM&quot;,&quot;arbre&quot;,&quot;ridge&quot;,&quot;lasso&quot;,&quot;foret&quot;,&quot;ada&quot;,&quot;logit&quot;),prev=0) SVM à noyau gaussien où le choix des paramètres du noyau se fait par validation croisée 4 blocs : prev.svm &lt;- function(df,newX){ C &lt;- c(0.01,1,10) sigma &lt;- c(0.1,1,3) gr &lt;- expand.grid(C=C,sigma=sigma) ctrl &lt;- trainControl(method=&quot;cv&quot;,number=4) cl &lt;- makePSOCKcluster(3) registerDoParallel(cl) res.svm &lt;- train(Y~.,data=df,method=&quot;svmRadial&quot;,trControl=ctrl, tuneGrid=gr,prob.model=TRUE) stopCluster(cl) predict(res.svm,newX,type=&quot;prob&quot;)[2] } prev[1,2] &lt;- prev.svm(ad.data1,newX) Arbre de classification où l’élagage est fait selon la procédure CART présentée dans le chapitre 3. prev.arbre &lt;- function(df,newX){ arbre &lt;- rpart(Y~.,data=df,cp=1e-8,minsplit=2) cp_opt &lt;- arbre$cptable %&gt;% as.data.frame() %&gt;% filter(xerror==min(xerror)) %&gt;% dplyr::select(CP) %&gt;% slice(1) %&gt;% as.numeric() arbre.opt &lt;- prune(arbre,cp=cp_opt) predict(arbre,newdata=newX,type=&quot;prob&quot;)[,2] } prev[2,2] &lt;- prev.arbre(ad.data1,newX) Lasso et Ridge où le paramètre de régularisation est choisi par validation croisée 10 blocs en minimisant la déviance binomiale : prev.ridge &lt;- function(df.X,df.Y,newX){ ridge &lt;- cv.glmnet(df.X,df.Y,family=&quot;binomial&quot;,alpha=0) as.vector(predict(ridge,newx = newX,type=&quot;response&quot;)) } prev.lasso &lt;- function(df.X,df.Y,newX){ lasso &lt;- cv.glmnet(df.X,df.Y,family=&quot;binomial&quot;,alpha=1) as.vector(predict(lasso,newx = newX,type=&quot;response&quot;)) } prev[3,2] &lt;- prev.ridge(X.ad,Y.ad,newX.X) prev[4,2] &lt;- prev.lasso(X.ad,Y.ad,newX.X) Forêt aléatoire avec les paramètres par défaut : prev.foret &lt;- function(df,newX){ foret &lt;- ranger(Y~.,data=df,probability=TRUE) predict(foret,data=newX,type=&quot;response&quot;)$predictions[,2] } prev[5,2] &lt;- prev.foret(ad.data1,newX) Adaboost et logitboost avec le nombre d’itérations choisi par validation croisée 5 blocs : prev.ada &lt;- function(df,newX){ ada &lt;- gbm(Y~.,data=df,distribution=&quot;adaboost&quot;,interaction.depth=2, bag.fraction=1,cv.folds = 5,n.trees=500) nb.it &lt;- gbm.perf(ada,plot.it=FALSE) predict(ada,newdata=newX,n.trees=nb.it,type=&quot;response&quot;) } prev.logit &lt;- function(df,newX){ logit &lt;- gbm(Y~.,data=df,distribution=&quot;bernoulli&quot;,interaction.depth=2, bag.fraction=1,cv.folds = 5,n.trees=500) nb.it &lt;- gbm.perf(logit,plot.it=FALSE) predict(logit,newdata=newX,n.trees=nb.it,type=&quot;response&quot;) } prev[6,2] &lt;- prev.ada(ad.data2,newX) prev[7,2] &lt;- prev.logit(ad.data2,newX) On peut visualiser la prévision de chaque algorithme prev # A tibble: 7 x 2 algo prev &lt;chr&gt; &lt;dbl&gt; 1 SVM 0.950 2 arbre 0.990 3 ridge 0.984 4 lasso 0.980 5 foret 0.979 6 ada 0.974 7 logit 0.983 Exercice 7.1 (Choix d’un algorithme par validation croisée) Choisir un algorithme parmi les précédents en utilisant comme critère l’erreur de classification ainsi que la courbe ROC et l’AUC. On pourra faire une validation croisée 10 blocs (même si ça peut être un peu long…). score1 &lt;- score %&gt;% mutate(obs=fct_recode(ad.data1$Y,&quot;0&quot;=&quot;ad.&quot;,&quot;1&quot;=&quot;nonad.&quot;)) %&gt;% pivot_longer(-obs,names_to=&quot;Methode&quot;,values_to=&quot;score&quot;) On remarque que la svm possède les plus mauvais résultats. Cela ne signifie pas forcément que la méthode est mauvaise, peut-être que les choix qui ont été faits (noyaux gaussien, et grilles de paramètres) ne sont pas pertinents. Les arbres se révèlent également peu efficaces pour la courbe ROC et l’AUC, il est rare que les arbres soient parmi les meilleurs algorithmes contrairement au gradient boosting et aux forêts aléatoires. En terme d’AUC, la régression ridge et les forêts aléatoires se distinguent avec de très bonnes performances. On choisira l’algorithme final parmi ces deux là. "],["références.html", "Références", " Références "]]
