[["index.html", "Machine learning Présentation", " Machine learning Laurent Rouvière 2021-01-20 Présentation Ce tutoriel présente une introduction au machine learning avec R. On pourra trouver : les supports de cours associés à ce tutoriel ainsi que les données utilisées à l’adresse suivante https://lrouviere.github.io/machine_learning/ ; le tutoriel sans les correction à l’url https://lrouviere.github.io/TUTO_ML/ le tutoriel avec les corrigés (à certains moment) à l’url https://lrouviere.github.io/TUTO_ML/correction/. Il est recommandé d’utiliser mozilla firefox pour lire le tutoriel. Les thèmes suivants sont abordés : Estimation du risque, présentation du package caret ; SVM, cas séparable, non séparable et astuce du noyau ; Arbres, notamment l’algorithme CART ; Agrégation d’arbres, forêts aléatoires et gradient boosting ; Réseaux de neurones et introduction au deep learning, perceptron multicouches avec keras. Il existe de nombreuses références sur le machine learning, la plus connue étant certainement Hastie, Tibshirani, and Friedman (2009), disponible en ligne à l’url https://web.stanford.edu/~hastie/ElemStatLearn/. On pourra également consulter Boehmke and Greenwell (2019) qui propose une présentation très claire des algorithmes machine learning avec R. Cet ouvrage est également disponible en ligne à l’url https://bradleyboehmke.github.io/HOML/. Références "],["caret.html", "Chapitre 1 Estimation du risque avec caret 1.1 Notion de risque en apprentissage supervisé 1.2 La validation croisée 1.3 Le package caret 1.4 La courbe ROC 1.5 Compléments", " Chapitre 1 Estimation du risque avec caret 1.1 Notion de risque en apprentissage supervisé L’apprentissage supervisé consiste à expliquer ou prédire une sortie \\(y\\in\\mathcal Y\\) par des entrées \\(x\\in\\mathcal X\\) (le plus souvent \\(\\mathcal X=\\mathbb R^p\\)). Cela revient à trouver un algorithme ou machine représenté par une fonction \\[f:\\mathcal X\\to\\mathcal Y\\] qui à une nouvelle observation \\(x\\) associe la prévision \\(f(x)\\). Bien entendu le problème consiste à chercher le meilleur algorithme pour le cas d’intérêt. Cette notion nécessite de meilleur algorithme la définition de critères que l’on va chercher à optimiser. Les critères sont le plus souvent définis à partir du fonction de perte \\[\\begin{align*} \\ell:\\mathcal Y \\times\\mathcal Y &amp; \\mapsto \\mathbb R^+ \\\\ (y,y^\\prime) &amp; \\to\\ell(y,y^\\prime) \\end{align*}\\] où \\(\\ell(y,y^\\prime)\\) représentera l’erreur (ou la perte) pour la prévision \\(y^\\prime\\) par rapport à l’observation \\(y\\). Si on représente le phénomène d’intérêt par un couple aléatoire \\((X,Y)\\) à valeurs dans \\(\\mathcal X\\times\\mathcal Y\\), on mesurera la performance d’un algorithme \\(f\\) par son risque \\[\\mathcal R(f)=\\mathbf E[\\ell(Y,f(X))].\\] Trouver le meilleur algorithme revient alors à trouver \\(f\\) qui minimise \\(\\mathcal R(f)\\). Bien entendu, ce cadre possède une utilité limitée en pratique puisqu’on ne connaît jamais la loi de \\((X,Y)\\), on ne pourra donc jamais calculé le vrai risque d’un algorithme \\(f\\). Tout le problème va donc être de trouver l’algorithme qui a le plus petit risque à partir de \\(n\\) observations \\((x_1,y_1),\\dots,(x_n,y_n)\\). Nous verrons dans les chapitres suivants plusieurs façons de construire des algorithmes mais, dans tous les cas, un algorithme est représenté par une fonction \\[f_n:\\mathcal X\\times(\\mathcal X\\times\\mathcal Y)^n\\to\\mathcal Y\\] qui, pour une nouvelle donnée \\(x\\), renverra la prévision \\(f_n(x)\\) calculée à partir de l’échantillon qui vit dans \\((\\mathcal X\\times\\mathcal Y)^n\\). Dès lors la question qui se pose est de calculer (ou plutôt d’estimer) le risque (inconnu) \\(\\mathcal R(f_n)\\) d’un algorithme \\(f_n\\). Les techniques classiques reposent sur des algorithmes de type validation croisée. Nous les mettons en œuvre dans cette partie pour un algorithme simple : les \\(k\\) plus proches voisins. On commencera par programmer ces techniques “à la main” puis on utilisera le package caret qui permet de calculer des risques pour quasiment tous les algorithmes que l’on retrouver en apprentissage supervisé. 1.2 La validation croisée On cherche à expliquer une variable binaire \\(Y\\) par deux variables quantitatives \\(X_1\\) et \\(X_2\\) à l’aide du jeu de données suivant n &lt;- 2000 set.seed(12345) X1 &lt;- runif(n) X2 &lt;- runif(n) set.seed(9012) R1 &lt;- X1&lt;=0.25 R2 &lt;- (X1&gt;0.25 &amp; X2&gt;=0.75) R3 &lt;- (X1&gt;0.25 &amp; X2&lt;0.75) Y &lt;- rep(0,n) Y[R1] &lt;- rbinom(sum(R1),1,0.25) Y[R2] &lt;- rbinom(sum(R2),1,0.25) Y[R3] &lt;- rbinom(sum(R3),1,0.75) donnees &lt;- data.frame(X1,X2,Y) donnees$Y &lt;- as.factor(donnees$Y) ggplot(donnees)+aes(x=X1,y=X2,color=Y)+geom_point() On considère la perte indicatrice : \\(\\ell(y,y^\\prime)=\\mathbf 1_{y\\neq y^\\prime}\\), le risque d’un algorithme \\(f\\) est donc \\[\\mathcal R(f)=\\mathbf E[\\mathbf 1_{Y\\neq f(X)}]=\\mathbf P(Y\\neq f(X)),\\] il est appelé probabilité d’erreur ou erreur de classification. Séparer le jeu de données en un échantillon d’apprentissage dapp de taille 1500 et un échantillon test dtest de taille 500. On considère la règle de classification des \\(k\\) plus proches voisins. Pour un entier \\(k\\) plus petit que \\(n\\) et un nouvel individu \\(x\\), cette règle affecte à \\(x\\) le label majoritaire des \\(k\\) plus proches voisins de \\(x\\). Sur R on utilise la fonction knn du package class. On peut par exemple obtenir les prévisions des individus de l’échantillon test de la règle des 3 plus proches voisins avec library(class) knn3 &lt;- knn(dapp[,1:2],dtest[,1:2],cl=dapp$Y,k=3) Calculer l’erreur de classification de la règle des 3 plus proches voisins sur les données test (procédure validation hold out). Expliquer la fonction knn.cv. Calculer l’erreur de classification de la règle des 3 plus proches voisins par validation croisée leave-one-out. On considère le vecteur de plus proches voisins suivant : K_cand &lt;- seq(1,500,by=20) Sélectionner une valeur de \\(k\\) dans ce vecteur à l’aide d’une validation hold out et d’un leave-one-out : On calcule l’erreur de classification par validation hold out pour chaque valeur de \\(k\\) : err.ho &lt;- rep(0,length(K_cand)) for (i in 1:length(K_cand)){ ... ... } On de même chose avec la validation croisée leave-one-out : err.loo &lt;- rep(0,length(K_cand)) for (i in 1:length(K_cand)){ ... ... } Faire la même chose à l’aide d’une validation croisée 10 blocs. On pourra construire les blocs avec set.seed(2345) blocs &lt;- caret::createFolds(1:nrow(donnees),10,returnTrain = TRUE) err.cv &lt;- rep(0,length(K_cand)) prev &lt;- donnees$Y for (i in 1:length(K_cand)){ for (j in 1:length(blocs)){ ... ... ... } ... } K_cand[which.min(err.cv)] 1.3 Le package caret Dans la partie précédente, nous avons utiliser des méthodes de validation croisée pour sélectionner le nombre de voisins dans l’algorithme des plus proches voisins. L’approche revenait à estimer un risque pour une grille de valeurs candidates de \\(k\\) choisir la valeur de \\(k\\) qui minimise le risque estimé. Cette pratique est courante en machine learning : on la retrouve fréquemment pour calibrer les algorithmes. Le protocole est toujours le même, pour un méthode donnée il faut spécifier : une grille de valeurs pour les paramètres un risque un algorithme pour estimer le risque. Le package caret permet d’appliquer ce protocole pour plus de 200 algorithmes machine learning. On pourra trouver une documentation complète à cette url http://topepo.github.io/caret/index.html. Deux fonctions sont à utiliser : traincontrol qui permettra notamment de spécifier l’algorithme pour estimer le risque ainsi que les paramètres de cet algorithme ; train dans laquelle on renseignera les données, la grille de candidats… On reprend les données de la partie précédente. Expliquer les sorties des commandes library(caret) set.seed(321) ctrl1 &lt;- trainControl(method=&quot;LGOCV&quot;,number=1) KK &lt;- data.frame(k=K_cand) caret.ho &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl1,tuneGrid=KK) caret.ho k-Nearest Neighbors 2000 samples 2 predictor 2 classes: &#39;0&#39;, &#39;1&#39; No pre-processing Resampling: Repeated Train/Test Splits Estimated (1 reps, 75%) Summary of sample sizes: 1500 Resampling results across tuning parameters: k Accuracy Kappa 1 0.602 0.1956346 21 0.690 0.3649415 41 0.694 0.3736696 61 0.706 0.3992546 81 0.700 0.3867338 101 0.712 0.4122641 121 0.700 0.3882944 141 0.706 0.4017971 161 0.700 0.3903629 181 0.702 0.3941710 201 0.700 0.3898471 221 0.696 0.3806637 241 0.692 0.3714491 261 0.698 0.3829078 281 0.692 0.3693074 301 0.696 0.3764358 321 0.682 0.3474407 341 0.682 0.3468831 361 0.678 0.3352601 381 0.672 0.3214167 401 0.668 0.3113633 421 0.666 0.3057172 441 0.658 0.2853800 461 0.658 0.2841354 481 0.654 0.2732314 Accuracy was used to select the optimal model using the largest value. The final value used for the model was k = 101. plot(caret.ho) En modifiant les paramètres du code précédent, retrouver les résultats de la validation hold out de la partie précédente. On pourra utiliser l’option index dans la fonction trainControl. Utiliser caret pour sélectionner \\(k\\) par validation croisée leave-one-out. Faire de même pour la validation croisée 10 blocs en gardant les mêmes blocs que dans la partie précédente. 1.4 La courbe ROC C’est un critère fréquemment utilisé pour mesurer la performance d’un score. Etant donné \\((X,Y)\\) un couple aléatoire à valeurs dans \\(\\mathcal X\\times \\{-1,1\\}\\), on rappelle qu’un score est une fonction \\(S:\\mathcal X\\to\\mathbb R\\). Dans la plupart des cas, un score s’obtient en estimant la probabilité \\(\\mathbf P(Y=1|X=x)\\). Pour un seuil \\(s\\in\\mathbb R\\) fixé, un score possède deux types d’erreur \\[\\alpha(s)=\\mathbf P(S(X)\\geq s|Y=-1)\\quad\\text{et}\\quad\\beta(s)=\\mathbf P(S(X)&lt; s|Y=1).\\] La courbe ROC est la courbe paramétrée définie par : \\[\\left\\{ \\begin{array}{l} x(s)=\\alpha(s)=\\mathbf P(S(X)&gt;s|Y=-1) \\\\ y(s)=1-\\beta(s)=\\mathbf P(S(X)\\geq s|Y=1) \\end{array}\\right.\\] Elle permet donc de visualiser sur une seul graphe 2D ces deux erreurs pour toutes les valeurs de seuil \\(s\\). Exercice 1.1 (Étude de la courbe ROC) On considère dans cet exercice une fonction de score \\(S\\) que l’on suppose absolument continue. Montrer que la courbe ROC vit dans le carré \\([0,1]^2\\). On suppose dans cette question que \\(S\\) est parfait, ce qui revient à dire qu’il sépare parfaitement les 2 groupes. Mathématiquement on traduit cela par l’existence d’un seuil \\(s^\\star\\in\\mathbb R\\) tel que \\[\\mathbf P(Y=1|S(X)\\geq s^\\star)=1\\quad\\textrm{et}\\quad\\mathbf P(Y=-1|S(X)&lt;s^\\star)=1.\\] Analyser la courbe ROC du score parfait. On suppose dans cette question que \\(S\\) est aléatoire dans le sens où \\(S(X)\\) est indépendante de \\(Y\\) (cela revient à dire que les notes \\(S(X)\\) n’ont aucun lien avec le groupe). Analyser la courbe ROC d’un tel score. Exercice 1.2 (Courbe ROC avec R) On dispose de 4 fonctions de score \\(S_j(x)\\) dont on souhaite visualiser les courbes ROC à partir des valeurs de score calculés sur un échantillon. On trouvera dans le tableau df les scores \\(S_j(X_i),i=1,\\dots,n\\) ainsi que les observations des groupes \\(Y_i\\) set.seed(12345) n &lt;- 200 Y &lt;- rbinom(n,1,0.5) S1 &lt;- runif(n) S2 &lt;- S1 S2[Y==1] &lt;- runif(sum(Y==1),0.6,1) S2[Y==0] &lt;- runif(sum(Y==0),0,0.6) S3 &lt;- S2 S3[Y==1][1:10] &lt;- runif(10,0,0.6) S3[Y==0][1:10] &lt;- runif(10,0.6,1) df &lt;- data.frame(S1,S2,S3,Y=Y) head(df) S1 S2 S3 Y 1 0.5885923 0.6301922 0.419557838 1 2 0.8925918 0.7897537 0.006244182 1 3 0.1237949 0.7058358 0.302344248 1 4 0.5133090 0.6922984 0.438635526 1 5 0.6636402 0.2034380 0.806095919 0 6 0.7655420 0.4036820 0.665537030 0 Visualiser, pour chaque score, les valeurs de score en fonction de \\(Y\\). Commenter Visualiser sur un même graphe les trois courbes ROC. On pourra utiliser d’abord utiliser la fonction roc du package pROC puis la fonction geom_roc du plotROC. Calculer les AUC à l’aide de la fonction auc. On rappelle que l’AUC vérifier la propriété suivante : si \\((X_1,Y_1)\\) et \\((X_2,Y_2)\\) sont indépendantes et de même loi que \\((X,Y)\\), on a \\[AUC(S)=\\mathbf P(S(X_1)\\geq S(X_2)|(Y_1,Y_2)=(1,-1)).\\] Utiliser cette propriété pour retrouver l’AUC de S3. 1.5 Compléments 1.5.1 Calcul parallèle Les validations croisées peuvent se révéler coûteuses en temps de calcul. On utilise souvent des techniques de parallélisation pour améliorer les performances computationnelles. Ces techniques sont relativement facile à mettre en œuvre avec caret, on peut par exemple utiliser la librairie doParallel pour utiliser plusieurs cœurs de la machine. On compare les temps de calculs pour une même validation croisée 10 blocs exécutée avec 1 cœur et 4 cœurs : library(doParallel) ctrl4 &lt;- trainControl(method=&quot;cv&quot;,index=blocs) cl &lt;- makePSOCKcluster(1) registerDoParallel(cl) temps1 &lt;- system.time(ee3 &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl4,tuneGrid=KK)) stopCluster(cl) cl &lt;- makePSOCKcluster(4) registerDoParallel(cl) temps4 &lt;- system.time(ee3 &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl4,tuneGrid=KK)) stopCluster(cl) On compare ces deux temps de calcul temps1 user system elapsed 12.985 0.062 13.124 temps4 user system elapsed 0.625 0.018 5.935 Sans surprise, l’exécution est beaucoup plus rapide avec 4 cœurs. 1.5.2 Répéter les méthodes de rééchantillonnage Les méthodes d’estimation du risque présentées dans cette partie (hold out, validation croisée) sont basées sur du rééchantillonnage. Elles peuvent se révéler sensible à la manière de couper l’échantillon. C’est pourquoi il est recommandé de les répéter plusieurs fois et de moyenner les erreurs sur les répétitions. Ces répétitions sont très faciles à mettre en œuvre avec caret, par exemple pour la validation hold out on utilise l’option number: ctrl &lt;- trainControl(method=&quot;LGOCV&quot;,number=5) caret.ho.rep &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl,tuneGrid=KK) la validation croisée on utilise les options repeatedcv et repeats : ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;,repeats=5) caret.ho.rep &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl,tuneGrid=KK) 1.5.3 Modifier le risque Enfin nous avons uniquement considéré l’erreur de classification. Il est bien entendu possible d’utiliser d’autres risques pour évaluer les performances. C’est l’option metric de la fonction train qui permet généralement de spécifier le risque, si on est par exemple intéressé par l’aire sur la courbe ROC (AUC) on fera : donnees1 &lt;- donnees names(donnees1)[3] &lt;- &quot;Class&quot; levels(donnees1$Class) &lt;- c(&quot;G0&quot;,&quot;G1&quot;) ctrl &lt;- trainControl(method=&quot;LGOCV&quot;,number=1,classProbs=TRUE,summary=twoClassSummary) caret.auc &lt;- train(Class~.,data=donnees1,method=&quot;knn&quot;,trControl=ctrl,tuneGrid=KK,metric=&quot;ROC&quot;) On obtient ici pour chaque valeur de \\(k\\), l’AUC ainsi que les sensibilité et spécificité : caret.auc k-Nearest Neighbors 2000 samples 2 predictor 2 classes: &#39;G0&#39;, &#39;G1&#39; No pre-processing Resampling: Repeated Train/Test Splits Estimated (1 reps, 75%) Summary of sample sizes: 1500 Resampling results across tuning parameters: k ROC Sens Spec 1 0.6338315 0.5937500 0.6739130 21 0.7488031 0.6473214 0.8152174 41 0.7599072 0.6696429 0.8115942 61 0.7554590 0.6785714 0.8188406 81 0.7586698 0.6875000 0.8224638 101 0.7628025 0.6785714 0.8333333 121 0.7603196 0.6830357 0.8333333 141 0.7605703 0.6830357 0.8297101 161 0.7625194 0.6875000 0.8224638 181 0.7616945 0.6875000 0.8188406 201 0.7609990 0.6830357 0.8188406 221 0.7582411 0.6696429 0.8188406 241 0.7567854 0.6607143 0.8224638 261 0.7563406 0.6473214 0.8188406 281 0.7546260 0.6383929 0.8297101 301 0.7530328 0.6294643 0.8333333 321 0.7554914 0.6205357 0.8297101 341 0.7530166 0.6205357 0.8369565 361 0.7518925 0.5848214 0.8405797 381 0.7500970 0.5357143 0.8550725 401 0.7472179 0.5133929 0.8586957 421 0.7472907 0.4866071 0.8695652 441 0.7432550 0.4732143 0.8768116 461 0.7429720 0.4598214 0.8840580 481 0.7404487 0.4241071 0.8876812 ROC was used to select the optimal model using the largest value. The final value used for the model was k = 101. Et on choisira la valeur de \\(k\\) qui maximise l’AUC : caret.auc$bestTune k 6 101 "],["lda.html", "Chapitre 2 Analyse discriminante linéaire 2.1 Prise en main : LDA et QDA sur les iris de Fisher 2.2 Un cas avec beaucoup de classes 2.3 Grande dimension : reconnaissance de phonèmes 2.4 Exercices", " Chapitre 2 Analyse discriminante linéaire L’analyse discriminante linéaire est un algorithme de référence en classification supervisée. Il peut être appréhendé de deux façons complémentaires : une approche géométrique qui revient à chercher des hyperplans qui séparent au mieux les groupes ; une approche modèle qui fait l’hypothèse que les lois des covariables sont des vecteurs gaussiens avec des valeurs de paramètres différentes pour chaque groupe. On considère \\((x_1,y_1),\\dots,(x_n,y_n)\\) un échantillon où \\(x_i\\) est à valeurs dans \\(\\mathbb R^d\\) et \\(y_i\\) dans \\(\\{0,1\\}\\). L’approche géométrique revient à chercher une droite de \\(\\mathbb R^d\\) d’équation \\(a_1x_1+\\dots+a_dx_d=0\\) telle que : les centres de gravité de chaque groupe projeté sur cette droite soit au mieux séparé \\(\\Longrightarrow\\) maximiser la distance inter-classe. les observations projetés soient proches de leur centre de gravité projeté \\(\\Longrightarrow\\) minimiser la distance intra-classe. Le compromis entre ces deux distances s’obtient en maximisant le coefficient de Rayleigh qui est le quotient entre ces deux distance : \\[J(a)=\\frac{B(a)}{W(a)}=\\frac{a^tBa}{a^tWa}\\] où \\(B\\) et \\(W\\) sont les matrices inter et intra classes définies pas \\[B=\\frac{1}{n}\\sum_{k=1}^Kn_k(g_k-g)(g_k-g)^t\\quad\\text{et}\\quad W=\\frac{1}{n}\\sum_{k=1}^Kn_kV_k\\quad\\text{avec}\\quad V_k=\\frac{1}{n_k}\\sum_{i:Y_i=k}(X_i-g_k)(X_i-g_k)^t.\\] Ici \\(g\\) désigne le centre de gravité du nuage \\(x_i,i=1,\\dots,n\\) et \\(g_k,k=0,1\\) les centres de gravité des deux groupes. La solution est donnée par un vecteur propre associé à la plus grande valeur propre de \\(W^{-1}B\\). L’approche modèle fait l’hypothèse que les vecteurs \\(X|Y=k,k=0,1\\) sont des vecteurs gaussiens d’espérance \\(\\mu_k\\in\\mathbb R^d\\) et de matrice de variance covariance \\(\\Sigma\\). Ces paramètres sont estimés par maximum de vraisemblance et on déduit les probablités a posteriori par la formule de Bayes : \\[\\mathbf P(Y=k|X=x)=\\frac{\\pi_kf_{X|Y=k}(x)}{f(x)}\\] Le lien entre ces deux approches est établi dans l’exercice 2.4. Nous proposons dans cette partie quelques exercices pour mettre en œuvre et analyser des analyses discriminantes avec R. 2.1 Prise en main : LDA et QDA sur les iris de Fisher On considère les données sur les iris de Fisher. data(iris) A l’aide de la fonction PCA du package FactoMineR, réaliser une ACP en utilisant comme variables actives les 4 variables quantitatives du jeu de données. On mettra la variable Species comme variable qualitative supplémentaire (option quali.sup). Représenter le nuage des individus sur les 2 premiers axes de l’ACP en utilisant une couleur différente pour chaque espèce d’iris (option habillage). A l’aide de la fonction lda du package MASS, effectuer une analyse discriminante linéaire permettant d’expliquer l’espèce par les 4 autres variables explicatives. Représenter le nuage des individus sur les deux premiers axes de l’analyse discriminante linéaire (en utilisant une couleur différente pour chaque espèce d’iris). Rappeler comment sont obtenues les coordonnées des individus sur chaque axe. En déduire une interprétation de la position des individus. Comparer les représentations des questions 2 et 4. Expliquer les sorties des commandes suivantes (mod.lda est l’objet construit avec la fonction lda). score &lt;- predict(mod.lda)$x ldahist(score[,1],iris[,5]) ldahist(score[,2],iris[,5]) ```` Exécuter et analyser les sorties de la commande mod.lda2 &lt;- lda(Species~.,data=iris,CV=TRUE) Comparer, en terme d’erreur de prévision, les performances de LDA et QDA. 2.2 Un cas avec beaucoup de classes On considère les jeux de données Vowel (training et test) qui se trouvent à cet url. On peut les importer avec dapp &lt;- read_csv(&quot;https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.train&quot;)[,-1] dtest &lt;- read_csv(&quot;https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.test&quot;)[,-1] Expliquer le problème. Effectuer une analyse discriminante linéaire (uniquement avec les données d’apprentissage) et visualiser les individus sur les 2 premiers axes de l’analyse discriminante. On pourra utiliser predict. La fonction suivante permet de choisir les axes à visualiser, ainsi que les centres de gravité projetés des groupes. repres_axes &lt;- function(prev,cdg,axe1=1,axe2=2){ cdg &lt;- prev %&gt;% group_by(y) %&gt;% summarise_all(mean) nom1 &lt;- paste(&quot;LD&quot;,as.character(axe1),sep=&quot;&quot;) nom2 &lt;- paste(&quot;LD&quot;,as.character(axe2),sep=&quot;&quot;) ggplot(prev)+aes_string(x=as.name(nom1),y=as.name(nom2))+geom_point(aes(color=y))+geom_point(data=cdg,aes(color=y),shape=17,size=4)+theme_classic() } Étudier la pertinence des axes. Représenter les individus sur le premier plan factoriel de l’ACP, on utilisera une couleur différente pour chaque groupe. On pourra utiliser le package FactoMineR. Comparer cette projection avec celle obtenue par l’analyse discriminante linéaire. Évaluer la performance de la lda sur les données test. Comparer avec l’analyse discriminante quadratique. Expliquer comment on peut faire de la prévision en réduisant la dimension de l’espace des \\(X\\). Proposer une méthode permettant de choisir le meilleur nombre d’axes. On pourra notamment utiliser l’option dimen de la fonction predict.lda. 2.3 Grande dimension : reconnaissance de phonèmes On considère le jeu de données phoneme téléchargeable à l’url https://github.com/cran/ElemStatLearn/blob/master/data/phoneme.RData. load(&quot;data/phoneme.RData&quot;) data(phoneme) donnees &lt;- phoneme[,-258] Expliquer le problème et représenter pour chaque groupe la courbe moyenne. Séparer les données en un échantillon d’apprentissage de taille 3000 et un échantillon test de taille 1509. Effectuer une analyse discriminante linéaire et une analyse discriminante quadratique sur les données d’apprentissage uniquement. Évaluer les performances de ces deux approches sur les données test. Quels peuvent être les intérêts d’effectuer une analyse discriminante régularisée dans ce contexte ? Effectuer une telle analyse à l’aide de la fonction rda du package klaR. Sélectionner les paramètres de régularisation à l’aide du package caret. Comparer le nouveau modèle aux précédents. 2.4 Exercices Exercice 2.1 (Optimalité de la règle de Bayes) On dispose de \\(n\\) observations \\((x_1,y_1),\\dots,(x_n,y_n)\\) telles que \\(x_i\\in\\mathbb R^p\\) et \\(y_i\\in\\{0,1\\}\\) pour \\(i=1,\\dots,n\\). On souhaite expliquer les sorties \\(y_i\\) par les entrées \\(x_i\\). Rappeler la définition d’une règle de prévision. Rappeler la définition de la règle de Bayes \\(g^\\star\\) et de l’erreur de Bayes \\(L^\\star\\). Soit \\(g\\) une règle de décision. Montrer que \\[\\mathbf P(g(X)\\neq Y|X=x)=1-(\\mathbf 1_{g(x)=1}\\eta(x)+\\mathbf 1_{g(x)=0}(1-\\eta(x)))\\] où \\(\\eta(x)=\\mathbf P(Y=1|X=x)\\). En déduire que pour tout \\(x\\in\\mathcal X\\) et pour toute règle \\(g\\) \\[\\mathbf P(g(X)\\neq Y|X=x)-\\mathbf P(g^\\star(X)\\neq Y|X=x)\\geq 0.\\] Conclure. On considère \\((X,Y)\\) un couple aléatoire à valeurs dans \\(\\mathbb R\\times\\{0,1\\}\\) tel que \\[\\begin{equation*} X\\sim\\mathcal U[-2,2]\\quad\\text{et}\\quad (Y|X=x)\\sim\\left\\{ \\begin{array}{ll} \\mathcal B(1/5) &amp; \\textrm{si } x\\leq 0 \\\\ \\mathcal B(9/10) &amp; \\textrm{si } x&gt;0 \\end{array}\\right. \\end{equation*}\\] où \\(\\mathcal U[a,b]\\) désigne la loi uniforme sur \\([a,b]\\) et \\(\\mathcal B(p)\\) la loi de Bernoulli de paramètre \\(p\\). Calculer la règle de Bayes et l’erreur de Bayes. Exercice 2.2 (MV pour LDA) On cherche à expliquer une variable aléatoire \\(Y\\) à valeurs dans \\(\\{0,1\\}\\) par une variable aléatoire \\(X\\) à valeurs dans \\(\\mathbb R\\). Quels sont les paramètres à estimer dans le modèle d’analyse discriminante linéaire. Calculer la vraisemblance conditionnelle à \\(Y\\) et en déduire les estimateurs des paramètres des lois gaussiennes. Comparer les estimateurs obtenus avec ceux du cours. Exercice 2.3 (Fonctions linéaires discriminantes) On cherche à expliquer une variable aléatoire \\(Y\\) à valeurs dans \\(\\{0,1\\}\\) par une variable aléatoire \\(X\\) à valeurs dans \\(\\mathbb R^p\\). Rappeler le modèle d’analyse discriminante linéaire. Soit \\(x\\in\\mathbb R^p\\) un nouvel individu. Montrer que la règle qui consiste à affecter \\(x\\) dans le groupe qui maximise \\(\\mathbf P(Y=k|X=x)\\) est équivalente à la règle qui consiste à affecter \\(x\\) dans le groupe qui maximise les fonctions linéaires discriminantes (on prendra soin de rappeler la définition des fonctions linéaires discriminantes). Exercice 2.4 (Approche géométrique de la LDA) On considère un \\(n\\)-échantillon i.i.d. \\((x_1,y_1),\\dots,(x_n,y_n)\\) où \\(x_i\\) est à valeurs dans \\(\\mathbb R^2\\) et \\(y_i\\) dans \\(\\{0,1\\}\\). On cherche une droite vectorielle \\(a\\) telle que les projections de chaque groupe sur \\(a\\) soient séparées “au mieux”. Dit autrement, on cherche \\(a\\) telle que la distance entre les centres de gravité \\[g_0=\\frac{1}{\\mbox{card}\\{i:y_i=0\\}}\\sum_{i:y_i=0}x_i\\quad\\textrm{et}\\quad g_1=\\frac{1}{\\mbox{card}\\{i:y_i=1\\}}\\sum_{i:y_i=1}x_i\\] projetés sur \\(a\\) soit maximale (cette distance est appelée distance interclasse) ; la distance entre les projections des individus et leur centre de gravité soit minimale (distance interclasse). Pour un vecteur \\(u\\) de \\(\\mathbb R^2\\), on désigne par \\(\\pi_a(u)\\) son projeté sur la droite engendrée par \\(a\\). Sans perte de généralité on supposera dans un premier temps que \\(a\\) est de norme 1. Rappeler les définitions des variances totale \\(V\\), intra \\(W\\) et inter \\(B\\) des observations \\((x_1,y_1),\\dots,(x_n,y_n)\\). Pour \\(u\\) fixé dans \\(\\mathbb R^2\\), exprimer \\(\\pi_a(u)\\) en fonction de \\(u\\) et \\(a\\) et en déduire que \\(\\|\\pi_a(u)\\|^2=a^tuu^ta\\). Exprimer les variances totale \\(V(a)\\), intra \\(W(a)\\) et inter \\(B(a)\\) projetées sur \\(a\\) en fonction des variances calculées à la question 1. On cherche maintenant à maximiser \\[J(a)=\\frac{B(a)}{W(a)}\\] ou encore à \\[\\begin{equation} \\textrm{maximiser }B(a)\\quad\\textrm{sous la contrainte}\\quad W(a)=1. \\tag{2.1} \\end{equation}\\] La méthode des multiplicateurs de Lagrange permet de résoudre un tel problème. La solution du problème de maximisation d’une fonction \\(f(x)\\) sujette à \\(h(x)=0\\) s’obtient en résolvant l’équation \\[\\frac{\\partial L(x,\\lambda)}{\\partial x}=0,\\quad\\textrm{où}\\quad L(x,\\lambda)=f(x)+\\lambda h(x).\\] Montrer que la solution du problème (2.1) est un vecteur propre de \\(W^{-1}B\\) associé à la plus grande valeur propre de \\(W^{-1}B\\). On note \\(a^\\star\\) cette solution. Montrer que \\(a^\\star\\) est colinéaire à \\(W^{-1}(g_1-g_0)\\). On pourra admettre que, dans le cas de 2 groupes, on a \\[B=\\frac{n_0n_1}{n^2}(g_1-g_0)(g_1-g_0)^t.\\] On considère la règle géométrique d’affectation qui consiste à classer un nouvel individu \\(x\\in\\mathbb R^p\\) au groupe 1 si son projeté sur \\(a^\\star\\) est plus proche de \\(\\pi_{a^\\star}(g_1)\\) que de \\(\\pi_{a^\\star}(g_0)\\). Montrer que \\(x\\) sera affecté au groupe 1 si \\[S(x)=x^tW^{-1}(g_1-g_0)&gt;s\\] où on exprimera \\(s\\) en fonction de \\(g_0\\), \\(g_1\\) et \\(W\\). Montrer que cette règle est équivalente à choisir le groupe qui minimise la distance de Mahalanobis \\[d(x,g_k)=(x-g_k)^tW^{-1}(x-g_k),\\quad k=0,1.\\] On revient maintenant à l’approche probabiliste de l’analyse discriminante linéaire vue en cours et on considère la règle d’affectation qui consiste à décider \"groupe 1’’ si \\(\\mathbf P(Y=1|X=x)\\geq 0.5\\). Montrer que dans ce cas, un nouvel individu \\(x\\) est affecter au groupe 1 si : \\[S(x)=x^t\\Sigma^{-1}(\\mu_1-\\mu_0)&gt;\\frac{1}{2}(\\mu_1+\\mu_0)^t\\Sigma^{-1}(\\mu_1-\\mu_0)-\\log\\left(\\frac{\\pi_1}{\\pi_0}\\right).\\] Conclure. "],["arbres.html", "Chapitre 3 Arbres 3.1 Coupures CART en fonction de la nature des variables 3.2 Élagage", " Chapitre 3 Arbres Les méthodes par arbres sont des algorithmes où la prévision s’effectue à partir de moyennes locales. Plus précisément, étant donné un échantillon \\((x_1,y_1)\\dots,(x_n,y_n)\\), l’approche consiste à : construire une partition de l’espace de variables explicatives (\\(\\mathbb R^p\\)) ; prédire la sortie d’une nouvelle observation \\(x\\) en faisant : la moyenne des \\(y_i\\) pour les \\(x_i\\) qui sont dans la même classe que \\(x\\) si on est en régression ; un vote à la majorité parmi les \\(y_i\\) tels que les \\(x_i\\) qui sont dans la même classe que \\(x\\) si on est en classification. Bien entendu toute la difficulté est de trouver la “bonne partition” pour le problème d’intérêt. Il existe un grand nombre d’algorithmes qui permettent de trouver une partition. Le plus connu est l’algorithme CART (Breiman et al. 1984) où la partition est construite par divisions successives au moyen d’hyperplan orthogonaux aux axes de \\(\\mathbb R^p\\). L’algorithme est récursif : il va à chaque étape séparer un groupe d’observations (nœuds) en deux groupes (nœuds fils) en cherchant la meilleure variable et le meilleur seuil de coupure. Ce choix s’effectue à partir d’un critère d’impureté : la meilleure coupure est celle pour laquelle l’impureté des 2 nœuds fils sera minimale. Nous étudions cet algorithme dans cette partie. 3.1 Coupures CART en fonction de la nature des variables Une partition CART s’obtient en séparant les observations en 2 selon une coupure parallèle aux axes puis en itérant ce procédé de séparation binaire sur les deux groupes… Par conséquent la première question à se poser est : pour un ensemble de données \\((x_1,y_1),\\dots,(x_n,y_n)\\) fixé, comment obtenir la meilleure coupure ? Comme souvent ce sont les données qui vont répondre à cette question. La sélection de la meilleur coupure s’effectue en introduisant une fonction d’impureté \\(\\mathcal I\\) qui va mesurer le degrés d’hétérogénéité d’un nœud \\(\\mathcal N\\). Cette fonction prendra de grandes valeurs pour les nœuds hétérogènes (les valeurs de \\(Y\\) diffèrent à l’intérieur du nœud) ; faibles valeurs pour les nœuds homogènes (les valeurs de \\(Y\\) sont proches à l’intérieur du nœud). On utilise souvent comme fonction d’impureté : la variance en régression \\[\\mathcal I(\\mathcal N)=\\frac{1}{|\\mathcal N|}\\sum_{i:x_i\\in\\mathcal N}(y_i-\\overline{y}_\\mathcal N)^2,\\] où \\(\\overline{y} _\\mathcal N\\) désigne la moyenne des \\(y_i\\) dans \\(\\mathcal N\\). l’impureté de Gini en classification binaire \\[\\mathcal I(\\mathcal N)=2p(\\mathcal N)(1-p(\\mathcal N))\\] où \\(p(\\mathcal N)\\) représente la proportion de 1 dans \\(\\mathcal N\\). Les coupures considérées par l’algorithme CART sont des hyperplans orthogonaux aux axes de \\(\\mathbb R^p\\), choisir une coupure revient donc à choisir une variable \\(j\\) parmi les \\(p\\) variables explicatives et un seuil \\(s\\) dans \\(\\mathbb R\\). On peut donc représenter une coupure par un couple \\((j,s)\\). Une fois l’impureté définie, on choisira la coupure \\((j,s)\\) qui maximise le gain d’impureté entre le noeud père et ses deux noeuds fils : \\[\\Delta(\\mathcal I)=\\mathbf P(\\mathcal N)\\mathcal I(\\mathcal N)-(\\mathbf P(\\mathcal N_1(j,s))\\mathcal I(\\mathcal N_1(j,s))+\\mathbf P(\\mathcal N_2(j,s))\\mathcal I(\\mathcal N_2(j,s))\\] où * \\(\\mathcal N_1(j,s)\\) et \\(\\mathcal N_2(j,s)\\) sont les 2 nœuds fils de \\(\\mathcal N\\) engendrés par la coupure \\((j,s)\\) ; * \\(\\mathbf P(\\mathcal N)\\) représente la proportion d’observations dans le nœud \\(\\mathcal N\\). 3.1.1 Arbres de régression On considère le jeu de données suivant où le problème est d’expliquer la variable quantitative \\(Y\\) par la variable quantitative \\(X\\). n &lt;- 50 set.seed(1234) X &lt;- runif(n) set.seed(5678) Y &lt;- 1*X*(X&lt;=0.6)+(-1*X+3.2)*(X&gt;0.6)+rnorm(n,sd=0.1) data1 &lt;- data.frame(X,Y) ggplot(data1)+aes(x=X,y=Y)+geom_point() A l’aide de la fonction rpart du package rpart, construire un arbre permettant d’expliquer \\(Y\\) par \\(X\\). library(rpart) Visualiser l’arbre à l’aide des fonctions prp et rpart.plot du package rpart.plot. Écrire l’estimateur associé à l’arbre. Ajouter sur le graphe de la question 1 la partition définie par l’arbre ainsi que les valeurs prédites. 3.1.2 Arbres de classification On considère les données suivantes où le problème est d’expliquer la variable binaire \\(Y\\) par deux variables quantitatives \\(X_1\\) et \\(X_2\\). n &lt;- 50 set.seed(12345) X1 &lt;- runif(n) set.seed(5678) X2 &lt;- runif(n) Y &lt;- rep(0,n) set.seed(54321) Y[X1&lt;=0.45] &lt;- rbinom(sum(X1&lt;=0.45),1,0.85) set.seed(52432) Y[X1&gt;0.45] &lt;- rbinom(sum(X1&gt;0.45),1,0.15) data2 &lt;- data.frame(X1,X2,Y) ggplot(data2)+aes(x=X1,y=X2,color=Y)+geom_point(size=2)+scale_x_continuous(name=&quot;&quot;)+ scale_y_continuous(name=&quot;&quot;)+theme_classic() Construire un arbre permettant d’expliquer \\(Y\\) par \\(X_1\\) et \\(X_2\\). Représenter l’arbre et identifier l’éventuel problème. Écrire la règle de classification ainsi que la fonction de score définies par l’arbre. Ajouter sur le graphe de la question 1 la partition définie par l’arbre. 3.1.3 Entrée qualitative On considère les données n &lt;- 100 X &lt;- factor(rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;),n)) set.seed(1234) Y[X==&quot;A&quot;] &lt;- rbinom(sum(X==&quot;A&quot;),1,0.9) Y[X==&quot;B&quot;] &lt;- rbinom(sum(X==&quot;B&quot;),1,0.25) Y[X==&quot;C&quot;] &lt;- rbinom(sum(X==&quot;C&quot;),1,0.8) Y[X==&quot;D&quot;] &lt;- rbinom(sum(X==&quot;D&quot;),1,0.2) Y &lt;- as.factor(Y) data3 &lt;- data.frame(X,Y) Construire un arbre permettant d’expliquer \\(Y\\) par \\(X\\). Expliquer la manière dont l’arbre est construit dans ce cadre là. 3.2 Élagage Le procédé de coupe présenté précédemment permet de définir un très grand nombre d’arbres à partir d’un jeu de données (arbre sans coupure, avec une coupure, deux coupures…). Se pose alors la question de trouver le meilleur arbre parmi tous les arbres possibles. Une première idée serait de choisir parmi tous les arbres possibles celui qui optimise un critère de performance. Cette approche, bien que cohérente, n’est généralement pas possible à mettre en œuvre en pratique car le nombre d’arbres à considérer est souvent trop important. La méthode CART propose une procédure permettant de choisir automatiquement un arbre en 3 étapes : On construit un arbre maximal (très profond) \\(\\mathcal T_{max}\\) ; On sélectionne une suite d’arbres emboités : \\[\\mathcal T_{max}=\\mathcal T_0\\supset\\mathcal T_1\\supset\\dots\\supset \\mathcal T_K.\\] La sélection s’effectue en optimisant un critère Cout/complexité qui permet de réguler le compromis entre ajustement et complexité de l’arbre. On sélectionne un arbre dans cette sous-suite en optimisant un critère de performance. Cette approche revient à choisir un sous-arbre de l’arbre \\(\\mathcal T_\\text{max}\\), c’est-à-dire à enlever des branches à \\(T_\\text{max}\\), c’est pourquoi on parle d’élagage. 3.2.1 Élagage pour un problème de régression On considère les données Carseats du package ISLR. library(ISLR) data(Carseats) summary(Carseats) Sales CompPrice Income Min. : 0.000 Min. : 77 Min. : 21.00 1st Qu.: 5.390 1st Qu.:115 1st Qu.: 42.75 Median : 7.490 Median :125 Median : 69.00 Mean : 7.496 Mean :125 Mean : 68.66 3rd Qu.: 9.320 3rd Qu.:135 3rd Qu.: 91.00 Max. :16.270 Max. :175 Max. :120.00 Advertising Population Price Min. : 0.000 Min. : 10.0 Min. : 24.0 1st Qu.: 0.000 1st Qu.:139.0 1st Qu.:100.0 Median : 5.000 Median :272.0 Median :117.0 Mean : 6.635 Mean :264.8 Mean :115.8 3rd Qu.:12.000 3rd Qu.:398.5 3rd Qu.:131.0 Max. :29.000 Max. :509.0 Max. :191.0 ShelveLoc Age Education Urban Bad : 96 Min. :25.00 Min. :10.0 No :118 Good : 85 1st Qu.:39.75 1st Qu.:12.0 Yes:282 Medium:219 Median :54.50 Median :14.0 Mean :53.32 Mean :13.9 3rd Qu.:66.00 3rd Qu.:16.0 Max. :80.00 Max. :18.0 US No :142 Yes:258 On cherche ici à expliquer la variable quantitative Sales par les autres variables. Construire un arbre permettant de répondre au problème. Expliquer les sorties de la fonction printcp appliquée à l’arbre de la question précédente et calculer le dernier terme de la colonne rel error. Construire une suite d’arbres plus grandes en jouant sur les paramètres cp et minsplit de la fonction rpart. Expliquer la sortie de la fonction plotcp appliquée à l’arbre de la question précédente. Sélectionner le “meilleur” arbre dans la suite construite. Visualiser l’arbre choisi (utiliser la fonction prune). On souhaite prédire les valeurs de \\(Y\\) pour de nouveaux individus à partir de l’arbre sélectionné. Pour simplifier on considèrera ces 4 individus : new_ind &lt;- Carseats %&gt;% slice(3,58,185,218) %&gt;% dplyr::select(-Sales) new_ind CompPrice Income Advertising Population Price ShelveLoc 3 113 35 10 269 80 Medium 58 93 91 0 22 117 Bad 185 132 33 7 35 97 Medium 218 106 44 0 481 111 Medium Age Education Urban US 3 59 12 Yes Yes 58 75 11 Yes No 185 60 11 No Yes 218 70 14 No No Calculer les valeurs prédites. Séparer les données en un échantillon d’apprentissage de taille 250 et un échantillon test de taille 150. On considère la suite d’arbres définie par set.seed(4321) tree &lt;- rpart(Sales~.,data=train,cp=0.000001,minsplit=2) Dans cette suite, sélectionner un arbre très simple (avec 2 ou 3 coupures) un arbre très grand l’arbre optimal (avec la procédure d’élagage classique). Calculer l’erreur quadratique de ces 3 arbres en utilisant l’échantillon test. Refaire la comparaison avec une validation croisée 10 blocs. 3.2.2 Élagage en classification binaire et matrice de coût On considère ici les mêmes données que précédemment mais on cherche à expliquer une version binaire de la variable Sales. Cette nouvelle variable, appelée High prend pour valeurs No si Sales est inférieur ou égal à 8, Yes sinon. On travaillera donc avec le jeu data1 défini ci-dessous. High &lt;- ifelse(Carseats$Sales&lt;=8,&quot;No&quot;,&quot;Yes&quot;) data1 &lt;- Carseats %&gt;% dplyr::select(-Sales) %&gt;% mutate(High) Construire un arbre permettant d’expliquer High par les autres variables (sans Sales évidemment !) et expliquer les principales différences par rapport à la partie précédente précédente. Expliquer l’option parms dans la commande : tree1 &lt;- rpart(High~.,data=data1,parms=list(split=&quot;information&quot;)) tree1$parms $prior 1 2 0.59 0.41 $loss [,1] [,2] [1,] 0 1 [2,] 1 0 $split [1] 2 Expliquer les sorties de la fonction printcp sur le premier arbre construit et retrouver la valeur du dernier terme de la colonne rel error. Sélectionner un arbre optimal dans la suite. On considère la suite d’arbres tree2 &lt;- rpart(High~.,data=data1,parms=list(loss=matrix(c(0,5,1,0),ncol=2)), cp=0.01,minsplit=2) Expliquer les sorties des commandes suivantes. On pourra notamment calculer le dernier terme de la colonne rel error de la table cptable. tree2$parms $prior 1 2 0.59 0.41 $loss [,1] [,2] [1,] 0 1 [2,] 5 0 $split [1] 1 printcp(tree2) Classification tree: rpart(formula = High ~ ., data = data1, parms = list(loss = matrix(c(0, 5, 1, 0), ncol = 2)), cp = 0.01, minsplit = 2) Variables actually used in tree construction: [1] Advertising Age CompPrice Education [5] Income Population Price ShelveLoc Root node error: 236/400 = 0.59 n= 400 CP nsplit rel error xerror xstd 1 0.101695 0 1.00000 5.0000 0.20840 2 0.050847 2 0.79661 3.8136 0.20909 3 0.036017 3 0.74576 3.2034 0.20176 4 0.035311 5 0.67373 3.1271 0.20038 5 0.025424 9 0.50847 2.6144 0.19069 6 0.016949 11 0.45763 2.3475 0.18307 7 0.015537 16 0.37288 2.1992 0.17905 8 0.014831 21 0.28814 2.1992 0.17905 9 0.010593 23 0.25847 2.0466 0.17367 10 0.010000 25 0.23729 2.0297 0.17292 Comparer les valeurs ajustées par les deux arbres considérés. correct &lt;- TRUE Références "],["SVM.html", "Chapitre 4 Support Vector Machine (SVM) 4.1 Cas séparable 4.2 Cas non séparable 4.3 L’astuce du noyau 4.4 Support vector régression 4.5 SVM sur les données spam 4.6 Exercices", " Chapitre 4 Support Vector Machine (SVM) Etant donnée un échantillon \\((x_1,y_1),\\dots,(x_n,y_n)\\) où les \\(x_i\\) sont à valeurs dans \\(\\mathbb R^p\\) et les \\(y_i\\) sont binaires à valeurs dans \\(\\{-1,1\\}\\), l’approche SVM cherche le meilleur hyperplan en terme de séparation des données. Globalement on veut que les 1 se trouvent d’un coté de l’hyperplan et les -1 de l’autre. Dans cette partie on propose d’étudier la mise en œuvre de cet algorithme tout d’abord dans le cas idéal où les données sont séparables puis dans le cas plus réel où elles ne le sont pas. Nous verrons ensuite comment introduire de la non linéarité ne utilisant l’astuce du noyau. 4.1 Cas séparable Le cas séparable est le cas facile : il correspond à la situation où il existe effectivement un (même plusieurs) hyperplan(s) qui sépare(nt) parfaitement les 1 des -1. Il ne se produit quasiment jamais en pratique mais il convient de l’étudier pour comprendre comment est construit l’algorithme. Dans ce cas on cherche l’hyperplan d’équation \\(\\langle w,x\\rangle+b=w^tx+b=0\\) tel que la marge (qui peut être vue comme la distance entre les observations les plus proches de l’hyperplan et l’hyperplan) soit maximale. Mathématiquement le problème se réécrit comme un problème d’optimisation sous contraintes : \\[\\begin{equation} \\min_{w,b}\\frac{1}{2}\\|w\\|^2 \\tag{4.1} \\end{equation}\\] \\[\\text{sous les contraintes } y_i(w^tx_i+b)\\geq 1,\\ i=1,\\dots,n.\\] La solution s’obtient de façon classique en résolvant le problème dual et elle s’écrit comme une combinaison linéaire des \\(x_i\\) \\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\] De plus, les conditions KKT impliquent que pour tout \\(i=1,\\dots,n\\): \\(\\alpha_i^\\star=0\\) ou \\(y_i(x_i^tw+b)-1=0.\\) Ces conditions impliquent que \\(w^\\star\\) s’écrit comme une combinaison linéaire de quelques points, appelés vecteurs supports qui se trouvent sur la marge. Nous proposons maintenant de retrouver ces points et de tracer la marge sur un exemple simple. On considère le nuage de points suivant : n &lt;- 20 set.seed(123) X1 &lt;- scale(runif(n)) set.seed(567) X2 &lt;- scale(runif(n)) Y &lt;- rep(-1,n) Y[X1&gt;X2] &lt;- 1 Y &lt;- as.factor(Y) donnees &lt;- data.frame(X1=X1,X2=X2,Y=Y) p &lt;- ggplot(donnees)+aes(x=X2,y=X1,color=Y)+geom_point() p La fonction svm du package e1071 permet d’ajuster une SVM : library(e1071) mod.svm &lt;- svm(Y~.,data=donnees,kernel=&quot;linear&quot;,cost=10000000000) Récupérer les vecteurs supports et visualiser les sur le graphe (en utilisant une autre couleur par exemple). On les affectera à un data.frame dont les 2 premières colonnes représenteront les valeurs de \\(X_1\\) et \\(X_2\\) des vecteurs supports. Les vecteurs supports se trouvent dans la sortie index de la fonction svm : ind.svm &lt;- mod.svm$index sv &lt;- donnees %&gt;% slice(ind.svm) sv X1 X2 Y 1 -1.61179777 -0.6599042 -1 2 0.06962369 0.7140262 -1 3 -0.31095135 -0.5332139 1 p1 &lt;- p+geom_point(data=sv,aes(x=X2,y=X1),color=&quot;blue&quot;,size=2) On peut ainsi représenter la marge en traçant les droites qui passent par ces points. sv1 &lt;- sv[,2:1] b &lt;- (sv1[1,2]-sv1[2,2])/(sv1[1,1]-sv1[2,1]) a &lt;- sv1[1,2]-b*sv1[1,1] a1 &lt;- sv1[3,2]-b*sv1[3,1] p1+geom_abline(intercept = c(a,a1),slope=b,col=&quot;blue&quot;,size=1) Retrouver ce graphe à l’aide de la fonction plot. plot(mod.svm,data=donnees,grid=250) Rappeler la règle de décision associée à la méthode SVM. Donner les estimations des paramètres de la règle de décision sur cet exemple. On pourra notamment regarder la sortie coef de la fonction svm. Une fois \\(w^\\star\\) et \\(b^\\star\\) obtenus, la règle s’écrit \\[g(x)=1_{\\langle w^\\star,x\\rangle+b^\\star\\leq 0}-1_{\\langle w^\\star,x\\rangle+b^\\star&gt;0}.\\] L’objet mod.svm$coefs contient les coefficients \\(\\alpha_i^\\star y_i\\) pour chaque vecteur support. On peut ainsi récupérer l’équation de l’hyperplan et faire la prévision avec w &lt;- apply(mod.svm$coefs*donnees[mod.svm$index,1:2],2,sum) #ou w &lt;- t(mod.svm$coefs) %*% mod.svm$SV b &lt;- -mod.svm$rho b [1] -0.4035113 L’hyperplan séparateur a donc pour équation : \\[-1.74x_1+2.12x_2-0.40=0.\\] On dispose d’un nouvel individu \\(x=(-0.5,0.5)\\). Expliquer comment on peut prédire son groupe. Il suffit de calculer \\(\\langle w^\\star,x\\rangle+b\\) et de prédire en fonction du signe de cette valeur : newX &lt;- data.frame(X1=-0.5,X2=0.5) sum(w*newX)+b [1] 1.537053 On prédira le groupe -1 pour ce nouvel individu. Retrouver les résultats de la question précédente à l’aide de la fonction predict. On pourra utiliser l’option decision.values = TRUE. predict(mod.svm,newX,decision.values = TRUE) 1 -1 attr(,&quot;decision.values&quot;) -1/1 1 1.537053 Levels: -1 1 Plus cette valeur est élevée, plus on est loin de l’hyperplan. On peut donc l’interpréter comme un score. Obtenir les probabilités prédites à l’aide de la fonction predict. On pourra utiliser probability=TRUE dans la fonction svm. mod.svm1 &lt;- svm(Y~.,data=donnees,kernel=&quot;linear&quot;,cost=10000000000,probability=TRUE) predict(mod.svm1,newX,decision.values=TRUE,probability=TRUE) 1 -1 attr(,&quot;decision.values&quot;) -1/1 1 1.537053 attr(,&quot;probabilities&quot;) -1 1 1 0.8294474 0.1705526 Levels: -1 1 Comme souvent, il est possible d’obtenir une estimation des probabilités d’être dans les groupes -1 et 1 à partir du score, il “suffit” de ramener ce score sur l’échelle \\([0,1]\\) avec des transformations de type logit par exemple. Pour la svm, ces probabilités sont obtenues en ajustant un modèle logistique sur les scores \\(S(x)\\) : \\[P(Y=1|X=x)=\\frac{1}{1+\\exp(aS(x)+b)}.\\] On peut retrouver ces probabilités avec : score.newX &lt;- sum(w*newX)+b 1/(1+exp(-(mod.svm1$probB+mod.svm1$probA*score.newX))) [1] 0.1705526 4.2 Cas non séparable Dans la vraie vie, les groupes ne sont généralement pas séparables et il n’existe donc pas de solution au problème (4.1). On va donc autoriser certains points à être : mal classés et/ou bien classés mais à l’intérieur de la marge. Mathématiquement, cela revient à introduire des variables ressorts (slacks variables) \\(\\xi_1,\\dots,\\xi_n\\) positives telles que : \\(\\xi_i\\in [0,1]\\Longrightarrow\\) \\(i\\) bien classé mais dans la région définie par la marge ; \\(\\xi_i&gt;1 \\Longrightarrow\\) \\(i\\) mal classé. Le problème d’optimisation est alors de minimiser en \\((w,b,\\xi)\\) \\[\\frac{1}{2}\\|w\\|^2 +C\\sum_{i=1}^n\\xi_i\\] \\[\\textrm{sous les contraintes } \\left\\{ \\begin{array}{l} y_i(w^tx_i+b)\\geq 1 -\\xi_i \\\\ \\xi_i\\geq 0, i=1,\\dots,n. \\end{array}\\right.\\] Le paramètre \\(C&gt;0\\) est à calibrer et on remarque que le cas séparable correspond à \\(C\\to +\\infty\\). Les solutions de ce nouveau problème d’optimisation s’obtiennent de la même façon que dans le cas séparable, en particulier \\(w^\\star\\) s’écrite toujours comme une combinaison linéaire \\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\] de vecteurs supports sauf qu’on distingue deux types de vecteurs supports (\\(\\alpha_i^\\star&gt;0\\)): ceux sur la frontière définie par la marge : \\(\\xi_i^\\star=0\\) ; ceux en dehors : \\(\\xi_i^\\star&gt;0\\) et \\(\\alpha_i^\\star=C\\). Le choix de \\(C\\) est crucial : ce paramètre régule le compromis biais/variance de la svm : \\(C\\searrow\\): la marge est privilégiée et les \\(\\xi_i\\nearrow\\) \\(\\Longrightarrow\\) beaucoup d’observations dans la marge ou mal classées (et donc beaucoup de vecteurs supports). \\(C\\nearrow\\Longrightarrow\\) \\(\\xi_i\\searrow\\) donc moins d’observations mal classées \\(\\Longrightarrow\\) meilleur ajustement mais petite marge \\(\\Longrightarrow\\) risque de surajustement. On choisit généralement ce paramètre à l’aide des techniques présentées dans le chapitre 1 : choix d’une grille de valeurs de \\(C\\) et d’un critère ; choix d’une méthode de ré-échantillonnage pour estimer le critère ; choix de la valeur de \\(C\\) qui minimise le critère estimé. On considère le jeu de données df3 définie ci-dessous. n &lt;- 1000 set.seed(1234) df &lt;- as.data.frame(matrix(runif(2*n),ncol=2)) df1 &lt;- df %&gt;% filter(V1&lt;=V2)%&gt;% mutate(Y=rbinom(nrow(.),1,0.95)) df2 &lt;- df %&gt;% filter(V1&gt;V2)%&gt;% mutate(Y=rbinom(nrow(.),1,0.05)) df3 &lt;- bind_rows(df1,df2) %&gt;% mutate(Y=as.factor(Y)) ggplot(df3)+aes(x=V2,y=V1,color=Y)+geom_point()+ scale_color_manual(values=c(&quot;#FFFFC8&quot;, &quot;#7D0025&quot;))+ theme(panel.background = element_rect(fill = &quot;#BFD5E3&quot;, colour = &quot;#6D9EC1&quot;,size = 2, linetype = &quot;solid&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Ajuster 3 svm en considérant comme valeur de \\(C\\) : 0.000001, 0.1 et 5. On pourra utiliser l’option cost. mod.svm1 &lt;- svm(Y~.,data=df3,kernel=&quot;linear&quot;,cost=0.000001) mod.svm2 &lt;- svm(Y~.,data=df3,kernel=&quot;linear&quot;,cost=0.1) mod.svm3 &lt;- svm(Y~.,data=df3,kernel=&quot;linear&quot;,cost=5) Calculer les nombres de vecteurs supports pour chaque valeur de \\(C\\). mod.svm1$nSV [1] 469 469 mod.svm2$nSV [1] 178 178 mod.svm3$nSV [1] 150 150 Visualiser les 3 svm obtenues. Interpréter. plot(mod.svm1,data=df3,grid=250) plot(mod.svm2,data=df3,grid=250) plot(mod.svm3,data=df3,grid=250) Pour \\(C\\) petit, toutes les observations sont classées 0, la marge est grande et le nombre de vecteurs supports importants. On remarque que ces deux quantités diminuent lorsque \\(C\\) augmente. 4.3 L’astuce du noyau Les SVM présentées précédemment font l’hypothèse que les groupes sont linéairement séparables, ce qui n’est bien entendu pas toujours le cas en pratique. L’astuce du noyau permet de mettre de la non linéarité, elle consiste à : plonger les données dans un nouvel espace appelé espace de représentation ou feature space ; appliquer une svm linéaire dans ce nouvel espace. Le terme astuce vient du fait que ce procédé ne nécessite pas de connaître explicitement ce nouvel espace : pour résoudre le problème d’optimisation dans le feature space on a juste besoin de connaître le noyau associé au feature space. D’un point de vu formel un noyau est une fonction \\[K:\\mathcal X\\times\\mathcal X\\to\\mathbb R\\] dont les propriétés sont proches d’un produit scalaire. Il existe donc tout un tas de noyau avec lesquels on peut faire des SVM, par exemple Linéaire (sur \\(\\mathbb R^d\\)) : \\(K(x,x&#39;)=x^tx&#39;\\). Polynomial (sur \\(\\mathbb R^d\\)) : \\(K(x,x&#39;)=(x^tx&#39;+1)^d\\). Gaussien (Gaussian radial basis function ou RBF) (sur \\(\\mathbb R^d\\)) \\[K(x,x&#39;)=\\exp\\left(-\\frac{\\|x-x&#39;\\|}{2\\sigma^2}\\right).\\] Laplace (sur \\(\\mathbb R\\)) : \\(K(x,x&#39;)=\\exp(-\\gamma|x-x&#39;|)\\). Noyau min (sur \\(\\mathbb R^+\\)) : \\(K(x,x&#39;)=\\min(x,x&#39;)\\). … Bien entendu, en pratique tout le problème va consister à trouver le bon noyau ! On considère le jeu de données suivant où le problème est d’expliquer \\(Y\\) par \\(V1\\) et \\(V2\\). n &lt;- 500 set.seed(13) X &lt;- matrix(runif(n*2,-2,2),ncol=2) %&gt;% as.data.frame() Y &lt;- rep(0,n) cond &lt;- (X$V1^2+X$V2^2)&lt;=2.8 Y[cond] &lt;- rbinom(sum(cond),1,0.9) Y[!cond] &lt;- rbinom(sum(!cond),1,0.1) df &lt;- X %&gt;% mutate(Y=as.factor(Y)) ggplot(df)+aes(x=V2,y=V1,color=Y)+geom_point()+theme_classic() Ajuster une svm linéaire et visualiser l’hyperplan séparateur. Que remarquez-vous ? mod.svm0 &lt;- svm(Y~.,data=df,kernel=&quot;linear&quot;,cost=1) plot(mod.svm0,df,grid=250) La svm linéaire ne permet pas de séparer les groupes (on pouvait s’y attendre). Exécuter la commande suivante et commenter la sortie. mod.svm1 &lt;- svm(Y~.,data=df,kernel=&quot;radial&quot;,gamma=1,cost=1) plot(mod.svm1,df,grid=250) Le noyau radial permet de mettre en évidence une séparation non linéaire. Faire varier les paramètres gamma et cost. Interpréter (on pourra notamment étudier l’évolution du nombre de vecteurs supports en fonction du paramètre cost). mod.svm2 &lt;- svm(Y~.,data=df,kernel=&quot;radial&quot;,gamma=1,cost=0.0001) mod.svm3 &lt;- svm(Y~.,data=df,kernel=&quot;radial&quot;,gamma=1,cost=1) mod.svm4 &lt;- svm(Y~.,data=df,kernel=&quot;radial&quot;,gamma=1,cost=100000) plot(mod.svm2,df,grid=250) plot(mod.svm3,df,grid=250) plot(mod.svm4,df,grid=250) mod.svm2$nSV [1] 244 244 mod.svm3$nSV [1] 114 114 mod.svm4$nSV [1] 78 77 Le nombre de vecteurs supports diminue lorsque \\(C\\) augmente. Une forte valeur de \\(C\\) autorise moins d’observations à être dans la marge, elle a donc tendance à diminuer (risque de surapprentissage). Sélectionner automatiquement ces paramètres. On pourra utiliser la fonction tune en faisant varier C dans c(0.1,1,10,100,1000) et gamma dans c(0.5,1,2,3,4). set.seed(1234) tune.out &lt;- tune(svm,Y~.,data=df,kernel=&quot;radial&quot;, ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4))) summary(tune.out) Parameter tuning of &#39;svm&#39;: - sampling method: 10-fold cross validation - best parameters: cost gamma 10 0.5 - best performance: 0.108 - Detailed performance results: cost gamma error dispersion 1 1e-01 0.5 0.182 0.04565572 2 1e+00 0.5 0.148 0.03155243 3 1e+01 0.5 0.108 0.03425395 4 1e+02 0.5 0.116 0.03373096 5 1e+03 0.5 0.112 0.03425395 6 1e-01 1.0 0.184 0.04402020 7 1e+00 1.0 0.120 0.03651484 8 1e+01 1.0 0.120 0.03126944 9 1e+02 1.0 0.112 0.03155243 10 1e+03 1.0 0.120 0.03887301 11 1e-01 2.0 0.170 0.04136558 12 1e+00 2.0 0.124 0.02458545 13 1e+01 2.0 0.122 0.03457681 14 1e+02 2.0 0.124 0.03502380 15 1e+03 2.0 0.142 0.03705851 16 1e-01 3.0 0.160 0.03651484 17 1e+00 3.0 0.124 0.02458545 18 1e+01 3.0 0.126 0.03134042 19 1e+02 3.0 0.132 0.04022161 20 1e+03 3.0 0.166 0.03272783 21 1e-01 4.0 0.154 0.03777124 22 1e+00 4.0 0.124 0.02458545 23 1e+01 4.0 0.126 0.03134042 24 1e+02 4.0 0.138 0.04467164 25 1e+03 4.0 0.190 0.05754226 La sélection est faite en minimisant l’erreur de classification par validation croisée 10 blocs. Faire de même avec caret, on utilisera method=“svmRadial” et prob.model=TRUE. library(caret) C &lt;- c(0.001,0.01,1,10,100,1000) sigma &lt;- c(0.5,1,2,3,4) gr &lt;- expand.grid(C=C,sigma=sigma) ctrl &lt;- trainControl(method=&quot;cv&quot;) res.caret1 &lt;- train(Y~.,data=df,method=&quot;svmRadial&quot;,trControl=ctrl,tuneGrid=gr,prob.model=TRUE) res.caret1 Support Vector Machines with Radial Basis Function Kernel 500 samples 2 predictor 2 classes: &#39;0&#39;, &#39;1&#39; No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 449, 450, 451, 450, 449, 450, ... Resampling results across tuning parameters: C sigma Accuracy Kappa 1e-03 0.5 0.8359976 0.6734429 1e-03 1.0 0.8439584 0.6890006 1e-03 2.0 0.8398359 0.6806352 1e-03 3.0 0.8578816 0.7164180 1e-03 4.0 0.8577623 0.7162786 1e-02 0.5 0.8400384 0.6813511 1e-02 1.0 0.8419584 0.6851382 1e-02 2.0 0.8458784 0.6926478 1e-02 3.0 0.8518407 0.7044624 1e-02 4.0 0.8577623 0.7162786 1e+00 0.5 0.8676871 0.7347434 1e+00 1.0 0.8857719 0.7713024 1e+00 2.0 0.8838127 0.7672737 1e+00 3.0 0.8798519 0.7594972 1e+00 4.0 0.8838928 0.7675507 1e+01 0.5 0.8798095 0.7596483 1e+01 1.0 0.8818111 0.7634823 1e+01 2.0 0.8838928 0.7676862 1e+01 3.0 0.8778503 0.7554248 1e+01 4.0 0.8720480 0.7438434 1e+02 0.5 0.8818111 0.7635668 1e+02 1.0 0.8839320 0.7677447 1e+02 2.0 0.8680480 0.7359331 1e+02 3.0 0.8517231 0.7031601 1e+02 4.0 0.8377591 0.6749363 1e+03 0.5 0.8760088 0.7521217 1e+03 1.0 0.8760496 0.7520939 1e+03 2.0 0.8498816 0.6998254 1e+03 3.0 0.8297967 0.6590033 1e+03 4.0 0.8100352 0.6192088 Accuracy was used to select the optimal model using the largest value. The final values used for the model were sigma = 1 and C = 1. On peut également répéter plusieurs fois la validation croisée pour stabiliser les résultats (on parallélise avec doParallel) : library(doParallel) ## pour paralléliser cl &lt;- makePSOCKcluster(4) registerDoParallel(cl) set.seed(12345) ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;,number=10,repeats=5) res.caret2 &lt;- train(Y~.,data=df,method=&quot;svmRadial&quot;,trControl=ctrl,tuneGrid=gr,prob.model=TRUE) on.exit(stopCluster(cl)) res.caret2 res.caret2 Support Vector Machines with Radial Basis Function Kernel 500 samples 2 predictor 2 classes: &#39;0&#39;, &#39;1&#39; No pre-processing Resampling: Cross-Validated (10 fold, repeated 5 times) Summary of sample sizes: 450, 449, 450, 451, 450, 449, ... Resampling results across tuning parameters: C sigma Accuracy Kappa 1e-03 0.5 0.8222641 0.6462662 1e-03 1.0 0.8458598 0.6928018 1e-03 2.0 0.8507000 0.7022920 1e-03 3.0 0.8555163 0.7118454 1e-03 4.0 0.8607898 0.7222353 1e-02 0.5 0.8242566 0.6502516 1e-02 1.0 0.8462519 0.6935931 1e-02 2.0 0.8499238 0.7007974 1e-02 3.0 0.8543078 0.7094622 1e-02 4.0 0.8639097 0.7284572 1e+00 0.5 0.8640626 0.7275460 1e+00 1.0 0.8839933 0.7678344 1e+00 2.0 0.8843858 0.7685479 1e+00 3.0 0.8823531 0.7644392 1e+00 4.0 0.8799766 0.7596759 1e+01 0.5 0.8848178 0.7696785 1e+01 1.0 0.8803851 0.7606211 1e+01 2.0 0.8775757 0.7549666 1e+01 3.0 0.8751989 0.7501291 1e+01 4.0 0.8727989 0.7453460 1e+02 0.5 0.8815531 0.7631204 1e+02 1.0 0.8751107 0.7501217 1e+02 2.0 0.8743443 0.7484731 1e+02 3.0 0.8615653 0.7229593 1e+02 4.0 0.8507228 0.7011391 1e+03 0.5 0.8803600 0.7607328 1e+03 1.0 0.8731277 0.7462531 1e+03 2.0 0.8499715 0.7000011 1e+03 3.0 0.8319834 0.6640949 1e+03 4.0 0.8089513 0.6174340 Accuracy was used to select the optimal model using the largest value. The final values used for the model were sigma = 0.5 and C = 10. Visualiser la règle sélectionnée. caret utilise la fonction ksvm du package kernlab. Ce package propose un choix plus large pour les noyaux. Par conséquent, si on souhaite visualiser la svm sélectionnée par caret, il est préférable d’utiliser cette fonction. library(kernlab) C.opt &lt;- res.caret2$bestTune$C sigma.opt &lt;- res.caret2$bestTune$sigma svm.sel &lt;- ksvm(Y~.,data=df,kernel=&quot;rbfdot&quot;,kpar=list(sigma=sigma.opt),C=C.opt) plot(svm.sel,data=df) 4.4 Support vector régression Dans un contexte de régression (lorsque \\(y_i\\in\\mathbb R\\)), on ne recherche plus la l’hyperplan qui va séparer au mieux. On va dans ce cas là cherche à approcher au mieux les valeurs de \\(y_i\\). Cela revient à chercher \\(w\\in\\mathbb R^p\\) et \\(b\\in\\mathbb R\\) tels que \\[|\\langle w,x_i\\rangle+b-y_i|\\leq \\varepsilon\\] avec \\(\\varepsilon&gt;0\\) petit à choisir par l’utilisateur. Par analogie avec la SVM binaire, on va ainsi chercher \\((w,b)\\) qui minimisent \\[\\frac{1}{2}\\|w\\|^2\\] \\[\\textrm{sous les contraintes } |y_i-\\langle w,x_i\\rangle-b|\\leq \\varepsilon,\\ i=1,\\dots,n,\\] Les contraintes impliquent que toute les observations doivent se définir dans une marge ou bande de taille \\(2\\varepsilon\\). Cette hypothèse peut amener l’utilisateur à utiliser des valeurs de \\(\\varepsilon\\) très grandes et empêcher la solution de bien ajuster le nuage de points. Pour pallier à cela, on introduit, comme dans le cas de la SVM binaire, des variables ressorts qui vont autoriser certaines observations à se situer en dehors de la marge. Le problème revient alors à trouver \\((w,b,\\xi,\\xi^\\star)\\) qui minimise \\[\\frac{1}{2}\\|w\\|^2+C\\sum_{i=1}^n(\\xi_i+\\xi_i^\\star)\\] \\[\\textrm{sous les contraintes } \\left\\{ \\begin{array}{l} y_i-\\langle w,x_i\\rangle-b\\leq \\varepsilon+\\xi_i,\\ i=1,\\dots,n,\\\\ \\langle w,x_i\\rangle+b-y_i\\leq \\varepsilon+\\xi_i^\\star,\\ i=1,\\dots,n \\\\ \\xi_i\\geq 0,\\xi_i^\\star\\geq 0,\\ i=1,\\dots,n \\end{array}\\right. \\] Les solutions s’obtiennent exactement de la même façon que dans le cas binaire. On montre notamment que \\(w^\\star\\) s’écrit comme une combinaison linéaire de vecteurs supports : \\[w^\\star=\\sum_{i=1}^n(\\alpha_i^\\star-\\alpha_i)x_i.\\] Les vecteurs supports sont les observations vérifiant \\(\\alpha_i^\\star-\\alpha_i\\neq 0\\). Ici encore il faudra calibrer le paramètre \\(C\\) et on pourra utiliser l’astuce du noyau. On considère le nuage de points \\((x_i,y_i),i=1,\\dots,n\\) définie ci-dessous: set.seed(321) n &lt;- 30 X &lt;- runif(n) eps &lt;- rnorm(n,0,0.2) Y &lt;- 1+X+eps df &lt;- data.frame(X,Y) p1 &lt;- ggplot(df)+aes(x=X,y=Y)+geom_point() p1 On souhaite faire une SVR permettant de prédire \\(Y\\) par \\(X\\). On peut l’obtenir sur R toujours avec la fonction svm de e1071: svr1 &lt;- svm(Y~.,data=df,kernel=&quot;linear&quot;,epsilon=0.5,cost=100,scale=FALSE) On choisit ici exceptionnellement de ne pas réduire les \\(X\\). Écrire une fonction R qui, à partir d’un objet svm, calcule l’équation de la droite de la SVR. Cette fonction pourra également tracer cette doite ainsi que la marge. droite_svr &lt;- function(svr,df){ SV &lt;- df %&gt;% slice(svr$index) w &lt;- sum(svr$coefs*SV[,1]) b &lt;- -svr$rho p &lt;- ggplot(df) + aes(x=X,y=Y)+geom_point()+ geom_point(data=SV,color=&quot;red&quot;)+ geom_abline(slope=w,intercept=c(b))+ geom_abline(slope=w,intercept=c(b-svr$epsilon,b+svr$epsilon),color=&quot;red&quot;) return(list(w=w,b=b,graph=p)) } svr11 &lt;- droite_svr(svr1,df) svr11$graph Comparer la SVR précédente avec celle utilisant epsilon=0.7. svr2 &lt;- svm(Y~.,data=df,kernel=&quot;linear&quot;,epsilon=0.7,cost=100,scale=FALSE) svr22 &lt;- droite_svr(svr2,df) svr22$graph La droite traverse moins bien le nuage de points pour cette valeur de \\(\\varepsilon\\) qui semble trop grande. On ajoute le point de coordonnées \\((0.05,3)\\) aux données. Discuter de la SVR pour ce nouveau jeu de données en utilisant plusieurs valeurs pour C et epsilon. df1 &lt;- df %&gt;% bind_rows(data.frame(X=0.05,Y=3)) On commence par faire grandir la valeur de epsilon pour que toutes les observations soient dans la marge. eps3 &lt;- 0.5*(3-min(df1$Y))-0.01 svr3 &lt;- svm(Y~.,data=df1,kernel=&quot;linear&quot;,epsilon=eps3,cost=100,scale=FALSE) svr33 &lt;- droite_svr(svr3,df1) svr33$graph Toutes les observations sont bien dans la marge mais la règle n’est clairement pas pertinente. Il est préférable d’autoriser certaines observations à se situer en dehors de la marge, par exemple : svr4 &lt;- svm(Y~.,data=df1,kernel=&quot;linear&quot;,epsilon=0.5,cost=100,scale=FALSE) svr44 &lt;- droite_svr(svr4,df1) svr44$graph 4.5 SVM sur les données spam On considère le jeu de données spam où le problème est d’expliquer la variable type par les autres. data(spam) summary(spam$type) nonspam spam 2788 1813 On veut comparer plusieurs svm en utilisant le package kernlab. On pourra trouver un descriptif du package à cette adresse https://www.jstatsoft.org/article/view/v011i09. Utiliser la fonction ksvm pour faire une svm linéaire et une svm à noyau gaussien. On prendra comme paramètre 1 pour C et pour le paramètre du noyau gaussien. La svm linéaire correspond au noyau polynomial avec des valeurs de paramètres particulières : svm.lin &lt;- ksvm(type~.,data=spam,kernel=&quot;polydot&quot;,C=1,kpar=list(degree=1,scale=1,offset=0)) Pour le noyau gaussien, il suffit d’utiliser l’option kernel=“rbfdot” : svm.gauss &lt;- ksvm(type~.,data=spam,kernel=&quot;rbfdot&quot;,C=1,kpar=list(sigma=1)) Évaluer la performance des 2 svm précédentes en calculant l’erreur de classification par validation croisée 5 blocs. Comparer ces deux algorithmes. Il suffit d’utiliser l’option cross dans ksvm. svm.lin &lt;- ksvm(type~.,data=spam,kernel=&quot;polydot&quot;,C=1, kpar=list(degree=1,scale=1,offset=0),cross=5) svm.gauss &lt;- ksvm(type~.,data=spam,kernel=&quot;rbfdot&quot;,C=1, kpar=list(sigma=1),cross=5) svm.lin Support Vector Machine object of class &quot;ksvm&quot; SV type: C-svc (classification) parameter : cost C = 1 Polynomial kernel function. Hyperparameters : degree = 1 scale = 1 offset = 0 Number of Support Vectors : 942 Objective Function Value : -881.4942 Training error : 0.067377 Cross validation error : 0.071942 svm.gauss Support Vector Machine object of class &quot;ksvm&quot; SV type: C-svc (classification) parameter : cost C = 1 Gaussian Radial Basis kernel function. Hyperparameter : sigma = 1 Number of Support Vectors : 3881 Objective Function Value : -1457.267 Training error : 0.005868 Cross validation error : 0.199957 On remarque que l’erreur de classification est plus faible pour la svm linéaire. De plus, il y a un gros écart entre l’erreur de prévision et l’erreur d’ajustement pour le noyau gaussien, il est fort possible que l’on soit en sur-apprentissage avec ces valeurs de paramètres. Refaire la svm à noyau gaussien avec l’option kpar='automatic'. Expliquer. svm.gauss &lt;- ksvm(type~.,data=spam,kernel=&quot;rbfdot&quot;,C=1,kpar=&#39;automatic&#39;,cross=5) svm.gauss Support Vector Machine object of class &quot;ksvm&quot; SV type: C-svc (classification) parameter : cost C = 1 Gaussian Radial Basis kernel function. Hyperparameter : sigma = 0.0298786979455573 Number of Support Vectors : 1412 Objective Function Value : -807.0795 Training error : 0.045642 Cross validation error : 0.068461 Le paramètre du noyau est ici calibré à partir d’une heuristique. La valeur choisie semble pertinente puisque l’erreur de prévision a diminué et est maintenant proche de l’erreur d’ajustement. On s’intéresse maintenant à l’AUC. À partir de validation croisée, sélectionner un noyau (linéaire ou gaussien) ainsi que des valeurs de paramètres associés au noyau, sans oublier le paramètre C. On pourra utiliser le package caret et comparer le résultat obtenu à celui d’une forêt aléatoire. Il faut tout d’abord définir des grilles. On peut consulter la page https://topepo.github.io/caret/available-models.html pour identifier les identifiants des paramètres. On fait les choix suivants : C &lt;- c(0.01,0.1,1,10) degree &lt;- c(1,2,3) scale &lt;- 1 gr.lin &lt;- expand.grid(C=C,degree=degree,scale=scale) sigma &lt;- c(0.001,0.01,0.05,0.2,1) gr.gauss &lt;- expand.grid(C=C,sigma=sigma) spam1 &lt;- spam names(spam1)[ncol(spam)] &lt;- &quot;Class&quot; levels(spam1$Class) &lt;- c(&quot;G0&quot;,&quot;G1&quot;) ctrl &lt;- trainControl(method=&quot;cv&quot;,classProbs=TRUE,summary=twoClassSummary) cl &lt;- makePSOCKcluster(4) registerDoParallel(cl) set.seed(12345) lin.caret &lt;- train(Class~.,data=spam1,method=&quot;svmPoly&quot;,trControl=ctrl, tuneGrid=gr.lin,prob.model=TRUE,metric=&quot;ROC&quot;) gauss.caret &lt;- train(Class~.,data=spam1,method=&quot;svmRadial&quot;,trControl=ctrl, tuneGrid=gr.gauss,prob.model=TRUE,metric=&quot;ROC&quot;) on.exit(stopCluster(cl)) On obtient les erreurs suivantes : lin.caret Support Vector Machines with Polynomial Kernel 4601 samples 57 predictor 2 classes: &#39;G0&#39;, &#39;G1&#39; No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 4141, 4141, 4141, 4142, 4141, 4141, ... Resampling results across tuning parameters: C degree ROC Sens Spec 0.01 1 0.9666560 0.9523014 0.8731316 0.01 2 0.9664432 0.9594814 0.8764495 0.01 3 0.9521303 0.9701243 0.7221480 0.10 1 0.9704662 0.9547169 0.8813900 0.10 2 0.9550894 0.9569686 0.8588155 0.10 3 0.9461588 0.9605620 0.7144308 1.00 1 0.9717055 0.9576842 0.8836045 1.00 2 0.9432383 0.9565154 0.7448356 1.00 3 0.9362428 0.9687605 0.4081459 10.00 1 0.9719611 0.9569648 0.8902313 10.00 2 0.9428121 0.9569796 0.7255441 10.00 3 0.9295425 0.9753488 0.3673047 Tuning parameter &#39;scale&#39; was held constant at a value of 1 ROC was used to select the optimal model using the largest value. The final values used for the model were degree = 1, scale = 1 and C = 10. gauss.caret Support Vector Machines with Radial Basis Function Kernel 4601 samples 57 predictor 2 classes: &#39;G0&#39;, &#39;G1&#39; No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 4141, 4141, 4141, 4140, 4141, 4142, ... Resampling results across tuning parameters: C sigma ROC Sens Spec 0.01 0.001 0.9421579 0.8575772 0.9100965 0.01 0.010 0.9497898 0.8468116 0.9205634 0.01 0.050 0.9382280 0.8687050 0.8935371 0.01 0.200 0.9400169 0.7754442 0.9503612 0.01 1.000 0.9438293 0.9906771 0.5730678 0.10 0.001 0.9472419 0.9257484 0.8770050 0.10 0.010 0.9630276 0.9497808 0.8742305 0.10 0.050 0.9581472 0.9472641 0.8438953 0.10 0.200 0.9445127 0.9235966 0.8378696 0.10 1.000 0.9441026 0.9913940 0.5708609 1.00 0.001 0.9619480 0.9465537 0.8753445 1.00 0.010 0.9735428 0.9583868 0.8952007 1.00 0.050 0.9732358 0.9562389 0.8896910 1.00 0.200 0.9604489 0.9637774 0.7915366 1.00 1.000 0.9443214 0.9939055 0.5543136 10.00 0.001 0.9699282 0.9558766 0.8929937 10.00 0.010 0.9795221 0.9609010 0.9029264 10.00 0.050 0.9711805 0.9548052 0.8896940 10.00 0.200 0.9594052 0.9630579 0.7876723 10.00 1.000 0.9425738 0.9881695 0.5543106 ROC was used to select the optimal model using the largest value. The final values used for the model were sigma = 0.01 and C = 10. et les valeurs de paramètres sélectionnés pour chaque SVM : lin.caret$bestTune degree scale C 10 1 1 10 gauss.caret$bestTune sigma C 17 0.01 10 On remarque que les AUC associés à ces deux jeux de paramètres sont très proches. On compare à une forêt aléatoire avec les paramètres par défaut : gr.foret &lt;- expand.grid(data.frame(mtry=7)) set.seed(123) foret.caret &lt;- train(Class~.,data=spam1,method=&quot;rf&quot;,trControl=ctrl, tuneGrid=gr.foret,metric=&quot;ROC&quot;) foret.caret Random Forest 4601 samples 57 predictor 2 classes: &#39;G0&#39;, &#39;G1&#39; No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 4141, 4140, 4141, 4141, 4140, 4141, ... Resampling results: ROC Sens Spec 0.9870745 0.971312 0.9260701 Tuning parameter &#39;mtry&#39; was held constant at a value of 7 La forêt aléatoire est (légèrement) plus pertinente en terme d’AUC. 4.6 Exercices Exercice 4.1 (Résolution du problème d’optimisation dans le cas séparable) On considère \\(n\\) observations \\((x_1,y_1),\\dots,(x_n,y_n)\\) telles que \\((x_i,y_i)\\in\\mathbb R^p\\times\\{-1,1\\}\\). On cherche à expliquer la variable \\(Y\\) par \\(X\\). On considère l’algorithme SVM et on se place dans le cas où les données sont séparables. Soit \\(\\mathcal H\\) un hyperplan séparateur d’équation \\(\\langle w,x\\rangle+b=0\\) où \\(w\\in\\mathbb R^p,b\\in\\mathbb R\\). Exprimer la distance entre \\(x_i,i=1,\\dots,n\\) et \\(\\mathcal H\\) en fonction de \\(w\\) et \\(b\\). Soit \\(x_0\\in\\mathcal H\\). La solution correspond à la norme du projeté orthogonal de \\(x-x_0\\) sur \\(\\mathcal H\\), elle est donc colinéaire à \\(w\\) (car \\(w\\) est normal à \\(\\mathcal H\\)) et s’écrit \\[\\frac{\\langle x-x_0,w\\rangle}{\\|w\\|^2}w=\\frac{\\langle x,w\\rangle}{\\|w\\|^2}w-\\frac{\\langle x_0,w\\rangle}{\\|w\\|^2}w,\\] Comme \\(\\langle x_0,w\\rangle=-b\\), on déduit que, si \\(\\|w\\|=1\\), alors \\[d_{\\mathcal H}(x)=\\frac{|\\langle w,x\\rangle+b|}{\\|w\\|}=|\\langle w,x\\rangle+b|=(\\langle w,x\\rangle+b)y\\] si \\(y=\\text{signe}(\\langle w,x\\rangle+b)\\). Expliquer la logique du problème d’optimisation \\[\\max_{w,b,\\|w\\|=1}M\\] \\[\\textrm{sous les contraintes } y_i(\\langle w,x_i\\rangle+b)\\geq M,\\ i=1,\\dots,n.\\] Si \\((w,b)\\) est un hyperplan séparateur sa marge vaut \\[\\min_{i=1,\\dots,n}y_i(\\langle w,x_i\\rangle+b).\\] Le problème proposé revient donc à chercher l’hyperplan : qui sépare les groupes ; tel que la distance entre les observations et lui soit maximale. Montrer que ce problème peut se réécrire \\[\\min_{w,b}\\frac{1}{2}\\|w\\|^2\\] \\[\\textrm{sous les contraintes } y_i(\\langle w,x_i\\rangle+b)\\geq 1,\\ i=1,\\dots,n.\\] Il suffit de poser comme contrainte \\(M=1/\\|w\\|\\). On rappelle que pour la minimisation d’une fonction \\(h:\\mathbb R^p\\to\\mathbb R\\) sous contraintes affines \\(g_i(u)\\geq 0,i=1,\\dots,n\\), le Lagrangien s’écrit \\[L(u,\\alpha)=h(u)-\\sum_{i=1}^n\\alpha_ig_i(u).\\] Si on désigne par \\(u_\\alpha=\\mathop{\\mathrm{argmin}}_uL(u,\\alpha)\\), la fonction duale est alors donnée par \\[\\theta(\\alpha)=L(u_\\alpha,\\alpha)=\\min_{u\\in\\mathbb R^p}L(u,\\alpha),\\] et le problème dual consiste à maximiser \\(\\theta(\\alpha)\\) sous les contraintes \\(\\alpha_i\\geq 0\\). En désignant par \\(\\alpha^\\star\\) la solution de ce problème, on déduit la solution du problème primal \\(u^\\star=u_{\\alpha^\\star}\\). Les conditions de Karush-Kuhn-Tucker sont données par \\(\\alpha_i^\\star\\geq 0\\). \\(g_i(u_{\\alpha^\\star})\\geq 0\\). \\(\\alpha_i^\\star g_i(u_{\\alpha^\\star})=0\\). Écrire le Lagrangien du problème considéré et en déduire une expression de \\(w\\) en fonction des \\(\\alpha_i\\) et des observations. Le lagrangien s’écrit \\[L(w,b;\\alpha)=\\frac{1}{2}\\|w\\|^2-\\sum_{i=1}^n\\alpha_i[y_i(\\langle w,x_i\\rangle+b)-1].\\] On a alors \\[\\frac{\\partial L(w,b;\\alpha)}{\\partial w}=w-\\sum_{i=1}^n\\alpha_iy_ix_i=0\\] et \\[\\frac{\\partial L(w,b;\\alpha)}{\\partial b}=-\\sum_{i=1}^n\\alpha_iy_i=0.\\] D’où \\(w_\\alpha=\\sum_{i=1}^n\\alpha_iy_ix_i\\). Écrire la fonction duale. La fonction duale s’écrit \\[\\begin{align*} \\theta(\\alpha)=L(w_\\alpha,b_\\alpha;\\alpha)= &amp;\\ \\frac{1}{2}\\langle \\sum_i\\alpha_iy_ix_i,\\sum_j\\alpha_jy_jx_j\\rangle-\\sum_i\\alpha_iy_i\\langle \\sum_j\\alpha_jy_jx_j,x_i\\rangle-\\sum_i\\alpha_iy_ib+\\sum_i\\alpha_i \\\\ = &amp;\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_j\\langle x_i,x_j\\rangle \\end{align*}\\] On note \\(\\alpha_i^*\\) les valeurs de \\(\\alpha_i\\) qui maximisent \\(\\theta(\\alpha)\\). Écrire les conditions KKT et en déduire les solutions \\(w^\\star\\) et \\(b^\\star\\). On peut déjà écrire \\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\] Les conditions KKT sont pour tout \\(i=1,\\dots,n\\) : \\[\\alpha_i^\\star\\geq 0 \\quad\\text{et}\\quad \\alpha_i^\\star[y_i(\\langle w^\\star,x_i\\rangle+b^\\star)-1]=0.\\] On obtient \\(b^\\star\\) en résolvant \\[\\alpha_i^\\star[y_i(\\langle w^\\star,x_i\\rangle+b^\\star)-1]=0\\] pour un \\(\\alpha_i^\\star\\) non nul. Interpréter les conditions KKT. Les \\(x_i\\) tels que \\(\\alpha_i^\\star&gt;0\\) vérifient \\[y_i(\\langle w^\\star,x_i\\rangle+b^\\star)=1.\\] Ils se situent donc sur la frontière définissant la marge maximale. Ce sont les vecteurs supports. Exercice 4.2 (Règle svm à partir de sorties R) On considère \\(n\\) observations \\((x_1,y_1),\\dots,(x_n,y_n)\\) telles que \\((x_i,y_i)\\in\\mathbb R^3\\times\\{-1,1\\}\\). On cherche à expliquer la variable \\(Y\\) par \\(X=(X_1,X_2,X_3)\\). On considère l’algorithme SVM et on se place dans le cas où les données sont séparables. On rappelle que cet algorithme consiste à chercher une droite d’équation \\(w^tx+b=0\\) où \\((w,b)\\in\\mathbb R^3\\times\\mathbb R\\) sont solutions du problème d’optimisation (problème primal) \\[\\min_{w,b}\\frac{1}{2}\\|w\\|^2\\] \\[\\textrm{sous les contraintes } y_i(w^tx_i+b)\\geq 1,\\ i=1,\\dots,n.\\] On désigne par \\(\\alpha_i^\\star,i=1,\\dots,n\\), les solutions du problème dual et par \\((w^\\star,b^\\star)\\) les solutions du problème ci-dessus. Donner la formule permettant de calculer \\(w^\\star\\) en fonction des \\(\\alpha_i^\\star\\). \\(w^\\star\\) se calcule selon \\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\] Les \\(\\alpha_i^\\star\\) étant nuls pour les vecteurs non supports, il suffit de sommer sur les vecteur supports. Expliquer comment on classe un nouveau point \\(x\\in\\mathbb R^3\\) par la méthode svm. Une fois \\(w^\\star\\) et \\(b^\\star\\) obtenus, la règle s’écrit \\[g(x)=1_{\\langle w^\\star,x\\rangle+b^\\star\\leq 0}-1_{\\langle w^\\star,x\\rangle+b^\\star&gt;0}.\\] Les données se trouvent dans un dataframe df. On exécute set.seed(1234) n &lt;- 100 X &lt;- data.frame(X1=runif(n),X2=runif(n),X3=runif(n)) X &lt;- data.frame(X1=scale(runif(n)),X2=scale(runif(n)),X3=scale(runif(n))) Y &lt;- rep(-1,100) Y[X[,1]&lt;X[,2]] &lt;- 1 #Y &lt;- (apply(X,1,sum)&lt;=0) %&gt;% as.numeric() %&gt;% as.factor() df &lt;- data.frame(X,Y=as.factor(Y)) mod.svm &lt;- svm(Y~.,data=df,kernel=&quot;linear&quot;,cost=10000000000) et on obtient df[mod.svm$index,] X1 X2 X3 Y 51 -1.1 -1.0 -1.0 1 92 0.7 0.8 1.1 1 31 0.7 0.5 -1.0 -1 37 -0.5 -0.6 0.3 -1 mod.svm$coefs [,1] [1,] 59 [2,] 49 [3,] -30 [4,] -79 mod.svm$rho [1] -0.5 Calculer les valeurs de \\(w^\\star\\) et \\(b^\\star\\). En déduire la règle de classification. \\(b^\\star\\) est l’opposé de mod.svm$rho. Pour \\(w^\\star\\) il suffit d’appliquer la formule et on trouve X1 X2 X3 -12.1 12.6 1.2 On dispose d’une nouvelle observation \\(x=(1,-0.5,-1)\\). Dans quel groupe (-1 ou 1) l’algorithme affecte cette nouvelle donnée ? On calcule la combinaison linéaire \\(\\langle w^\\star,x\\rangle+b^\\star\\) : newX &lt;- data.frame(X1=1,X2=-0.5,X3=-1) sum(w*newX)+b [1] -19.1 On affectera donc la nouvelle donnée au groupe -1. "],["agregation.html", "Chapitre 5 Agrégation : forêts aléatoires et gradient boosting 5.1 Forêts aléatoires 5.2 Gradient boosting", " Chapitre 5 Agrégation : forêts aléatoires et gradient boosting Les méthodes par arbres présentées précédemment sont des algorithmes qui possèdent tout un tas de qualités (facile à mettre en œuvre, interprétable…). Ce sont néanmoins rarement les algorithmes qui se révèlent les plus performants. Les méthodes d’agrégation d’arbres présentées dans cette partie sont souvent beaucoup plus pertinentes, notamment en terme de qualité de prédiction. Elles consistent à construire un très grand nombre d’arbres “simples” : \\(g_1,\\dots,g_B\\) et à les agréger en faisant la moyenne : \\[\\frac{1}{B}\\sum_{k=1}^Bg_k(x).\\] Les forêts aléatoires (Breiman 2001) et le gradient boosting (Friedman 2001) utilisent ce procédé d’agrégation. 5.1 Forêts aléatoires L’algorithme des forêts aléatoires consiste à construire des arbres sur des échantillons bootstrap et à les agréger. Il peut s’écrire de la façon suivante : Entrées : \\(x\\in\\mathbb R^d\\) l’observation à prévoir, \\(\\mathcal D_n\\) l’échantillon ; \\(B\\) nombre d’arbres ; \\(n_{max}\\) nombre max d’observations par nœud \\(m\\in\\{1,\\dots,d\\}\\) le nombre de variables candidates pour découper un nœud. Algorithme : pour \\(k=1,\\dots,B\\) : Tirer un échantillon bootstrap dans \\(\\mathcal D_n\\) Construire un arbre CART sur cet échantillon bootstrap, chaque coupure est sélectionnée en minimisant la fonction de coût de CART sur un ensemble de \\(m\\) variables choisies au hasard parmi les \\(d\\). On note \\(T(.,\\theta_k,\\mathcal D_n)\\) l’arbre construit. Sortie : l’estimateur \\(T_B(x)=\\frac{1}{B}\\sum_{k=1}^BT(x,\\theta_k,\\mathcal D_n)\\). Cet algorithme peut être utilisé sur R avec la fonction randomForest du package randomForest. Nous la présentons à travers l’exemple du jeu de données spam du package kernlab. library(kernlab) data(spam) set.seed(1234) spam &lt;- spam[sample(nrow(spam)),] Le problème est d’expliquer la variable binaire type par les autres. A l’aide de la fonction randomForest du package randomForest, ajuster une forêt aléatoire pour répondre au problème posé. On commence par charger le package library(randomForest) Et on construit la forêt avec randomForest : rf1 &lt;- randomForest(type~.,data=spam) Appliquer la fonction plot à l’objet construit avec randomForest et expliquer le graphe obtenu. A quoi peut servir ce graphe en pratique ? plot(rf1) Ce graphe permet de visualiser l’erreur de classication ainsi que les taux de faux positifs et faux négatifs calculés par Out Of Bag en fonction du nombre d’arbres de la forêt. Ce graphe peut être utilisé pour voir si l’algorithme a bien “convergé”. Si ce n’est pas le cas, il faut construire une forêt avec plus d’abres. Construire la forêt avec mtry=1 et comparer ses performances avec celle construite précédemment. rf2 &lt;- randomForest(type~.,data=spam,mtry=1) rf1 Call: randomForest(formula = type ~ ., data = spam) Type of random forest: classification Number of trees: 500 No. of variables tried at each split: 7 OOB estimate of error rate: 4.56% Confusion matrix: nonspam spam class.error nonspam 2711 77 0.02761836 spam 133 1680 0.07335907 rf2 Call: randomForest(formula = type ~ ., data = spam, mtry = 1) Type of random forest: classification Number of trees: 500 No. of variables tried at each split: 1 OOB estimate of error rate: 7.89% Confusion matrix: nonspam spam class.error nonspam 2729 59 0.02116212 spam 304 1509 0.16767788 La forêt rf1 est plus performante en terme d’erreur de classification OOB. Utiliser la fonction train du package caret pour choisir le paramètre mtry dans la grille seq(1,30,by=5). library(caret) grille.mtry &lt;- data.frame(mtry=seq(1,30,by=5)) ctrl &lt;- trainControl(method=&quot;oob&quot;) library(doParallel) ## pour paralléliser cl &lt;- makePSOCKcluster(4) registerDoParallel(cl) set.seed(12345) sel.mtry &lt;- train(type~.,data=spam,method=&quot;rf&quot;,trControl=ctrl,tuneGrid=grille.mtry) on.exit(stopCluster(cl)) On choisit sel.mtry$bestTune mtry 2 6 Construire la forêt avec le paramètre mtry sélectionné. Calculer l’importance des variables et représenter ces importance à l’aide d’un diagramme en barres. rf3 &lt;- randomForest(type~.,data=spam,mtry=unlist(sel.mtry$bestTune),importance=TRUE) Imp &lt;- randomForest::importance(rf3,type=1) %&gt;% as.data.frame() %&gt;% mutate(variable=names(spam)[-58]) %&gt;% arrange(desc(MeanDecreaseAccuracy)) head(Imp) MeanDecreaseAccuracy variable 1 47.58430 charExclamation 2 40.83001 remove 3 40.79968 charDollar 4 40.39225 capitalAve 5 37.18721 free 6 36.17332 edu ggplot(Imp) + aes(x=reorder(variable,MeanDecreaseAccuracy),y=MeanDecreaseAccuracy)+geom_bar(stat=&quot;identity&quot;)+coord_flip()+xlab(&quot;&quot;)+theme_classic() La fonction vip du package vip permet de faire le diagramme en barres plus facilement library(vip) vip(rf3) La fonction ranger du package ranger permet également de calculer des forêts aléatoires. Comparer les temps de calcul de cette fonction avec randomForest library(ranger) system.time(rf4 &lt;- ranger(type~.,data=spam)) user system elapsed 2.171 0.033 0.927 system.time(rf5 &lt;- randomForest(type~.,data=spam)) user system elapsed 8.686 0.090 8.901 Le temps de calcul est plus rapide avec ranger. Ce package permet une implémentation efficace des forêts aléatoires pour des données de grande dimension. on peut touver plus d’information ici. 5.2 Gradient boosting Les algorithmes de gradient boosting permettent de minimiser des pertes empiriques de la forme \\[\\frac{1}{n}\\sum_{i=1}^n\\ell(y_i,f(x_i)).\\] où \\(\\ell:\\mathbb R\\times\\mathbb R\\to\\mathbb R\\) est une fonction de coût convexe en son second argument. Il existe plusieurs type d’algorithmes boosting. Un des plus connus et utilisés a été proposé par Friedman (2001), c’est la version que nous étudions dans cette partie. Cette approche propose de chercher la meilleure combinaison linéaire d’arbres binaires, c’est-à-dire que l’on recherche \\(g(x)=\\sum_{m=1}^M\\alpha_mh_m(x)\\) qui minimise \\[\\mathcal R_n(g)=\\frac{1}{n}\\sum_{i=1}^n\\ell(y_i,g(x_i)).\\] Optimiser sur toutes les combinaisons d’arbres binaires se révélant souvent trop compliqué, Friedman (2001) utilise une descente de gradient pour construire la combinaison d’abres de façon récursive. L’algorithme est le suivant : Entrées : \\(d_n=(x_1,y_1),\\dots,(x_n,y_n)\\) l’échantillon, \\(\\lambda\\) un paramètre de régularisation tel que \\(0&lt;\\lambda\\leq 1\\). \\(M\\in\\mathbb N\\) le nombre d’itérations. paramètres de l’arbre (nombre de coupures…) Itérations : Initialisation : \\(g_0(.)=\\mathop{\\mathrm{argmin}}_c \\frac{1}{n}\\sum_{i=1}^n \\ell(y_i,c)\\) Pour \\(m=1\\) à \\(M\\) : Calculer l’opposé du gradient \\(-\\frac{\\partial}{\\partial g(x_i)}\\ell(y_i,g(x_i))\\) et l’évaluer aux points \\(g_{m-1}(x_i)\\) : \\[U_i=-\\frac{\\partial}{\\partial g(x_i)}\\ell(y_i,g(x_i)) _{\\Big |g(x_i)=g_{m-1}(x_i)},\\quad i=1,\\dots,n.\\] Ajuster un arbre sur l’échantillon \\((x_1,U_1),\\dots,(x_n,U_n)\\), on le note \\(h_m\\). Mise à jour : \\(g_m(x)=g_{m-1}(x)+\\lambda h_m(x)\\). Sortie : la suite \\((g_m(x))_m\\). Sur R On peut utiliser différents packages pour faire du gradient boosting. Nous utilisons ici le package gbm (Ridgeway 2006). 5.2.1 Un exemple simple en régression On considère un jeu de données \\((x_i,y_i),i=1,\\dots,200\\) issu d’un modèle de régression \\[y_i=m(x_i)+\\varepsilon_i\\] où la vraie fonction de régression est la fonction sinus (mais on va faire comme si on ne le savait pas). x &lt;- seq(-2*pi,2*pi,by=0.01) y &lt;- sin(x) set.seed(1234) X &lt;- runif(200,-2*pi,2*pi) Y &lt;- sin(X)+rnorm(200,sd=0.2) df1 &lt;- data.frame(X,Y) df2 &lt;- data.frame(X=x,Y=y) p1 &lt;- ggplot(df1)+aes(x=X,y=Y)+geom_point()+geom_line(data=df2,size=1)+xlab(&quot;&quot;)+ylab(&quot;&quot;) p1 Rappeler ce que siginifie le \\(L_2\\)-boosting. Il s’agit de l’algorithme de gradient boosting présenté ci-dessus appliqué à la fonction de perte \\[\\ell(y,f(x))=\\frac{1}{2}(y-f(x))^2.\\] A l’aide de la fonction gbm du package gbm construire un algorithme de \\(L_2\\)-boosting. On utilisera 500000 itérations et gardera les autres valeurs par défaut de paramètres. library(gbm) L2boost &lt;- gbm(Y~.,data=df1,n.trees = 500000,distribution=&quot;gaussian&quot;,bag.fraction = 1) Visualiser l’estimateur à la première itération. On pourra faire un predict avec l’option n.trees. prev1 &lt;- predict(L2boost,newdata=df2,n.trees=1) df3 &lt;- df2 %&gt;% rename(vraie=Y) %&gt;% mutate(`M=1`=prev1) df4 &lt;- df3 %&gt;% pivot_longer(-X,names_to=&quot;courbes&quot;,values_to=&quot;prev&quot;) ggplot(df4)+aes(x=X,y=prev,color=courbes)+geom_line(size=1) On remarque que l’estimateur est un arbre avec une seule coupure. On aurait aussi pu utiliser : plot(L2boost,n.trees=1) Faire de même pour les itérations 1000 et 500000. prev1000 &lt;- predict(L2boost,newdata=df2,n.trees=1000) prev500000 &lt;- predict(L2boost,newdata=df2,n.trees=500000) df31 &lt;- df3 %&gt;% mutate(`M=1000`=prev1000,`M=500000`=prev500000) df41 &lt;- df31 %&gt;% pivot_longer(-X,names_to=&quot;courbes&quot;,values_to=&quot;prev&quot;) ggplot(df41)+aes(x=X,y=prev,color=courbes)+geom_line(size=1) On surajuste lorsque le nombre d’itérations est trop important. Sélectionner le nombre d’itérations par la procédure de votre choix. On propose de faire une validation hold out. C’est assez facile avec gbm il suffit de renseigner l’option train.fraction de gbm. #parallel:::setDefaultClusterOptions(setup_strategy = &quot;sequential&quot;) L2boost.sel &lt;- gbm(Y~.,data=df1,n.trees = 10000,distribution=&quot;gaussian&quot;, bag.fraction = 1,train.fraction=0.75) gbm.perf(L2boost.sel) [1] 4787 5.2.2 Adaboost et logitboost pour la classification binaire. On considère le jeu de données spam du package kernlab. library(kernlab) data(spam) set.seed(1234) spam &lt;- spam[sample(nrow(spam)),] Exécuter la commande model_ada1 &lt;- gbm(type~.,data=spam,distribution=&quot;adaboost&quot;,interaction.depth=2, shrinkage=0.05,n.trees=500) On obtient le message d’erreur suivant : model_ada1 &lt;- gbm(type~.,data=spam,distribution=&quot;adaboost&quot;,interaction.depth=2, shrinkage=0.05,n.trees=500) Error in gbm.fit(x = x, y = y, offset = offset, distribution = distribution, : This version of AdaBoost requires the response to be in {0,1} Proposer une correction permettant de faire fonctionner l’algorithme. Il est nécessaire que la variable qualitative à expliquer soit codée 0-1 pour adaboost. spam1 &lt;- spam spam1$type &lt;- as.numeric(spam1$type)-1 set.seed(1234) model_ada1 &lt;- gbm(type~.,data=spam1,distribution=&quot;adaboost&quot;,interaction.depth=2, shrinkage=0.05,n.trees=500) Expliciter le modèle ajusté par la commande précédente. L’algorithme gbm est une descente de gradient qui minimise la fonction de perte \\[\\frac{1}{n}\\sum_{i=1}^n \\ell(y_i,g(x_i)).\\] Dans le cas de adaboost on utilise la perte exponentielle : \\(\\ell(y,g(x))=\\exp(-yg(x))\\). Effectuer un summary du modèle ajusté. Expliquer la sortie. summary(model_ada1) var rel.inf charExclamation charExclamation 20.04035224 charDollar charDollar 17.51535261 remove remove 11.51692621 free free 7.49397637 hp hp 6.25654932 capitalLong capitalLong 5.42905223 capitalAve capitalAve 4.69521299 your your 4.23371585 george george 2.50300727 edu edu 2.19692796 our our 1.99655393 money money 1.79063219 email email 1.51773292 capitalTotal capitalTotal 1.43872496 internet internet 1.12579132 receive receive 0.97001932 will will 0.94015881 you you 0.89915372 business business 0.84418397 re re 0.82959153 num1999 num1999 0.80016393 num650 num650 0.79468746 meeting meeting 0.69494729 num000 num000 0.56448978 charRoundbracket charRoundbracket 0.39921437 report report 0.38621968 charSemicolon charSemicolon 0.29835251 credit credit 0.27841575 over over 0.27064075 order order 0.26017226 mail mail 0.22398163 technology technology 0.10340435 hpl hpl 0.10151723 original original 0.09615196 font font 0.09539134 make make 0.08995855 project project 0.07970985 all all 0.05392468 people people 0.05359692 address address 0.04690996 parts parts 0.04260362 conference conference 0.02037549 num85 num85 0.01155488 num3d num3d 0.00000000 addresses addresses 0.00000000 lab lab 0.00000000 labs labs 0.00000000 telnet telnet 0.00000000 num857 num857 0.00000000 data data 0.00000000 num415 num415 0.00000000 pm pm 0.00000000 direct direct 0.00000000 cs cs 0.00000000 table table 0.00000000 charSquarebracket charSquarebracket 0.00000000 charHash charHash 0.00000000 On obtient un indicateur qui permet de mesurer l’importance des variable dans la construction de la méthode. Utiliser la fonction vip du package vip pour retrouver ce sorties. library(vip) vip(model_ada1,num_features = 20L) Sélectionner le nombre d’itérations pour l’algorithme adaboost en faisant de la validation croisée 5 blocs. model_ada2 &lt;- gbm(type~.,data=spam1,distribution=&quot;adaboost&quot;,interaction.depth=2,bag.fraction=1,cv.folds = 5,n.trees=500) gbm.perf(model_ada2) [1] 233 Faire la même procédure en changeant la valeur du paramètre shrinkage. Interpréter. model_ada3 &lt;- gbm(type~.,data=spam1,distribution=&quot;adaboost&quot;,interaction.depth=2,bag.fraction=1,cv.folds = 5,n.trees=500,shrinkage=0.05) gbm.perf(model_ada3) [1] 370 model_ada4 &lt;- gbm(type~.,data=spam1,distribution=&quot;adaboost&quot;,interaction.depth=2,bag.fraction=1,cv.folds = 5,n.trees=500,shrinkage=0.5) gbm.perf(model_ada4) [1] 36 Le nombre d’itérations optimal augmente lorsque shrinkage diminue. C’est logique car ce dernier paramètre contrôle la vitesse de descente de gradient : plus il est grand, plus on minimise vite et moins on itère. Il faut néanmoins veiller à ne pas le prendre trop petit pour avoir un estimateur stable. Ici, 0.05 semble être une bonne valeur. Expliquer la différence entre adaboost et logitboost et précisez comment on peut mettre en œuvre ce dernier algorithme. La seule différence se situe au niveau de la fonction de perte, adaboost utilise \\[\\exp(-yg(x))\\] tandis que logitboost utilise \\[\\log(1+\\exp(-2yg(x)))\\] Avec gbm il faudra utiliser l’option distribution=“bernoulli” pour faire du logitboost, par exemple : set.seed(4321) logitboost &lt;- gbm(type~.,data=spam1,distribution=&quot;bernoulli&quot;,interaction.depth=2,bag.fraction=1,cv.folds = 5,n.trees=500,shrinkage=0.4) gbm.perf(logitboost) [1] 288 5.2.3 Exercices Rappeler la fonction de risque adaboost. La fonction de perte adaboost est \\(\\ell(y,f(x))=\\exp(-yf(x))\\). Le risque est donc \\[\\mathcal R(f)=\\mathbf E[\\exp(-Yf(X))].\\] Montrer que le risque est minimum en \\[f^\\star(x)=\\frac{1}{2}\\log\\frac{\\mathbf P(Y=1|X=x)}{\\mathbf P(Y=-1|X=x)}.\\] On cherche \\(f(x)\\) qui minimise \\(\\mathbf E[\\exp(-Yf(X))|X=x]\\). On a \\[\\mathbf E[\\exp(-Yf(X))|X=x]=\\mathbf P(Y=1|X=x)\\exp(-f(x))+\\mathbf P(Y=-1|X=x)\\exp(f(x)).\\] D’où \\[\\frac{\\partial \\mathbf E[\\exp(-Yf(X))|X=x]}{\\partial f(x)}=-\\mathbf P(Y=1|X=x)\\exp(-f(x))+\\mathbf P(Y=-1|X=x)\\exp(f(x)).\\] La quantité ci-dessus est égale à 0 en \\[f^\\star(x)=\\frac{1}{2}\\log\\frac{\\mathbf P(Y=1|X=x)}{\\mathbf P(Y=-1|X=x)}.\\] Mêmes questions pour le risque logitboost. On cherche ici à minimiser l’opposé de la log-vraisemblance \\[-(y\\log p(x)+(1-y)\\log(1-p(x)))\\] avec \\[p(x)=\\frac{1}{1+\\exp(-2f(x))}\\quad\\text{et}\\quad 1-p(x)=\\frac{1}{1+\\exp(2f(x))}.\\] On déduit \\[-(y\\log p(x)+(1-y)\\log(1-p(x)))=\\log(1+\\exp(-2\\tilde yf(x)))\\] avec \\(\\tilde y=2y-1\\in\\{-1,1\\}\\). Le risque peut donc s’écrire \\[\\mathcal R(f)=\\log(1+\\exp(-2\\tilde Yf(X))).\\] De plus \\[\\begin{align*} \\mathbf E[\\log(1+\\exp(-2\\tilde Yf(X)))|X=x]= &amp;p(x)\\log(1+\\exp(-2f(x))) \\\\ &amp;+(1-p(x))\\log(1+\\exp(2f(x))). \\end{align*}\\] Donc \\[\\frac{\\partial \\mathbf E[\\log(1+\\exp(-2\\tilde Yf(X)))|X=x]}{\\partial f(x)}=p(x)\\frac{-2\\exp(-2f(x))}{1+\\exp(-2f(x))}+(1-p(x)) \\frac{2\\exp(2f(x))}{1+\\exp(2f(x))}.\\] En annulant la dérivée on obtient \\[-p(x)\\frac{1}{1+\\exp(2f(x))}+(1-p(x))\\frac{1}{1+\\exp(-2f(x))}=0,\\] d’où \\[\\frac{p(x)}{1-p(x)}=\\frac{1+\\exp(2f(x))}{1+\\exp(-2f(x))}=\\frac{\\exp(2f(x))(1+\\exp(2f(x)))}{\\exp(2f(x))(1+\\exp(-2f(x)))}=\\exp(2f(x)).\\] Par conséquent \\[f^\\star(x)=\\frac{1}{2}\\log\\frac{\\mathbf P(Y=1|X=x)}{\\mathbf P(Y=-1|X=x)}.\\] Références "],["deep.html", "Chapitre 6 Réseaux de neurones avec Keras", " Chapitre 6 Réseaux de neurones avec Keras Nous présentons ici une introduction au réseau de neurones à l’aide du package keras. On pourra trouver une documentation complète ainsi qu’un très bon tutoriel aux adresses suivantes https://keras.rstudio.com et https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/. On commence par charger la librairie library(keras) #install_keras() 1 seule fois sur la machine On va utiliser des réseaux de neurones pour le jeu de données spam où le problème est d’expliquer la variable binaires typepar les 57 autres variables du jeu de données : library(kernlab) data(spam) spamX &lt;- as.matrix(spam[,-58]) #spamY &lt;- to_categorical(as.numeric(spam$type)-1, 2) spamY &lt;- as.numeric(spam$type)-1 On sépare les données en un échantillon d’apprentissage et un échantillon test set.seed(5678) perm &lt;- sample(4601,3000) appX &lt;- spamX[perm,] appY &lt;- spamY[perm] validX &lt;- spamX[-perm,] validY &lt;- spamY[-perm] A l’aide des données d’apprentissage, entrainer un perceptron simple avec une fonction d’activation sigmoïde. On utilisera 30 epochs et des batchs de taille 5. On définit tout d’abord la structure du réseau, 1 seule couche ici de 1 neurone : percep.sig &lt;- keras_model_sequential() percep.sig %&gt;% layer_dense(units=1,input_shape = 57,activation=&quot;sigmoid&quot;) summary(percep.sig) Model: &quot;sequential&quot; ____________________________________________________________ Layer (type) Output Shape Param # ============================================================ dense (Dense) (None, 1) 58 ============================================================ Total params: 58 Trainable params: 58 Non-trainable params: 0 ____________________________________________________________ On donne ensuite la fonction de perte, l’algorithme d’optimisation ainsi que le critère pour mesurer la performance du réseau : percep.sig %&gt;% compile( loss=&quot;binary_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=&quot;accuracy&quot; ) On donne enfin dans fit les paramètres qui permettent d’entrainer le modèle (taille des batchs, nombre d’epochs…) p.sig &lt;- percep.sig %&gt;% fit( x=appX, y=appY, epochs=30, batch_size=5, validation_split=0.2, verbose=0 ) La fonction plot permet de visualiser la perte et la performance en fonction du nombre d’epochs : plot(p.sig) Faire de même avec la fonction d’activation softmax. On utilisera pour cela 2 neurones avec une sortie \\(Y\\) possédant la forme suivante. spamY1 &lt;- to_categorical(as.numeric(spam$type)-1, 2) appY1 &lt;- spamY1[perm,] validY1 &lt;- spamY1[-perm,] percep.soft &lt;- keras_model_sequential() percep.soft %&gt;% layer_dense(units=2,input_shape = 57,activation=&quot;softmax&quot;) summary(percep.soft) Model: &quot;sequential_1&quot; ____________________________________________________________ Layer (type) Output Shape Param # ============================================================ dense_1 (Dense) (None, 2) 116 ============================================================ Total params: 116 Trainable params: 116 Non-trainable params: 0 ____________________________________________________________ percep.soft %&gt;% compile( loss=&quot;binary_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=&quot;accuracy&quot; ) p.soft &lt;- percep.soft %&gt;% fit( x=appX, y=appY1, epochs=30, batch_size=1, validation_split=0.2, verbose=0 ) plot(p.soft) Comparer les performances des deux perceptrons sur les données de validation à l’aide de la fonction evaluate. percep.sig %&gt;% evaluate(validX,validY) loss accuracy 0.2418458 0.9169269 percep.soft %&gt;% evaluate(validX,validY1) loss accuracy 0.4350162 0.9050593 Construire un ou deux réseaux avec deux couches cachées. On pourra faire varier les nombre de neurones dans ces couches. Comparer les performances des réseaux construits. On propose tout d’abord 2 couches cachées composées de 100 neurones : mod2c &lt;- keras_model_sequential() mod2c %&gt;% layer_dense(units=100,activation=&quot;softmax&quot;) %&gt;% layer_dense(units=100,activation=&quot;softmax&quot;) %&gt;% layer_dense(units = 1,activation = &quot;sigmoid&quot;) mod2c %&gt;% compile( loss=&quot;binary_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=&quot;accuracy&quot; ) mod2c %&gt;% fit( x=appX, y=appY, epochs=100, batch_size=3, validation_split=0.2, verbose=0 ) mod2c %&gt;% evaluate(validX,validY) loss accuracy 0.2225624 0.9312930 On propose ici 50 neurones pour la première couche cachée et 30 neurones pour la seconde. On ajoute de plus un dropout dans la première couche cachée (permet généralement d’éviter le sur-apprentissage, mais pas forcément utile ici). mod2cd &lt;- keras_model_sequential() mod2cd %&gt;% layer_dropout(0.7) %&gt;% layer_dense(units=50,activation=&quot;softmax&quot;) %&gt;% layer_dense(units=30,activation=&quot;softmax&quot;) %&gt;% layer_dense(units = 1,activation = &quot;sigmoid&quot;) mod2cd %&gt;% compile( loss=&quot;binary_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=&quot;accuracy&quot; ) mod2cd %&gt;% fit( x=appX, y=appY, epochs=150, batch_size=5, validation_split=0.2, verbose=0 ) On évalue la performance sur les données test : mod2cd %&gt;% evaluate(validX,validY) loss accuracy 0.3068645 0.9063085 "],["dondes.html", "Chapitre 7 Données déséquilibrées 7.1 Critères de performance pour données déséquilibrées 7.2 Ré-équilibrage 7.3 Exercices supplémentaires", " Chapitre 7 Données déséquilibrées On parle de données déséquilibrées lorsque les deux modalités de la variable cible \\(Y\\) ne sont pas représentées de façon égale dans l’échantillon, ou plus précisément lorsqu’une des deux modalités est fortement majoritaire. Ce contexte est fréquemment rencontré en pratique, on peut citer les cas de détection de fraudes (peu de fraudeurs), de la présence d’une maladie rare (peu de patients atteints), du risque de crédit (peu de mauvais payeurs)… Les algorithmes standards peuvent être mis en difficultés et de nouvelles stratégies doivent être élaborées. Les stratégies classiques permettant de répondre à ce problème consistent à utiliser des critères de performance adaptés au déséquilibre ; ré-échantillonner les données pour se rapprocher d’une situation d’équilibre. Nous présentons ces stratégies à travers quelques exercices. 7.1 Critères de performance pour données déséquilibrées La notion de risque en machine learning est capitale puisque c’est à partir de l’estimation de ces risques que l’on calibre des algorithmes et que l’on choisit un algorithme de prévision. En présence de données déséquilibré, il convient de choisir un risque adapté. En effet, il est le plus souvent important de parvenir à bien identifier des individus de la classe minoritaire. Des critères tels que l’accuracy ou l’erreur de classification ne sont pas pertinents pour ce cadre. On va privilégier des critères comme le balanced accuracy \\[\\text{Bal Acc}=\\frac{1}{2}\\mathbf P(g(X)=1|Y=1)+\\frac{1}{2}\\mathbf P(g(X)=-1|Y=-1)=\\frac{\\text{TPR+TNR}}{2}.\\] le \\(F_1\\)-score \\[F_1=2\\,\\frac{\\text{Precision }\\times\\text{Recall}}{\\text{Precision }+\\text{Recall}},\\] avec \\[\\text{Precision}=\\mathbf P(Y=1|g(X)=1)\\quad\\text{et}\\quad\\text{Recall}=\\mathbf P(g(X)=1|Y=1).\\] le kappa de Cohen \\[\\kappa=\\frac{\\mathbf P(a)-\\mathbf P(e)}{1-\\mathbf P(e)}\\] où \\(\\mathbf P(a)\\) représente l’accuracy et \\(\\mathbf P(e)\\) l’accuracy sous une hypothèse d’indépendance. la courbe ROC et l’AUC… Comme d’habitude, ces critères sont inconnus et doivent être estimés par des méthodes de ré-échantillonnage de type validation croisée. Exercice 7.1 (Calculer des critères) Générer un vecteur d’observations Y de taille 500 selon une loi de Bernoulli de paramètre 0.05. set.seed(1235) n &lt;- 500 Y &lt;- rbinom(n,1,0.05) %&gt;% as.factor() Générer un vecteur de prévisions P1 de taille 500 selon une loi de Bernoulli de paramètre 0.01. set.seed(987654321) P1 &lt;- rbinom(n,1,0.01) %&gt;% factor(levels=c(&quot;0&quot;,&quot;1&quot;)) Générer un vecteur de prévision P2 de taille 500 tel que \\[\\mathcal L(P2|Y=0)=\\mathcal B(0.10)\\quad\\text{et}\\quad \\mathcal L(P2|Y=1)=\\mathcal B(0.85).\\] set.seed(123) P2 &lt;- rep(0,n) P2[Y==1] &lt;- rbinom(sum(Y==1),1,0.85) P2[Y==0] &lt;- rbinom(sum(Y==0),1,0.1) P2 &lt;- factor(P2,levels=c(&quot;0&quot;,&quot;1&quot;)) Dresser les tables de contingence de P1 et P2 à l’aide de table. Commenter. table(P1,Y) Y P1 0 1 0 471 23 1 5 1 table(P2,Y) Y P2 0 1 0 432 8 1 44 16 On remarque que P1 a tendance à prédire très souvent 0 (la classe majoritaire) alors que P2 est capable d’identifier plus d’invididus de la petite classe. Du point de vue de l’accuracy on va privilégier P1, néanmoins dans de nombreux cas P2 est plus pertinent. Pour P2, calculer, avec les fonctions usuelles de R, l’accuracy, le recall et la précision. T2 &lt;- table(P2,Y) acc &lt;- sum(T2[c(1,4)])/sum(T2) rec &lt;- T2[2,2]/sum(T2[,2]) prec &lt;- T2[2,2]/sum(T2[2,]) c(acc,rec,prec) [1] 0.8960000 0.6666667 0.2666667 En déduire le F1-score. F1 &lt;- 2*(rec*prec)/(rec+prec) F1 [1] 0.3809524 Même question pour le \\(\\kappa\\) de Cohen. rand &lt;- sum(T2[1,])/n*sum(T2[,1])/n+sum(T2[2,])/n*sum(T2[,2])/n kappa &lt;- (acc-rand)/(1-rand) kappa [1] 0.3353783 Retrouver ces indicateurs à l’aide de la fonction confusionMatrix de caret puis comparer les prévisions P1 et P2. confusionMatrix(data=P1,reference=Y,mode=&quot;everything&quot;,positive=&quot;1&quot;) Confusion Matrix and Statistics Reference Prediction 0 1 0 471 23 1 5 1 Accuracy : 0.944 95% CI : (0.9201, 0.9625) No Information Rate : 0.952 P-Value [Acc &gt; NIR] : 0.827961 Kappa : 0.0484 Mcnemar&#39;s Test P-Value : 0.001315 Sensitivity : 0.04167 Specificity : 0.98950 Pos Pred Value : 0.16667 Neg Pred Value : 0.95344 Precision : 0.16667 Recall : 0.04167 F1 : 0.06667 Prevalence : 0.04800 Detection Rate : 0.00200 Detection Prevalence : 0.01200 Balanced Accuracy : 0.51558 &#39;Positive&#39; Class : 1 confusionMatrix(data=P2,reference=Y,mode=&quot;everything&quot;,positive=&quot;1&quot;) Confusion Matrix and Statistics Reference Prediction 0 1 0 432 8 1 44 16 Accuracy : 0.896 95% CI : (0.8659, 0.9213) No Information Rate : 0.952 P-Value [Acc &gt; NIR] : 1 Kappa : 0.3354 Mcnemar&#39;s Test P-Value : 1.212e-06 Sensitivity : 0.6667 Specificity : 0.9076 Pos Pred Value : 0.2667 Neg Pred Value : 0.9818 Precision : 0.2667 Recall : 0.6667 F1 : 0.3810 Prevalence : 0.0480 Detection Rate : 0.0320 Detection Prevalence : 0.1200 Balanced Accuracy : 0.7871 &#39;Positive&#39; Class : 1 L’accuracy privilégie clairement P1 alors que d’autres critères comme le balanced accuracy, le F1-score ou le kappa de Cohen vont sélectionner P2. Ces derniers critères sont mieux adaptés pour prendre en considération la capacité à bien identifier la classe minoritaire. 7.2 Ré-équilibrage En complément du choix d’un critère pertinent, il peut être intéressant de tenter de ré-équilibrer l’échantillon pour aider les algorithmes à mieux détecter les individus de la classe minoritaire. Les méthodes classiques consistent à créer de nouvelles observations de la classe minoritaire (oversampling) et/ou supprimer des individus de la classe minoritaire (undersampling). Exercice 7.2 (Quelques algorithmes de ré-équilibrage) On considère le jeu de données df ci-dessous où on cherche à prédire Y par X1 et X2. n &lt;- 2000 set.seed(1234) X1 &lt;- runif(n) set.seed(5678) X2 &lt;- runif(n) set.seed(9012) R1 &lt;- X1&lt;=0.25 R2 &lt;- (X1&gt;0.25 &amp; X2&gt;=0.75) R3 &lt;- (X1&gt;0.25 &amp; X2&lt;0.75) Y &lt;- rep(0,n) Y[R1] &lt;- rbinom(sum(R1),1,0.75) Y[R2] &lt;- rbinom(sum(R2),1,0.75) Y[R3] &lt;- rbinom(sum(R3),1,0.25) df1 &lt;- data.frame(X1,X2,Y) df1$Y &lt;- factor(df1$Y) indDY1 &lt;- which(df1$Y==1) df1.1 &lt;- df1[-indDY1[1:650],] df1.2 &lt;- df1.1[sample(nrow(df1.1),1000),] df &lt;- df1.2[sample(nrow(df1.2),100),] rownames(df) &lt;- NULL p1 &lt;- ggplot(df)+aes(x=X1,y=X2,color=Y)+geom_point() p1 On a ici 4 fois plus d’observations dans le groupe 0. summary(df$Y) 0 1 80 20 On commence par faire du oversampling avec la fonction RandOverClassif. Effectuer le ré-échantillonnage et expliquer. library(UBL) over1 &lt;- RandOverClassif(Y~.,dat = df) summary(over1$Y) 0 1 80 80 On duplique des observations du groupe 1 pour atteindre le nombre d’observations du groupe 0. Corriger les paramètres de la fonction de manière à avoir 80 observations dans le groupe 0 et 60 dans le groupe 1. Il suffit de laisser intact le groupe 0 et de multiplier par 3 le nombre d’observations du groupe 1. library(UBL) over2 &lt;- RandOverClassif(Y~.,dat = df,C.perc=list(&quot;0&quot;=1,&quot;1&quot;=3)) summary(over2$Y) 0 1 80 60 On s’intéresse maintenant à l’algorithme SMOTE Exécuter la fonction SmoteClassif avec k=3 et les les paramètres par défaut smote1 &lt;- SmoteClassif(Y~.,dat=df,k=3) summary(smote1$Y) 0 1 50 50 30 observations ont été crées par l’algorithme SMOTE. On a également enlevé des observations du groupe 1 pour avoir autant d’observations dans les deux groupes. Visualiser les observations smote. new &lt;- anti_join(smote1,df) old &lt;- inner_join(smote1,df) ggplot(old)+aes(x=X1,y=X2)+geom_point(aes(color=Y))+ geom_point(data=new,color=&quot;black&quot;) Corriger les paramètres de la fonction de manière à avoir 80 observations dans le groupe 0 et 60 dans le groupe 1. Ici encore il “suffit” de jouer avec l’option C.perc. smote2 &lt;- SmoteClassif(Y~.,dat=df,k=3,C.perc=list(&quot;0&quot;=1,&quot;1&quot;=3)) summary(smote2$Y) 0 1 80 60 On souhaite maintenant ré-équilibrer par random undersampling. Utiliser la fonction RandUnderClassif pour effectuer un tel ré-équilibrage. Ici encore on pourra faire varier les paramètres. under1 &lt;- RandUnderClassif(Y~.,dat=df) summary(under1) X1 X2 Y Min. :0.01288 Min. :0.006945 0:20 1st Qu.:0.23283 1st Qu.:0.277733 1:20 Median :0.38112 Median :0.525170 Mean :0.42021 Mean :0.512639 3rd Qu.:0.54655 3rd Qu.:0.740836 Max. :0.96958 Max. :0.999183 On supprimer des obseravtions du groupe 0 pour avoir le même nombre d’observations dans les deux groupes. Ici encore on peut contrôler le niveau de ré-équilibrage avec C.perc : under2 &lt;- RandUnderClassif(Y~.,dat=df,C.perc = list(&quot;0&quot;=0.5,&quot;1&quot;=1)) summary(under2$Y) 0 1 40 20 On passe maintenant à l’algorithme Tomek. Sans utiliser la fonction TomekClassif identifier les paires d’observations qui ont un lien de Tomek. On pourra utiliser la fonction nng du package cccd. On rappelle que deux observations ont un lien de Tomek si elles sont plus proches voisins mutuels et de deux groupes différents. On commence donc par identifier les plus proches voisins mutuels : library(cccd) unppv &lt;- nng(x=df[,1:2],k=1,mutual=TRUE) graph_mut &lt;- as_data_frame(unppv) graph_mut from to 1 1 13 2 2 50 3 3 54 4 4 98 5 5 42 6 6 35 7 9 96 8 12 30 9 16 41 10 18 100 11 19 44 12 20 92 13 24 28 14 26 87 15 27 91 16 29 64 17 31 82 18 32 94 19 33 69 20 36 68 21 37 99 22 40 62 23 45 55 24 48 51 25 52 66 26 56 58 27 65 75 28 67 77 29 72 86 30 73 93 31 74 89 Puis on récupère les groupes de ces observations afin de ne conserver que les paires qui proviennent de groupes différents : Yfrom &lt;- df$Y[graph_mut$from] Yto &lt;- df$Y[graph_mut$to] tomek_link &lt;- graph_mut %&gt;% filter(Yfrom!=Yto) tomek_link from to 1 4 98 2 16 41 3 18 100 4 19 44 5 26 87 6 29 64 7 45 55 8 72 86 9 74 89 Retrouver ces paires à l’aide de la fonction Tomek LinK. tomek1 &lt;- TomekClassif(Y~.,dat=df) tomek1[[2]] [1] 4 98 16 41 18 100 19 44 26 87 29 64 45 55 [15] 72 86 74 89 Visualiser les observations supprimées. On prendra soin d’expliquer l’option rem de TomekClassif. On dispose de deux stratégies. On peut enlever les paires entières qui sont T-link avec l’option par défaut ggplot(tomek1[[1]])+aes(x=X1,y=X2,color=Y)+geom_point()+ geom_point(data=df[tomek1[[2]],],shape=17,size=2) On peut également ne supprimer que l’élelément du T-link du groupe majoritaire avec l’option rem=“maj” : tomek2 &lt;- TomekClassif(Y~.,dat=df,rem=&quot;maj&quot;) tomek2[[2]] [1] 4 41 100 19 87 29 55 86 89 ggplot(tomek2[[1]])+aes(x=X1,y=X2,color=Y)+geom_point()+ geom_point(data=df[tomek2[[2]],],shape=17,size=2) Exercice 7.2 (Comparaison de méthodes de ré-équilibrage) On considère 3 jeux de données df1, df2 et df3. n &lt;- 2000 set.seed(12345) X1 &lt;- runif(n) set.seed(5678) X2 &lt;- runif(n) set.seed(9012) R1 &lt;- X1&lt;=0.25 R2 &lt;- (X1&gt;0.25 &amp; X2&gt;=0.75) R3 &lt;- (X1&gt;0.25 &amp; X2&lt;0.75) Y &lt;- rep(0,n) Y[R1] &lt;- rbinom(sum(R1),1,0.75) Y[R2] &lt;- rbinom(sum(R2),1,0.75) Y[R3] &lt;- rbinom(sum(R3),1,0.25) df1 &lt;- data.frame(X1,X2,Y) df1$Y &lt;- factor(df1$Y) indDY1 &lt;- which(df1$Y==1) df2 &lt;- df1[-indDY1[1:400],] df3 &lt;- df1[-indDY1[1:700],] df1 &lt;- df1[sample(nrow(df1),1000),] df2 &lt;- df2[sample(nrow(df2),1000),] df3 &lt;- df3[sample(nrow(df3),1000),] Comparer la distribution de Y pour ces trois jeux de données et visualiser les observations. summary(df1$Y) 0 1 559 441 summary(df2$Y) 0 1 692 308 summary(df3$Y) 0 1 842 158 ggplot(df1)+aes(x=X1,y=X2,color=Y)+geom_point() ggplot(df2)+aes(x=X1,y=X2,color=Y)+geom_point() ggplot(df3)+aes(x=X1,y=X2,color=Y)+geom_point() Les trois échantillons sont de même taille. Le groupe 0 est toujours plus important que le groupe 1 mais le déséquilibre est plus prononcé pour l’échantillon 2 et surtout l’échantillon 3. On sépare ces 3 échantillons en un échantillon d’apprentissage et un échantillon test. set.seed(123) a1 &lt;- createDataPartition(1:nrow(df1),p=2/3) a2 &lt;- createDataPartition(1:nrow(df2),p=2/3) a3 &lt;- createDataPartition(1:nrow(df3),p=2/3) train1 &lt;- df1[a1$Resample1,] train2 &lt;- df2[a2$Resample1,] train3 &lt;- df3[a3$Resample1,] test1 &lt;- df1[-a1$Resample1,] test2 &lt;- df2[-a2$Resample1,] test3 &lt;- df3[-a3$Resample1,] Ajuster une forêt aléatoire sur les 3 échantillon d’apprentissage, calculer les labels prédits sur les échantillons tests et estimer les différents indicateurs vus en cours à l’aide de confusionMatrix. library(randomForest) rf1 &lt;- randomForest(Y~.,data=train1) rf2 &lt;- randomForest(Y~.,data=train2) rf3 &lt;- randomForest(Y~.,data=train3) p1 &lt;- predict(rf1,newdata=test1) p2 &lt;- predict(rf2,newdata=test2) p3 &lt;- predict(rf3,newdata=test3) confusionMatrix(data=p1,reference=test1$Y) Confusion Matrix and Statistics Reference Prediction 0 1 0 141 67 1 43 81 Accuracy : 0.6687 95% CI : (0.6152, 0.7191) No Information Rate : 0.5542 P-Value [Acc &gt; NIR] : 1.385e-05 Kappa : 0.3187 Mcnemar&#39;s Test P-Value : 0.02831 Sensitivity : 0.7663 Specificity : 0.5473 Pos Pred Value : 0.6779 Neg Pred Value : 0.6532 Prevalence : 0.5542 Detection Rate : 0.4247 Detection Prevalence : 0.6265 Balanced Accuracy : 0.6568 &#39;Positive&#39; Class : 0 confusionMatrix(data=p2,reference=test2$Y) Confusion Matrix and Statistics Reference Prediction 0 1 0 196 48 1 37 51 Accuracy : 0.744 95% CI : (0.6935, 0.7901) No Information Rate : 0.7018 P-Value [Acc &gt; NIR] : 0.05113 Kappa : 0.3681 Mcnemar&#39;s Test P-Value : 0.27808 Sensitivity : 0.8412 Specificity : 0.5152 Pos Pred Value : 0.8033 Neg Pred Value : 0.5795 Prevalence : 0.7018 Detection Rate : 0.5904 Detection Prevalence : 0.7349 Balanced Accuracy : 0.6782 &#39;Positive&#39; Class : 0 confusionMatrix(data=p3,reference=test3$Y) Confusion Matrix and Statistics Reference Prediction 0 1 0 253 47 1 21 11 Accuracy : 0.7952 95% CI : (0.7477, 0.8373) No Information Rate : 0.8253 P-Value [Acc &gt; NIR] : 0.933101 Kappa : 0.1373 Mcnemar&#39;s Test P-Value : 0.002432 Sensitivity : 0.9234 Specificity : 0.1897 Pos Pred Value : 0.8433 Neg Pred Value : 0.3438 Prevalence : 0.8253 Detection Rate : 0.7620 Detection Prevalence : 0.9036 Balanced Accuracy : 0.5565 &#39;Positive&#39; Class : 0 On remarque que l’accuracy est meilleur pour le 3ème échantillon, contrairement à des indicateurs tels que le \\(\\kappa\\) de Cohen ou le balanced accuracy. On considère uniquement l’échantillon df3. Refaire l’analyse précédente en utilisant des techniques de ré-échantillonnage. Le ré-équilibrage doit porter uniquement sur l’échantillon d’apprentissage. On propose d’utiliser le radom oversampling, smote, le random undersampling et tomek. train3.under &lt;- RandUnderClassif(Y~.,dat=train3) train3.over &lt;- RandOverClassif(Y~.,dat=train3) train3.smote &lt;- SmoteClassif(Y~.,dat=train3) train3.tomek &lt;- TomekClassif(Y~.,dat=train3)[[1]] On entraîne les forêts aléatoires sur ces nouveaux échantillons et on prédit les individus de l’échantillon test rf3.under &lt;- randomForest(Y~.,data=train3.under) rf3.over &lt;- randomForest(Y~.,data=train3.over) rf3.smote &lt;- randomForest(Y~.,data=train3.smote) rf3.tomek &lt;- randomForest(Y~.,data=train3.tomek) p3.under &lt;- predict(rf3.under,newdata=test3) p3.over &lt;- predict(rf3.over,newdata=test3) p3.smote &lt;- predict(rf3.smote,newdata=test3) p3.tomek &lt;- predict(rf3.tomek,newdata=test3) mat_prev &lt;- data.frame(aucun=p3,under=p3.under,over=p3.over,smote=p3.smote,tomek=p3.tomek,obs=test3$Y) On en déduit les critères monba &lt;- function(prev,obs){confusionMatrix(prev,obs,positive=&quot;1&quot;)$byClass[11]} monF1 &lt;- function(prev,obs){confusionMatrix(prev,obs,positive=&quot;1&quot;)$byClass[7]} monkappa &lt;- function(prev,obs){confusionMatrix(prev,obs,positive=&quot;1&quot;)$overall[2]} mat_prev %&gt;% pivot_longer(-obs,names_to=&quot;method&quot;,values_to=&quot;prev&quot;) %&gt;% group_by(method,.add=TRUE) %&gt;% summarize(Acc=mean(obs==prev),BA=monba(prev,obs),F1=monF1(prev,obs),kapp=monkappa(prev,obs),.groups=&quot;keep&quot;) %&gt;% summarize_all(~round(.,3)) # A tibble: 5 x 5 method Acc BA F1 kapp &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 aucun 0.795 0.557 0.244 0.137 2 over 0.792 0.629 0.389 0.264 3 smote 0.699 0.634 0.383 0.204 4 tomek 0.819 0.564 0.25 0.17 5 under 0.675 0.687 0.432 0.249 L’accuracy est sans surprise meilleur lorsqu’on ne ré-équilibre pas. Cependant les méthodes de ré-équilibrage permettent ici d’améliorer (plus ou moins) les autres critères. 7.3 Exercices supplémentaires Exercice 7.3 (Echantillonnage rétrospectif) Dans le cadre de l’échantillonnage rétrospectif pour le modèle logistique vu en cours, démontrer la propriété qui lie le modèle logistique initial au modèle ré-équilibré. On a \\[\\text{logit}\\, p_\\beta(x_i)=\\log\\frac{\\mathbf P(y_i=1)}{\\mathbf P(y_i=0)}\\quad\\text{et}\\quad \\text{logit}\\, p_\\gamma(x_i)=\\log\\frac{\\mathbf P(y_i=1|s_i=1)}{\\mathbf P(y_i=0|s_i=1)}.\\] Or \\[\\mathbf P(y_i=1|s_i=1)=\\frac{\\mathbf P(y_i=1,s_i=1)}{\\mathbf P(s_i=1)}=\\frac{\\mathbf P(s_i=1|y_i=1)\\mathbf P(y_i=1)}{\\mathbf P(s_i=1)}\\] et \\[\\mathbf P(y_i=0|s_i=1)=\\frac{\\mathbf P(y_i=0,s_i=1)}{\\mathbf P(s_i=1)}=\\frac{\\mathbf P(s_i=1|y_i=0)\\mathbf P(y_i=0)}{\\mathbf P(s_i=1)}.\\] Donc \\[\\text{logit}\\, p_\\gamma(x_i)=\\log\\frac{\\mathbf P(y_i=1)}{\\mathbf P(y_i=0)}+\\log\\frac{\\mathbf P(s_i=1|y_i=1)}{\\mathbf P(s_i=1|y_i=0)}=\\text{logit}\\,p_\\beta(x_i)+\\log\\left(\\frac{\\tau_{1}}{\\tau_{0}}\\right).\\] Exercice 7.4 (Echantillonnage rétrospectif) Une étude cas/témoins est réalisée pour mesurer l’effet du tabac sur une pathologie. Pour ce faire, on choisit \\(n_1=250\\) patients atteints de la pathologie (cas) et \\(n_0=250\\) patients sains (témoins). Les résultats de l’étude sont présentés ci-dessous Fumeur Non fumeur Non malade 48 202 Malade 208 42 A partir des données obtenues, estimer à l’aide d’un modèle logistique la probabilité d’être atteint pour un fumeur, puis pour un non fumeur. Pour simplifier on va construire un jeu de données qui correspond au résultat : X&lt;-rep(1,500) X[1:256]&lt;-&quot;fumeur&quot; X[257:500]&lt;-&quot;non_fumeur&quot; Y&lt;-rep(1,500) Y[1:48]&lt;-0 Y[257:458]&lt;-0 Y&lt;-factor(Y) X&lt;-factor(X) df &lt;- data.frame(Y,X) model&lt;-glm(Y~.,data=df,family=binomial) On peut maintenant estimer le modèle logistique et obtenir les prévisions demandées : newX &lt;- data.frame(X=c(&quot;fumeur&quot;,&quot;non_fumeur&quot;)) predict(model,newdata=newX,type=&quot;response&quot;) %&gt;% round(3) 1 2 0.812 0.172 Comment interpréter ces deux probabilités ? Est-ce qu’elles estiment la probabilité d’être atteint pour un individu quelconque dans la population ? Non ! On a échantillonné de manière à avoir autant de patients malades que non malades, ce qui n’est pas vrai dans la population totale. On est face à un biais d’échantillonnage que l’on peut corriger à l’aide de l’exercice précédent. Des études précédentes ont montré que cinq individus sur mille sont atteints par la pathologie dans la population entière. En utilisant la propriété de l’exercice précédent, en déduire les probabilités d’être atteint pour un fumeur et un non fumeur dans la population. On rappelle que \\[\\tau_1=\\mathbf P(S=1|Y=1)=\\frac{\\mathbf P(Y=1|S=1)\\mathbf P(S=1)}{\\mathbf P(Y=1)}.\\] Comme \\(\\mathbf P(Y=1|S=1)=\\mathbf P(Y=0|S=1)=1/2\\), on déduit \\[\\frac{\\tau_1}{\\tau_0}=\\frac{\\mathbf P(Y=0)}{\\mathbf P(Y=1)}=\\frac{\\pi_0}{\\pi_1}=\\frac{0.005}{0.995}.\\] On obtient ainsi les probabilités demandées avec beta&lt;-model$coefficients pi1&lt;-0.005 pi0&lt;-0.995 beta1_cor &lt;-beta[1]-log(pi0/pi1) p1F &lt;- exp(beta1_cor)/(1+exp(beta1_cor)) p1NF &lt;- exp(beta1_cor+beta[2])/(1+exp(beta1_cor+beta[2])) p1F (Intercept) 0.02131148 p1NF (Intercept) 0.001043738 On peut retrouver les paramètres du modèle corrigé en utilisant l’option offset mod_cor &lt;- glm(Y~.,data=df,family=&quot;binomial&quot;,offset = rep(log(pi0/pi1),nrow(df))) coef(mod_cor) (Intercept) Xnon_fumeur -3.826968 -3.036935 c(beta1_cor,beta[2]) (Intercept) Xnon_fumeur -3.826968 -3.036935 "],["comp-algo.html", "Chapitre 8 Comparaison d’algorithmes", " Chapitre 8 Comparaison d’algorithmes Les chapitres précédents ont présenté plusieurs algorithmes permettant de répondre à un problème posé, le plus souvent de classification supervisée. Se pose bien entendu la question de choisir un unique algorithme. Etant donné un échantillon \\(\\mathcal D_n=\\{(x_1,y_1),\\dots,(x_n,y_y)\\}\\) on rappelle qu’un algorithme de prévision est une fonction \\[g:\\mathcal X\\times(\\mathcal X\\times \\mathcal Y)^n\\to\\mathcal Y\\] qui, à une nouvelle observation \\(x\\in\\mathcal X\\) renverra la prévision \\(g(x,\\mathcal D_n)\\) calculée à partir de l’échantillon \\(\\mathcal D_n\\). Cette fonction \\(g\\) peut contenir tout un tas d’étapes comme : la gestion des données manquantes une procédure de choix de variables une méthode pour ré-équilibrer les données des procédures pour calibrer des paramètres (qui peuvent éventuellement inclure des validations croisées) … Le machine learning se focalisant sur la capacité d’un algorithme à bien prédire, les stratégies classiques pour choisir un algorithme vont (une fois de plus) consister à évaluer le pouvoir prédictif de chaque algorithme. Il n’y a rien de bien nouveau puisque cela va reposer sur les techniques présentées aux chapitres 1 : choisir un ou plusieurs critères (erreur de classification, AUC, \\(F_1\\)-score…) choisir une procédure de ré-échantillonnage pour estimer ce critère (validation hold-out, validation croisée, OOB…). Nous proposons de développer une stratégie pour choisir un algorithme sur le jeu de données Internet Advertisements Data Set disponible sur cette page https://archive.ics.uci.edu/ml/datasets/internet+advertisements. Le problème est d’identifier la présence d’une image publicitaire sur des pages webs. Il comporte ad.data &lt;- read.table(&quot;data/ad_data.txt&quot;,header=FALSE,sep=&quot;,&quot;,dec=&quot;.&quot;,na.strings = &quot;?&quot;,strip.white = TRUE) dim(ad.data) [1] 3279 1559 Ce jeu de données contient 1558 variables explicatives, ces variables contiennent différentes caractériques de la page web (voir le site où sont présentées les données pour plus d’information). La dernière variable est la variable à expliquer, elle vaut ad. si présence d’une publicité, nonad. sinon. names(ad.data)[ncol(ad.data)] &lt;- &quot;Y&quot; ad.data$Y &lt;- as.factor(ad.data$Y) summary(ad.data$Y) ad. nonad. 459 2820 Ce jeu de données contient des données manquantes. sum(is.na(ad.data)) [1] 2729 On remarque que : 920 lignes 4 colonnes ont au moins une valeur manquante. apply(is.na(ad.data),1,any) %&gt;% sum() [1] 920 var.na &lt;- apply(is.na(ad.data),2,any) names(ad.data)[var.na] [1] &quot;V1&quot; &quot;V2&quot; &quot;V3&quot; &quot;V4&quot; On choisit de retirer ces 4 variables de l’analyse (il faudrait peut-être réfléchir un peu plus…). ad.data1 &lt;- ad.data[,var.na==FALSE] dim(ad.data1) [1] 3279 1555 sum(is.na(ad.data1)) [1] 0 On se retrouve donc en présence de 3279 individus et 1554 variables explicatives. On construit la matrice des X et le vecteur des Y qui sont nécessaires pour certaines fonctions comme glmnet : X.ad &lt;- model.matrix(Y~.,data=ad.data1)[,-1] Y.ad &lt;- ad.data1$Y et on transforme la variable cible en 0-1 pour utiliser gbm: ad.data2 &lt;- ad.data1 %&gt;% mutate(Y=recode(Y,&quot;ad.&quot;=0,&quot;nonad.&quot;=1)) On souhaite comparer les algorithmes présentés précédemment. Ils nécessitent les packages suivants library(e1071) library(caret) library(rpart) library(glmnet) library(ranger) library(gbm) On commence tout d’abord par représenter un algorithme par une fonction R qui admettra en entrée un jeu de données et renverra une unique prévision pour de nouveaux individus. On illustre ces fonctions pour prédire ce nouvel individu. newX &lt;- ad.data1[1000,] newX.X &lt;- matrix(X.ad[1000,],nrow=1) On stockera les prévisions dans l’objet suivant prev &lt;- tibble(algo=c(&quot;SVM&quot;,&quot;arbre&quot;,&quot;ridge&quot;,&quot;lasso&quot;,&quot;foret&quot;,&quot;ada&quot;,&quot;logit&quot;),prev=0) SVM à noyau gaussien où le choix des paramètres du noyau se fait par validation croisée 4 blocs : prev.svm &lt;- function(df,newX){ C &lt;- c(0.01,1,10) sigma &lt;- c(0.1,1,3) gr &lt;- expand.grid(C=C,sigma=sigma) ctrl &lt;- trainControl(method=&quot;cv&quot;,number=4) cl &lt;- makePSOCKcluster(3) registerDoParallel(cl) res.svm &lt;- train(Y~.,data=df,method=&quot;svmRadial&quot;,trControl=ctrl, tuneGrid=gr,prob.model=TRUE) stopCluster(cl) predict(res.svm,newX,type=&quot;prob&quot;)[2] } prev[1,2] &lt;- prev.svm(ad.data1,newX) Arbre de classification où l’élagage est fait selon la procédure CART présentée dans le chapitre 3. prev.arbre &lt;- function(df,newX){ arbre &lt;- rpart(Y~.,data=df,cp=1e-8,minsplit=2) cp_opt &lt;- arbre$cptable %&gt;% as.data.frame() %&gt;% filter(xerror==min(xerror)) %&gt;% dplyr::select(CP) %&gt;% slice(1) %&gt;% as.numeric() arbre.opt &lt;- prune(arbre,cp=cp_opt) predict(arbre,newdata=newX,type=&quot;prob&quot;)[,2] } prev[2,2] &lt;- prev.arbre(ad.data1,newX) Lasso et Ridge où le paramètre de régularisation est choisi par validation croisée 10 blocs en minimisant la déviance binomiale : prev.ridge &lt;- function(df.X,df.Y,newX){ ridge &lt;- cv.glmnet(df.X,df.Y,family=&quot;binomial&quot;,alpha=0) as.vector(predict(ridge,newx = newX,type=&quot;response&quot;)) } prev.lasso &lt;- function(df.X,df.Y,newX){ lasso &lt;- cv.glmnet(df.X,df.Y,family=&quot;binomial&quot;,alpha=1) as.vector(predict(lasso,newx = newX,type=&quot;response&quot;)) } prev[3,2] &lt;- prev.ridge(X.ad,Y.ad,newX.X) prev[4,2] &lt;- prev.lasso(X.ad,Y.ad,newX.X) Forêt aléatoire avec les paramètres par défaut : prev.foret &lt;- function(df,newX){ foret &lt;- ranger(Y~.,data=df,probability=TRUE) predict(foret,data=newX,type=&quot;response&quot;)$predictions[,2] } prev[5,2] &lt;- prev.foret(ad.data1,newX) Adaboost et logitboost avec le nombre d’itérations choisi par validation croisée 5 blocs : prev.ada &lt;- function(df,newX){ ada &lt;- gbm(Y~.,data=df,distribution=&quot;adaboost&quot;,interaction.depth=2, bag.fraction=1,cv.folds = 5,n.trees=500) nb.it &lt;- gbm.perf(ada,plot.it=FALSE) predict(ada,newdata=newX,n.trees=nb.it,type=&quot;response&quot;) } prev.logit &lt;- function(df,newX){ logit &lt;- gbm(Y~.,data=df,distribution=&quot;bernoulli&quot;,interaction.depth=2, bag.fraction=1,cv.folds = 5,n.trees=500) nb.it &lt;- gbm.perf(logit,plot.it=FALSE) predict(logit,newdata=newX,n.trees=nb.it,type=&quot;response&quot;) } prev[6,2] &lt;- prev.ada(ad.data2,newX) prev[7,2] &lt;- prev.logit(ad.data2,newX) On peut visualiser la prévision de chaque algorithme prev # A tibble: 7 x 2 algo prev &lt;chr&gt; &lt;dbl&gt; 1 SVM 0.950 2 arbre 0.990 3 ridge 0.984 4 lasso 0.980 5 foret 0.979 6 ada 0.974 7 logit 0.983 Exercice 8.1 (Choix d’un algorithme par validation croisée) Choisir un algorithme parmi les précédents en utilisant comme critère l’erreur de classification ainsi que la courbe ROC et l’AUC. On pourra faire une validation croisée 10 blocs (même si ça peut être un peu long…). On commence par définir les blocs set.seed(1234) bloc &lt;- sample(1:10,nrow(ad.data1),replace=TRUE) table(bloc) bloc 1 2 3 4 5 6 7 8 9 10 309 327 329 310 358 354 304 337 342 309 On effectue la validation croisée pour obtenir un score pour chaque individu : set.seed(4321) score &lt;- data.frame(matrix(0,nrow=nrow(ad.data1),ncol=7)) names(score) &lt;- c(&quot;svm&quot;,&quot;arbre&quot;,&quot;ridge&quot;,&quot;lasso&quot;,&quot;foret&quot;,&quot;ada&quot;,&quot;logit&quot;) for (k in 1:10){ ind.test &lt;- bloc==k dapp &lt;- ad.data1[!ind.test,] dtest &lt;- ad.data1[ind.test,] dapp2 &lt;- ad.data2[!ind.test,] dtest2 &lt;- ad.data2[ind.test,] X.app &lt;- X.ad[!ind.test,] X.test &lt;- X.ad[ind.test,] Y.app &lt;- Y.ad[!ind.test] Y.test &lt;- Y.ad[ind.test] svm.k &lt;- prev.svm(df=dapp,newX = dtest) arbre.k &lt;- prev.arbre(df=dapp,newX = dtest) ridge.k &lt;- prev.ridge(df.X=X.app,df.Y=Y.app,newX=X.test) lasso.k &lt;- prev.lasso(df.X=X.app,df.Y=Y.app,newX=X.test) foret.k &lt;- prev.foret(df=dapp,newX = dtest) ada.k &lt;- prev.ada(df=dapp2,newX=dtest2) logit.k &lt;- prev.logit(df=dapp2,newX=dtest2) score[ind.test,] &lt;- data.frame(arbre=svm.k,arbre=arbre.k,ridge=ridge.k, lasso=lasso.k,foret=foret.k,ada=ada.k,logit=logit.k) } On ajoute à cette matrice score les valeurs observées que l’on recode en 0-1 : score1 &lt;- score %&gt;% mutate(obs=fct_recode(ad.data1$Y,&quot;0&quot;=&quot;ad.&quot;,&quot;1&quot;=&quot;nonad.&quot;)) %&gt;% pivot_longer(-obs,names_to=&quot;Methode&quot;,values_to=&quot;score&quot;) On peut maintenant déduire tous les critères en confondant les valeurs prédites aux valeurs observées. Courbe ROC library(plotROC) rocplot &lt;- ggplot(score1)+aes(m=score,d=as.numeric(obs),color=Methode)+geom_roc()+theme_classic() rocplot AUC score1 %&gt;% group_by(Methode) %&gt;% summarize(AUC=as.numeric(pROC::auc(obs,score))) %&gt;% mutate(AUC=round(AUC,3)) %&gt;% arrange(desc(AUC)) # A tibble: 7 x 2 Methode AUC &lt;chr&gt; &lt;dbl&gt; 1 ridge 0.98 2 foret 0.976 3 lasso 0.951 4 logit 0.945 5 ada 0.94 6 arbre 0.897 7 svm 0.813 Erreur de classification score1 %&gt;% mutate(prev=round(score),err=prev!=obs) %&gt;% group_by(Methode) %&gt;% summarize(Err_classif=mean(err)) %&gt;% mutate(Err_classif=round(Err_classif,3)) %&gt;% arrange(Err_classif) # A tibble: 7 x 2 Methode Err_classif &lt;chr&gt; &lt;dbl&gt; 1 foret 0.029 2 arbre 0.03 3 logit 0.03 4 ridge 0.03 5 lasso 0.031 6 ada 0.038 7 svm 0.101 On remarque que la svm possède les plus mauvais résultats. Cela ne signifie pas forcément que la méthode est mauvaise, peut-être que les choix qui ont été faits (noyaux gaussien, et grilles de paramètres) ne sont pas pertinents. Les arbres se révèlent également peu efficaces pour la courbe ROC et l’AUC, il est rare que les arbres soient parmi les meilleurs algorithmes contrairement au gradient boosting et aux forêts aléatoires. En terme d’AUC, la régression ridge et les forêts aléatoires se distinguent avec de très bonnes performances. On choisira l’algorithme final parmi ces deux là. Exercice 8.2 (Choix d’un algorithme de ré-équilibrage par validation croisée) On considère le même jeu de données que précédemment. Choisir un algorithme de ré-équilibrage par validation croisée. Il s’agira de combiner des méthodes de ré-équilibrage (random over/under sampling, smote, tomek…) avec des algorithmes de prévision de machine learning. On pourra se restreindre au modèle logistique avec calcul des estimateurs par maximum de vraisemblance, ridge, lasso… On recode les modalités de \\(Y\\) en 0-1 : ad.data1 &lt;- ad.data1 %&gt;% transform(Y=fct_recode(Y,&quot;0&quot;=&quot;nonad.&quot;,&quot;1&quot;=&quot;ad.&quot;)) %&gt;% transform(Y=fct_inseq(Y)) On définit d’abord les 10 blocs pour la validation croisée. set.seed(1234) bloc &lt;- sample(1:10,nrow(ad.data1),replace=TRUE) table(bloc) Puis on définit une liste SCORE de longueur 5 dont chaque élément contiendra un dataframe avec les probabilités que l’image soit une publicité estimées par chaque méthode pour chaque algorithme de ré-équilibrage. score &lt;- data.frame(matrix(0,nrow=nrow(ad.data1),ncol=3)) names(score) &lt;- c(&quot;logit&quot;,&quot;lasso&quot;,&quot;ridge&quot;) SCORE &lt;- list(brute=score,over=score,smote=score,under=score,tomek=score) On peut maintenant faire la validation croisée, pour chaque valeur de k entre 1 et 10 : on calcule les échantillons d’apprentissage et de test ind.test &lt;- bloc==k dapp &lt;- ad.data1[!ind.test,] dtest &lt;- ad.data1[ind.test,] X.test &lt;- model.matrix(Y~.,data=dtest)[,-1] on ré-équilibre les données d’apprentissage uniquement ech.app &lt;- list(norm=dapp, over=RandOverClassif(Y~.,dat=dapp), smote=SmoteClassif(Y~.,dat=dapp), under=RandUnderClassif(Y~.,dat=dapp), tomek=TomekClassif(Y~.,dat=dapp)[[1]]) mod.mat.list &lt;- function(df){model.matrix(Y~.,data=df)[,-1]} Y.list &lt;- function(df) df$Y X.app &lt;- lapply(ech.app,mod.mat.list) Y.app &lt;- lapply(ech.app,Y.list) on entraine les algorithmes logistique (MV), lasso et ridge sur tous les échantillons d’apprentissage. Le paramètre de régularisation des méthode lasso et ridge est sélectionné en faisant une validation croisée 10 blocs, c’est-à-dire que l’échantillon d’apprentissage est coupé en 10 blocs pour choisir ce paramètre library(glmnet) for (j in 1:5){ lasso &lt;- cv.glmnet(X.app[[j]],Y.app[[j]],family=&quot;binomial&quot;) ridge &lt;- cv.glmnet(X.app[[j]],Y.app[[j]],family=&quot;binomial&quot;,alpha=0) logit &lt;- glm(Y~.,data=ech.app[[j]],family=&quot;binomial&quot;) on calcule enfin les probabilités estimées que l’image soit une publicité pour chaque individu de l’échantillon test SCORE[[j]][ind.test,] &lt;- data.frame( logit=predict.glm(logit,newdata=dtest,type=&quot;response&quot;), lasso=as.vector(predict(lasso,newx=X.test,type=&quot;response&quot;)), ridge=as.vector(predict(ridge,newx=X.test,type=&quot;response&quot;)) ) } Le code entier se trouve ci-dessous : set.seed(4321) library(glmnet) score &lt;- data.frame(matrix(0,nrow=nrow(ad.data1),ncol=3)) names(score) &lt;- c(&quot;logit&quot;,&quot;lasso&quot;,&quot;ridge&quot;) SCORE &lt;- list(brute=score,over=score,smote=score,under=score,tomek=score) for (k in 1:10){ print(k) ind.test &lt;- bloc==k dapp &lt;- ad.data1[!ind.test,] dtest &lt;- ad.data1[ind.test,] X.test &lt;- model.matrix(Y~.,data=dtest)[,-1] ech.app &lt;- list(norm=dapp, over=RandOverClassif(Y~.,dat=dapp), smote=SmoteClassif(Y~.,dat=dapp), under=RandUnderClassif(Y~.,dat=dapp), tomek=TomekClassif(Y~.,dat=dapp)[[1]]) mod.mat.list &lt;- function(df){model.matrix(Y~.,data=df)[,-1]} Y.list &lt;- function(df) df$Y X.app &lt;- lapply(ech.app,mod.mat.list) Y.app &lt;- lapply(ech.app,Y.list) for (j in 1:5){ print(j) lasso &lt;- cv.glmnet(X.app[[j]],Y.app[[j]],family=&quot;binomial&quot;) ridge &lt;- cv.glmnet(X.app[[j]],Y.app[[j]],family=&quot;binomial&quot;,alpha=0) logit &lt;- glm(Y~.,data=ech.app[[j]],family=&quot;binomial&quot;) SCORE[[j]][ind.test,] &lt;- data.frame( logit=predict.glm(logit,newdata=dtest,type=&quot;response&quot;), lasso=as.vector(predict(lasso,newx=X.test,type=&quot;response&quot;)), ridge=as.vector(predict(ridge,newx=X.test,type=&quot;response&quot;)) ) } } On assemble tous les résultats dans un seul dataframe de 4 colonnes qui contiendra une colonne : meth pour le nom de l’algorithme de ré-équilibrage ; obs pour les valeurs observées de \\(Y\\) pour chaque individu (0 ou 1) ; algo pour l’algorithme de prévision (logistique, lasso, ridge) ; score pour la probabilité que l’image soit une publicité estimée par l’algorithme. mat.score &lt;- bind_rows(brutes=SCORE[[1]], over=SCORE[[2]], smote=SCORE[[3]], under=SCORE[[4]], tomek=SCORE[[5]],.id=&quot;meth&quot;) %&gt;% mutate(obs=rep(ad.data1$Y,5)) %&gt;% gather(key=&quot;algo&quot;,value=&quot;score&quot;,logit,lasso,ridge) head(mat.score) meth obs algo score 1 brutes 1 logit 1.000000e+00 2 brutes 1 logit 1.000000e+00 3 brutes 1 logit 1.000000e+00 4 brutes 1 logit 2.220446e-16 5 brutes 1 logit 1.000000e+00 6 brutes 1 logit 1.000000e+00 On pourra déduire de cette matrice différents critères de performance. Nous commençons par l’AUC qui peut se calculer directement à partir des valeurs de score res_auc &lt;- mat.score %&gt;% group_by(meth,algo) %&gt;% summarize(AUC=round(pROC::auc(obs,score),3)) %&gt;% spread(key=algo,value=AUC) res_auc # A tibble: 5 x 4 # Groups: meth [5] meth lasso logit ridge &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 brutes 0.943 0.831 0.98 2 over 0.973 0.79 0.977 3 smote 0.973 0.68 0.977 4 tomek 0.95 0.763 0.979 5 under 0.956 0.787 0.964 La plupart des autres critères (accuracy, kappa de Cohen…) nécessitent d’avoir les groupes prédits par l’algorithme. Ces prévisions s’obtiennent en fixant un seuil \\(s\\) entre 0 et 1 et en prédisant groupe 1 si la probabilité est plus grande que \\(s\\), 0 sinon. On peut utiliser des méthodes de choix automatique de seuil mais, par souci de concision, nous présentons les résultats pour le seuil de 0.5. On crée donc une nouvelle colonne prev qui contiendra les groupes prédits : mat.score &lt;- mat.score %&gt;% mutate(prev=as.factor(round(score))) On peut maintenant en déduire : l’accuracy res_acc &lt;- mat.score %&gt;% mutate(ok=(prev==obs)) %&gt;% group_by(meth,algo) %&gt;% summarize(Acc=round(mean(ok),3)) %&gt;% spread(key=algo,value=Acc) res_acc # A tibble: 5 x 4 # Groups: meth [5] meth lasso logit ridge &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 brutes 0.969 0.886 0.97 2 over 0.961 0.847 0.963 3 smote 0.96 0.699 0.96 4 tomek 0.969 0.818 0.97 5 under 0.954 0.808 0.955 le balanced accuracy monba &lt;- function(prev,obs){confusionMatrix(prev,obs,positive=&quot;1&quot;)$byClass[11]} res_ba &lt;- mat.score %&gt;% group_by(meth,algo) %&gt;% summarize(ba=round(monba(prev,obs),3)) %&gt;% spread(key=algo,value=ba) res_ba # A tibble: 5 x 4 # Groups: meth [5] meth lasso logit ridge &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 brutes 0.898 0.832 0.9 2 over 0.931 0.806 0.935 3 smote 0.933 0.68 0.933 4 tomek 0.896 0.797 0.9 5 under 0.921 0.792 0.899 le \\(F_1\\) score monF1 &lt;- function(prev,obs){confusionMatrix(prev,obs,positive=&quot;1&quot;)$byClass[7]} res_F1 &lt;- mat.score %&gt;% group_by(meth,algo) %&gt;% summarize(F1=round(monF1(prev,obs),3)) %&gt;% spread(key=algo,value=F1) res_F1 # A tibble: 5 x 4 # Groups: meth [5] meth lasso logit ridge &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 brutes 0.879 0.65 0.884 2 over 0.864 0.579 0.873 3 smote 0.863 0.378 0.862 4 tomek 0.876 0.542 0.882 5 under 0.843 0.529 0.837 le \\(\\kappa\\) de Cohen monkappa &lt;- function(prev,obs){confusionMatrix(prev,obs,positive=&quot;1&quot;)$overall[2]} res_kappa &lt;- mat.score %&gt;% group_by(meth,algo) %&gt;% summarize(kappa=round(monkappa(prev,obs),3)) %&gt;% spread(key=algo,value=kappa) res_kappa # A tibble: 5 x 4 # Groups: meth [5] meth lasso logit ridge &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 brutes 0.862 0.584 0.867 2 over 0.842 0.491 0.851 3 smote 0.839 0.224 0.838 4 tomek 0.859 0.44 0.866 5 under 0.816 0.422 0.811 Il n’est pas facile de synthétiser tous ces résultats. Plusieurs tendances semblent néanmoins se dégager : le modèle logistique est clairement moins performant que les algorithmes ridge et lasso. Cela s’explique par le nombre important de variables dans le modèle. Les méthodes régularisées sont plus pertinentes dans ce contexte. les performances des régressions ridge et lasso sont proches, avec une préférence pour le ridge. l’apport des méthodes de ré-échantillonnage est discutable sur cet exemple. On remarque en effet les algorithmes appliqués sur les données brûtes (non ré-équilibrées) sont performants. Le ré-équilibrage peut néanmoins améliorer certains critères comme par exemple le balanced accuracy pour lequel les méthodes de sur-échantillonnage (random oversampling et smote) permettent d’améliorer. Nous terminons en présentant quelques résultats pour d’autres valeurs de seuil. En effet, nous avons ici calculer la plupart des critères en utilisant le seuil de 0.5 pour déduire les groupes prédits à partir des valeurs de score. Il est souvent intéressant de visualiser ces critères pour différentes valeurs de seuil, on utilise alors des grilles de score qui consistent à présenter les valeurs de critère en fonction des scores. Nous comparons ici les grilles de score de la méthode ridge appliquée sur les données brûtes et les données ré-échantillonnées par random oversampling et smote. Ces grilles peuvent aider l’utilisateur à choisir le seuil de prévision en fonction des objectifs métiers. On créé tout d’abord une fonction qui permettra d’obtenir les grilles de score : grille.score &lt;- function(score,nom_algo=&quot;ridge&quot;,meth=&quot;norm&quot;,grille.seuil=seq(0,1,by=0.1)){ S &lt;- score %&gt;% filter(nom_algo==algo &amp; meth==meth) S1 &lt;- S pp &lt;- as.character(grille.seuil) for (i in 1:length(grille.seuil)){ S1 &lt;- S1 %&gt;% mutate(as.numeric(score&gt;grille.seuil[i])) names(S1)[5+i] &lt;- pp[i] } S2 &lt;- S1 %&gt;% gather(key=&quot;seuil&quot;,value=&quot;prev&quot;,-meth,-obs,-algo,-score,-prev) %&gt;% dplyr::select(-meth,-algo,-score) %&gt;% mutate(prev=as.factor(prev)) mat.res &lt;- S2 %&gt;% group_by(seuil) %&gt;% summarize(sens=sensitivity(prev,obs),spec=specificity(prev,obs),Acc=mean(obs==prev), bal.acc=monba(prev,obs),F1=monF1(prev,obs),kappa=monkappa(prev,obs)) mat.res[,2:7] &lt;- round(mat.res[,2:7],3) return(mat.res) } grille.score(mat.score,nom_algo=&quot;ridge&quot;,meth=&quot;norm&quot;) # A tibble: 11 x 7 seuil sens spec Acc bal.acc F1 kappa &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 0 1 0.14 0.5 0.246 0 2 0.1 0.671 0.955 0.711 0.813 0.48 0.343 3 0.2 0.745 0.932 0.771 0.839 0.533 0.416 4 0.3 0.851 0.911 0.86 0.881 0.645 0.566 5 0.4 0.918 0.885 0.913 0.901 0.74 0.69 6 0.5 0.983 0.843 0.964 0.913 0.867 0.846 7 0.6 0.991 0.797 0.964 0.894 0.861 0.841 8 0.7 0.995 0.739 0.959 0.867 0.834 0.811 9 0.8 0.997 0.672 0.952 0.835 0.796 0.77 10 0.9 0.999 0.562 0.938 0.78 0.717 0.685 11 1 1 0 0.86 0.5 NA 0 grille.score(mat.score,nom_algo=&quot;ridge&quot;,meth=&quot;over&quot;) # A tibble: 11 x 7 seuil sens spec Acc bal.acc F1 kappa &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 0 1 0.14 0.5 0.246 0 2 0.1 0.671 0.955 0.711 0.813 0.48 0.343 3 0.2 0.745 0.932 0.771 0.839 0.533 0.416 4 0.3 0.851 0.911 0.86 0.881 0.645 0.566 5 0.4 0.918 0.885 0.913 0.901 0.74 0.69 6 0.5 0.983 0.843 0.964 0.913 0.867 0.846 7 0.6 0.991 0.797 0.964 0.894 0.861 0.841 8 0.7 0.995 0.739 0.959 0.867 0.834 0.811 9 0.8 0.997 0.672 0.952 0.835 0.796 0.77 10 0.9 0.999 0.562 0.938 0.78 0.717 0.685 11 1 1 0 0.86 0.5 NA 0 grille.score(mat.score,nom_algo=&quot;ridge&quot;,meth=&quot;smote&quot;) # A tibble: 11 x 7 seuil sens spec Acc bal.acc F1 kappa &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 0 1 0.14 0.5 0.246 0 2 0.1 0.671 0.955 0.711 0.813 0.48 0.343 3 0.2 0.745 0.932 0.771 0.839 0.533 0.416 4 0.3 0.851 0.911 0.86 0.881 0.645 0.566 5 0.4 0.918 0.885 0.913 0.901 0.74 0.69 6 0.5 0.983 0.843 0.964 0.913 0.867 0.846 7 0.6 0.991 0.797 0.964 0.894 0.861 0.841 8 0.7 0.995 0.739 0.959 0.867 0.834 0.811 9 0.8 0.997 0.672 0.952 0.835 0.796 0.77 10 0.9 0.999 0.562 0.938 0.78 0.717 0.685 11 1 1 0 0.86 0.5 NA 0 "],["références.html", "Références", " Références "]]
