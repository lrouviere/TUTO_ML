[["index.html", "Machine learning Présentation", " Machine learning Laurent Rouvière 2021-02-17 Présentation Ce tutoriel présente une introduction au machine learning avec R. On pourra trouver : les supports de cours associés à ce tutoriel ainsi que les données utilisées à l’adresse suivante https://lrouviere.github.io/machine_learning/ ; le tutoriel sans les correction à l’url https://lrouviere.github.io/TUTO_ML/ le tutoriel avec les corrigés (à certains moment) à l’url https://lrouviere.github.io/TUTO_ML/correction/. Il est recommandé d’utiliser mozilla firefox pour lire le tutoriel. Les thèmes suivants sont abordés : Estimation du risque, présentation du package caret ; SVM, cas séparable, non séparable et astuce du noyau ; Arbres, notamment l’algorithme CART ; Agrégation d’arbres, forêts aléatoires et gradient boosting ; Réseaux de neurones et introduction au deep learning, perceptron multicouches avec keras. Il existe de nombreuses références sur le machine learning, la plus connue étant certainement Hastie, Tibshirani, and Friedman (2009), disponible en ligne à l’url https://web.stanford.edu/~hastie/ElemStatLearn/. On pourra également consulter Boehmke and Greenwell (2019) qui propose une présentation très claire des algorithmes machine learning avec R. Cet ouvrage est également disponible en ligne à l’url https://bradleyboehmke.github.io/HOML/. Références "],["caret.html", "Chapitre 1 Estimation du risque avec caret 1.1 Notion de risque en apprentissage supervisé 1.2 La validation croisée 1.3 Le package caret 1.4 La courbe ROC 1.5 Compléments", " Chapitre 1 Estimation du risque avec caret 1.1 Notion de risque en apprentissage supervisé L’apprentissage supervisé consiste à expliquer ou prédire une sortie \\(y\\in\\mathcal Y\\) par des entrées \\(x\\in\\mathcal X\\) (le plus souvent \\(\\mathcal X=\\mathbb R^p\\)). Cela revient à trouver un algorithme ou machine représenté par une fonction \\[f:\\mathcal X\\to\\mathcal Y\\] qui à une nouvelle observation \\(x\\) associe la prévision \\(f(x)\\). Bien entendu le problème consiste à chercher le meilleur algorithme pour le cas d’intérêt. Cette notion nécessite de meilleur algorithme la définition de critères que l’on va chercher à optimiser. Les critères sont le plus souvent définis à partir du fonction de perte \\[\\begin{align*} \\ell:\\mathcal Y \\times\\mathcal Y &amp; \\mapsto \\mathbb R^+ \\\\ (y,y^\\prime) &amp; \\to\\ell(y,y^\\prime) \\end{align*}\\] où \\(\\ell(y,y^\\prime)\\) représentera l’erreur (ou la perte) pour la prévision \\(y^\\prime\\) par rapport à l’observation \\(y\\). Si on représente le phénomène d’intérêt par un couple aléatoire \\((X,Y)\\) à valeurs dans \\(\\mathcal X\\times\\mathcal Y\\), on mesurera la performance d’un algorithme \\(f\\) par son risque \\[\\mathcal R(f)=\\mathbf E[\\ell(Y,f(X))].\\] Trouver le meilleur algorithme revient alors à trouver \\(f\\) qui minimise \\(\\mathcal R(f)\\). Bien entendu, ce cadre possède une utilité limitée en pratique puisqu’on ne connaît jamais la loi de \\((X,Y)\\), on ne pourra donc jamais calculé le vrai risque d’un algorithme \\(f\\). Tout le problème va donc être de trouver l’algorithme qui a le plus petit risque à partir de \\(n\\) observations \\((x_1,y_1),\\dots,(x_n,y_n)\\). Nous verrons dans les chapitres suivants plusieurs façons de construire des algorithmes mais, dans tous les cas, un algorithme est représenté par une fonction \\[f_n:\\mathcal X\\times(\\mathcal X\\times\\mathcal Y)^n\\to\\mathcal Y\\] qui, pour une nouvelle donnée \\(x\\), renverra la prévision \\(f_n(x)\\) calculée à partir de l’échantillon qui vit dans \\((\\mathcal X\\times\\mathcal Y)^n\\). Dès lors la question qui se pose est de calculer (ou plutôt d’estimer) le risque (inconnu) \\(\\mathcal R(f_n)\\) d’un algorithme \\(f_n\\). Les techniques classiques reposent sur des algorithmes de type validation croisée. Nous les mettons en œuvre dans cette partie pour un algorithme simple : les \\(k\\) plus proches voisins. On commencera par programmer ces techniques “à la main” puis on utilisera le package caret qui permet de calculer des risques pour quasiment tous les algorithmes que l’on retrouver en apprentissage supervisé. 1.2 La validation croisée On cherche à expliquer une variable binaire \\(Y\\) par deux variables quantitatives \\(X_1\\) et \\(X_2\\) à l’aide du jeu de données suivant n &lt;- 2000 set.seed(12345) X1 &lt;- runif(n) X2 &lt;- runif(n) set.seed(9012) R1 &lt;- X1&lt;=0.25 R2 &lt;- (X1&gt;0.25 &amp; X2&gt;=0.75) R3 &lt;- (X1&gt;0.25 &amp; X2&lt;0.75) Y &lt;- rep(0,n) Y[R1] &lt;- rbinom(sum(R1),1,0.25) Y[R2] &lt;- rbinom(sum(R2),1,0.25) Y[R3] &lt;- rbinom(sum(R3),1,0.75) donnees &lt;- data.frame(X1,X2,Y) donnees$Y &lt;- as.factor(donnees$Y) ggplot(donnees)+aes(x=X1,y=X2,color=Y)+geom_point() On considère la perte indicatrice : \\(\\ell(y,y^\\prime)=\\mathbf 1_{y\\neq y^\\prime}\\), le risque d’un algorithme \\(f\\) est donc \\[\\mathcal R(f)=\\mathbf E[\\mathbf 1_{Y\\neq f(X)}]=\\mathbf P(Y\\neq f(X)),\\] il est appelé probabilité d’erreur ou erreur de classification. Séparer le jeu de données en un échantillon d’apprentissage dapp de taille 1500 et un échantillon test dtest de taille 500. set.seed(234) indapp &lt;- sample(nrow(donnees),1500) dapp &lt;- donnees[indapp,] dtest &lt;- donnees[-indapp,] On considère la règle de classification des \\(k\\) plus proches voisins. Pour un entier \\(k\\) plus petit que \\(n\\) et un nouvel individu \\(x\\), cette règle affecte à \\(x\\) le label majoritaire des \\(k\\) plus proches voisins de \\(x\\). Sur R on utilise la fonction knn du package class. On peut par exemple obtenir les prévisions des individus de l’échantillon test de la règle des 3 plus proches voisins avec library(class) knn3 &lt;- knn(dapp[,1:2],dtest[,1:2],cl=dapp$Y,k=3) Calculer l’erreur de classification de la règle des 3 plus proches voisins sur les données test (procédure validation hold out). mean(knn3!=dtest$Y) [1] 0.338 Expliquer la fonction knn.cv. Cette fonction permet, pour la règle des plus proches voisins, de prédire le groupe de chaque individu par validation croisée leave-one-out : \\[\\widehat y_i=g_{k,i}(x_i),\\quad i=1,\\dots,n\\] où \\(g_{k,i}\\) désigne la règle de \\(k\\) plus proche voisins construites à partir de l’échantillon amputé de la \\(i\\)ème observation. Calculer l’erreur de classification de la règle des 3 plus proches voisins par validation croisée leave-one-out. prev_cv &lt;- knn.cv(donnees[,-3],cl=donnees$Y,k=3) On peut alors estimer l’erreur de la règle des 10 ppv par \\[\\frac{1}{n}\\sum_{i=1}^n1_{g_{k,i}(x_i)\\neq y_i}.\\] mean(prev_cv!=donnees$Y) [1] 0.334 On considère le vecteur de plus proches voisins suivant : K_cand &lt;- seq(1,500,by=20) Sélectionner une valeur de \\(k\\) dans ce vecteur à l’aide d’une validation hold out et d’un leave-one-out : On calcule l’erreur de classification par validation hold out pour chaque valeur de \\(k\\) : err.ho &lt;- rep(0,length(K_cand)) for (i in 1:length(K_cand)){ knni &lt;- knn(dapp[,1:2],dtest[,1:2],cl=dapp$Y,k=K_cand[i]) err.ho[i] &lt;- mean(knni!=dtest$Y) } Puis on choisit la valeur de \\(k\\) pour laquelle l’erreur est minimale. K_cand[which.min(err.ho)] [1] 41 On de même chose avec la validation croisée leave-one-out : err.loo &lt;- rep(0,length(K_cand)) for (i in 1:length(K_cand)){ knni &lt;- knn.cv(donnees[,-3],cl=donnees$Y,k=K_cand[i]) err.loo[i] &lt;- mean(knni!=donnees$Y) } K_cand[which.min(err.loo)] [1] 121 Faire la même chose à l’aide d’une validation croisée 10 blocs. On pourra construire les blocs avec set.seed(2345) blocs &lt;- caret::createFolds(1:nrow(donnees),10,returnTrain = TRUE) err.cv &lt;- rep(0,length(K_cand)) prev &lt;- donnees$Y for (i in 1:length(K_cand)){ for (j in 1:length(blocs)){ train &lt;- donnees[blocs[[j]],] test &lt;- donnees[-blocs[[j]],] prev[-blocs[[j]]] &lt;- knn(train[,1:2],test[,1:2],cl=train$Y,k=K_cand[i]) } err.cv[i] &lt;- mean(prev!=donnees$Y) } K_cand[which.min(err.cv)] [1] 101 1.3 Le package caret Dans la partie précédente, nous avons utiliser des méthodes de validation croisée pour sélectionner le nombre de voisins dans l’algorithme des plus proches voisins. L’approche revenait à estimer un risque pour une grille de valeurs candidates de \\(k\\) choisir la valeur de \\(k\\) qui minimise le risque estimé. Cette pratique est courante en machine learning : on la retrouve fréquemment pour calibrer les algorithmes. Le protocole est toujours le même, pour un méthode donnée il faut spécifier : une grille de valeurs pour les paramètres un risque un algorithme pour estimer le risque. Le package caret permet d’appliquer ce protocole pour plus de 200 algorithmes machine learning. On pourra trouver une documentation complète à cette url http://topepo.github.io/caret/index.html. Deux fonctions sont à utiliser : traincontrol qui permettra notamment de spécifier l’algorithme pour estimer le risque ainsi que les paramètres de cet algorithme ; train dans laquelle on renseignera les données, la grille de candidats… On reprend les données de la partie précédente. Expliquer les sorties des commandes library(caret) set.seed(321) ctrl1 &lt;- trainControl(method=&quot;LGOCV&quot;,number=1) KK &lt;- data.frame(k=K_cand) caret.ho &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl1,tuneGrid=KK) caret.ho k-Nearest Neighbors 2000 samples 2 predictor 2 classes: &#39;0&#39;, &#39;1&#39; No pre-processing Resampling: Repeated Train/Test Splits Estimated (1 reps, 75%) Summary of sample sizes: 1500 Resampling results across tuning parameters: k Accuracy Kappa 1 0.602 0.1956346 21 0.690 0.3649415 41 0.694 0.3736696 61 0.706 0.3992546 81 0.700 0.3867338 101 0.712 0.4122641 121 0.700 0.3882944 141 0.706 0.4017971 161 0.700 0.3903629 181 0.702 0.3941710 201 0.700 0.3898471 221 0.696 0.3806637 241 0.692 0.3714491 261 0.698 0.3829078 281 0.692 0.3693074 301 0.696 0.3764358 321 0.682 0.3474407 341 0.682 0.3468831 361 0.678 0.3352601 381 0.672 0.3214167 401 0.668 0.3113633 421 0.666 0.3057172 441 0.658 0.2853800 461 0.658 0.2841354 481 0.654 0.2732314 Accuracy was used to select the optimal model using the largest value. The final value used for the model was k = 101. plot(caret.ho) On obtient ici l’accuracy (1 moins l’erreur de classification) pour chaque valeur de \\(k\\) calculé par validation hold out. Cette technique a été précisée dans la fonction trainControl via l’option method=“LGOCV”. Un autre indicateur est calculé : le kappa de Cohen. Cet indicateur peut se révéler pertinent en présence de données déséquilibrées, on pourra trouver de l’information sur cet indicateur dans ce document https://lrouviere.github.io/INP-HB/cours_don_des.pdf En modifiant les paramètres du code précédent, retrouver les résultats de la validation hold out de la partie précédente. On pourra utiliser l’option index dans la fonction trainControl. ctrl2 &lt;- trainControl(method=&quot;LGOCV&quot;,number=1,index=list(indapp)) caret.ho2 &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl2,tuneGrid=KK) On retrouve bien la même valeur de \\(k\\). caret.ho2$bestTune k 3 41 Utiliser caret pour sélectionner \\(k\\) par validation croisée leave-one-out. ctrl3 &lt;- trainControl(method=&quot;LOOCV&quot;,number=1) caret.loo &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl3,tuneGrid=KK) caret.loo$bestTune On remarque que le temps de calcul ets beaucoup plus long qu’avec la fonction knn.cv. Cela vient du fait que train recalcule l’algorithme des kppv \\(n\\) fois tandis que knn.cv utilise une astuce matricielle pour faire la validation croisée leave-one-out. Heureusement, on a quand même le même résultat : caret.loo$bestTune k 7 121 Faire de même pour la validation croisée 10 blocs en gardant les mêmes blocs que dans la partie précédente. ctrl4 &lt;- trainControl(method=&quot;cv&quot;,index=blocs) caret.cv &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl4,tuneGrid=KK) Là encore, on retrouve bien la même valeur : caret.cv$bestTune k 6 101 1.4 La courbe ROC C’est un critère fréquemment utilisé pour mesurer la performance d’un score. Etant donné \\((X,Y)\\) un couple aléatoire à valeurs dans \\(\\mathcal X\\times \\{-1,1\\}\\), on rappelle qu’un score est une fonction \\(S:\\mathcal X\\to\\mathbb R\\). Dans la plupart des cas, un score s’obtient en estimant la probabilité \\(\\mathbf P(Y=1|X=x)\\). Pour un seuil \\(s\\in\\mathbb R\\) fixé, un score possède deux types d’erreur \\[\\alpha(s)=\\mathbf P(S(X)\\geq s|Y=-1)\\quad\\text{et}\\quad\\beta(s)=\\mathbf P(S(X)&lt; s|Y=1).\\] La courbe ROC est la courbe paramétrée définie par : \\[\\left\\{ \\begin{array}{l} x(s)=\\alpha(s)=\\mathbf P(S(X)&gt;s|Y=-1) \\\\ y(s)=1-\\beta(s)=\\mathbf P(S(X)\\geq s|Y=1) \\end{array}\\right.\\] Elle permet donc de visualiser sur une seul graphe 2D ces deux erreurs pour toutes les valeurs de seuil \\(s\\). Exercice 1.1 (Étude de la courbe ROC) On considère dans cet exercice une fonction de score \\(S\\) que l’on suppose absolument continue. Montrer que la courbe ROC vit dans le carré \\([0,1]^2\\). On note \\(x(-\\infty)=\\lim_{n\\to -\\infty}x(s)\\). On a \\[x(-\\infty)=\\lim_{n\\to -\\infty}\\mathbf P(S(X)&gt;s|Y=-1)=1.\\] De même on montre facilement \\(y(-\\infty)=1\\), \\(x(\\infty)=0\\) et \\(y(\\infty)=0\\). On déduit donc que la courbe ROC part du point \\((1,1)\\) pour arriver au point \\((0,0)\\) sans quitter le carré \\([0,1]^2\\). On suppose dans cette question que \\(S\\) est parfait, ce qui revient à dire qu’il sépare parfaitement les 2 groupes. Mathématiquement on traduit cela par l’existence d’un seuil \\(s^\\star\\in\\mathbb R\\) tel que \\[\\mathbf P(Y=1|S(X)\\geq s^\\star)=1\\quad\\textrm{et}\\quad\\mathbf P(Y=-1|S(X)&lt;s^\\star)=1.\\] Analyser la courbe ROC du score parfait. Il est facile de voir que \\(\\alpha(s^\\star)=\\beta(s^\\star)=0\\). Par conséquent la courbe ROC du score parfait passe par le point \\((0,1)\\). On déduit que la courbe ROC du score parfait est l’union des segments \\[[(0,0),(1,0)]\\quad\\text{et}\\quad[(1,0),(1,1)].\\] On suppose dans cette question que \\(S\\) est aléatoire dans le sens où \\(S(X)\\) est indépendante de \\(Y\\) (cela revient à dire que les notes \\(S(X)\\) n’ont aucun lien avec le groupe). Analyser la courbe ROC d’un tel score. En utilisant l’indépendance et l’absolu continuité de \\(S(X)\\) on a \\[x(s)=\\mathbf P(S(X)&gt;s|Y=-1)=\\mathbf P(S(X)&gt;s)=\\mathbf P(S(X)\\geq s)=\\mathbf P(S(X)\\geq s|Y=1)=y(s),\\] On déduit que la courbe ROC est la première bissectrice. Exercice 1.2 (Courbe ROC avec R) On dispose de 4 fonctions de score \\(S_j(x)\\) dont on souhaite visualiser les courbes ROC à partir des valeurs de score calculés sur un échantillon. On trouvera dans le tableau df les scores \\(S_j(X_i),i=1,\\dots,n\\) ainsi que les observations des groupes \\(Y_i\\) set.seed(12345) n &lt;- 200 Y &lt;- rbinom(n,1,0.5) S1 &lt;- runif(n) S2 &lt;- S1 S2[Y==1] &lt;- runif(sum(Y==1),0.6,1) S2[Y==0] &lt;- runif(sum(Y==0),0,0.6) S3 &lt;- S2 S3[Y==1][1:10] &lt;- runif(10,0,0.6) S3[Y==0][1:10] &lt;- runif(10,0.6,1) df &lt;- data.frame(S1,S2,S3,Y=Y) head(df) S1 S2 S3 Y 1 0.5885923 0.6301922 0.419557838 1 2 0.8925918 0.7897537 0.006244182 1 3 0.1237949 0.7058358 0.302344248 1 4 0.5133090 0.6922984 0.438635526 1 5 0.6636402 0.2034380 0.806095919 0 6 0.7655420 0.4036820 0.665537030 0 Visualiser, pour chaque score, les valeurs de score en fonction de \\(Y\\). Commenter df1 &lt;- df %&gt;% pivot_longer(-Y,names_to=&quot;Score&quot;,values_to=&quot;valeurs&quot;) ggplot(df1)+aes(x=valeurs,y=Score,color=as.factor(Y))+geom_point() On remarque que S2 sépare parfaitement les deux groupes. La séparation de S3 est satisfaisante mais pas parfaite. S1 semble qunt à lui ordonner les observations indépendamment du groupe (proche du score aléatoire). Visualiser sur un même graphe les trois courbes ROC. On pourra utiliser d’abord utiliser la fonction roc du package pROC puis la fonction geom_roc du plotROC. Avec pROC on peut comobiner les fonction roc et plot. library(pROC) roc.obj &lt;- roc(Y~.,data=df) plot(roc.obj[[1]]) plot(roc.obj[[2]],add=TRUE,col=&quot;red&quot;) plot(roc.obj[[3]],add=TRUE,col=&quot;blue&quot;) legend(&quot;bottomright&quot;,legend=c(&quot;S1&quot;,&quot;S2&quot;,&quot;S3&quot;),col=c(&quot;black&quot;,&quot;red&quot;,&quot;blue&quot;),lwd=3,cex=0.5) pROC possède également une fonction ggroc qui permet d’obtenir les courbes ROC en ggplot. pROC::ggroc(roc.obj)+labs(color=&quot;Score&quot;) Enfin plotROC possède le verbe geom_roc : library(plotROC) ggplot(df1)+aes(d=Y,m=valeurs,color=Score)+geom_roc(n.cuts=0) Calculer les AUC à l’aide de la fonction auc. lapply(roc.obj,auc) $S1 Area under the curve: 0.5785 $S2 Area under the curve: 1 $S3 Area under the curve: 0.9148 On peut aussi les obtenir avec la syntaxe dplyr df %&gt;% mutate(Y=as.factor(Y)) %&gt;% summarize_at(1:3,~auc(Y,.)) %&gt;% round(3) S1 S2 S3 1 0.579 1 0.915 df1 %&gt;% mutate(Y=as.factor(Y)) %&gt;% group_by(Score) %&gt;% summarize(AUC=round(as.numeric(auc(Y,valeurs)),3)) # A tibble: 3 x 2 Score AUC * &lt;chr&gt; &lt;dbl&gt; 1 S1 0.579 2 S2 1 3 S3 0.915 On rappelle que l’AUC vérifier la propriété suivante : si \\((X_1,Y_1)\\) et \\((X_2,Y_2)\\) sont indépendantes et de même loi que \\((X,Y)\\), on a \\[AUC(S)=\\mathbf P(S(X_1)\\geq S(X_2)|(Y_1,Y_2)=(1,-1)).\\] Utiliser cette propriété pour retrouver l’AUC de S3. On peut estimer l’AUC en calculant, parmi les paires qui vérifient \\((Y_1,Y_2)=(1,-1)\\), la proportion de paires qui vérifient \\(S(X_1)&lt;S(X_2)\\). Si on note \\[\\mathcal I=\\{(i,j),y_i=1,y_j=0\\},\\] alors l’estimateur s’écrit \\[\\widehat{AUC}(S)=\\frac{1}{|\\mathcal I|}\\sum_{(i,j)\\in\\mathcal I}\\mathbf 1_{S(X_i)&gt;S(S_j)}.\\] D0 &lt;- which(df$Y==0) D1 &lt;- which(df$Y==1) S30 &lt;- S3[D0] S31 &lt;- S3[D1] S3.01 &lt;- expand.grid(S30=S30,S31=S31) mean(S3.01$S31&gt;S3.01$S30) [1] 0.9147727 1.5 Compléments 1.5.1 Calcul parallèle Les validations croisées peuvent se révéler coûteuses en temps de calcul. On utilise souvent des techniques de parallélisation pour améliorer les performances computationnelles. Ces techniques sont relativement facile à mettre en œuvre avec caret, on peut par exemple utiliser la librairie doParallel pour utiliser plusieurs cœurs de la machine. On compare les temps de calculs pour une même validation croisée 10 blocs exécutée avec 1 cœur et 4 cœurs : library(doParallel) ctrl4 &lt;- trainControl(method=&quot;cv&quot;,index=blocs) cl &lt;- makePSOCKcluster(1) registerDoParallel(cl) temps1 &lt;- system.time(ee3 &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;, trControl=ctrl4,tuneGrid=KK)) stopCluster(cl) cl &lt;- makePSOCKcluster(4) registerDoParallel(cl) temps4 &lt;- system.time(ee3 &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;, trControl=ctrl4,tuneGrid=KK)) stopCluster(cl) On compare ces deux temps de calcul temps1 user system elapsed 12.985 0.062 13.124 temps4 user system elapsed 0.625 0.018 5.935 Sans surprise, l’exécution est beaucoup plus rapide avec 4 cœurs. 1.5.2 Répéter les méthodes de rééchantillonnage Les méthodes d’estimation du risque présentées dans cette partie (hold out, validation croisée) sont basées sur du rééchantillonnage. Elles peuvent se révéler sensible à la manière de couper l’échantillon. C’est pourquoi il est recommandé de les répéter plusieurs fois et de moyenner les erreurs sur les répétitions. Ces répétitions sont très faciles à mettre en œuvre avec caret, par exemple pour la validation hold out on utilise l’option number: ctrl &lt;- trainControl(method=&quot;LGOCV&quot;,number=5) caret.ho.rep &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl,tuneGrid=KK) la validation croisée on utilise les options repeatedcv et repeats : ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;,repeats=5) caret.ho.rep &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl,tuneGrid=KK) 1.5.3 Modifier le risque Enfin nous avons uniquement considéré l’erreur de classification. Il est bien entendu possible d’utiliser d’autres risques pour évaluer les performances. C’est l’option metric de la fonction train qui permet généralement de spécifier le risque, si on est par exemple intéressé par l’aire sur la courbe ROC (AUC) on fera : donnees1 &lt;- donnees names(donnees1)[3] &lt;- &quot;Class&quot; levels(donnees1$Class) &lt;- c(&quot;G0&quot;,&quot;G1&quot;) ctrl &lt;- trainControl(method=&quot;LGOCV&quot;,number=1,classProbs=TRUE,summary=twoClassSummary) caret.auc &lt;- train(Class~.,data=donnees1,method=&quot;knn&quot;,trControl=ctrl, tuneGrid=KK,metric=&quot;ROC&quot;) On obtient ici pour chaque valeur de \\(k\\), l’AUC ainsi que les sensibilité et spécificité : caret.auc k-Nearest Neighbors 2000 samples 2 predictor 2 classes: &#39;G0&#39;, &#39;G1&#39; No pre-processing Resampling: Repeated Train/Test Splits Estimated (1 reps, 75%) Summary of sample sizes: 1500 Resampling results across tuning parameters: k ROC Sens Spec 1 0.6338315 0.5937500 0.6739130 21 0.7488031 0.6473214 0.8152174 41 0.7599072 0.6696429 0.8115942 61 0.7554590 0.6785714 0.8188406 81 0.7586698 0.6875000 0.8224638 101 0.7628025 0.6785714 0.8333333 121 0.7603196 0.6830357 0.8333333 141 0.7605703 0.6830357 0.8297101 161 0.7625194 0.6875000 0.8224638 181 0.7616945 0.6875000 0.8188406 201 0.7609990 0.6830357 0.8188406 221 0.7582411 0.6696429 0.8188406 241 0.7567854 0.6607143 0.8224638 261 0.7563406 0.6473214 0.8188406 281 0.7546260 0.6383929 0.8297101 301 0.7530328 0.6294643 0.8333333 321 0.7554914 0.6205357 0.8297101 341 0.7530166 0.6205357 0.8369565 361 0.7518925 0.5848214 0.8405797 381 0.7500970 0.5357143 0.8550725 401 0.7472179 0.5133929 0.8586957 421 0.7472907 0.4866071 0.8695652 441 0.7432550 0.4732143 0.8768116 461 0.7429720 0.4598214 0.8840580 481 0.7404487 0.4241071 0.8876812 ROC was used to select the optimal model using the largest value. The final value used for the model was k = 101. Et on choisira la valeur de \\(k\\) qui maximise l’AUC : caret.auc$bestTune k 6 101 "],["lda.html", "Chapitre 2 Analyse discriminante linéaire 2.1 Prise en main : LDA et QDA sur les iris de Fisher 2.2 Un cas avec beaucoup de classes 2.3 Grande dimension : reconnaissance de phonèmes 2.4 Exercices", " Chapitre 2 Analyse discriminante linéaire L’analyse discriminante linéaire est un algorithme de référence en classification supervisée. Il peut être appréhendé de deux façons complémentaires : une approche géométrique qui revient à chercher des hyperplans qui séparent au mieux les groupes ; une approche modèle qui fait l’hypothèse que les lois des covariables sont des vecteurs gaussiens avec des valeurs de paramètres différentes pour chaque groupe. On considère \\((x_1,y_1),\\dots,(x_n,y_n)\\) un échantillon où \\(x_i\\) est à valeurs dans \\(\\mathbb R^d\\) et \\(y_i\\) dans \\(\\{0,1\\}\\). L’approche géométrique revient à chercher une droite de \\(\\mathbb R^d\\) d’équation \\(a_1x_1+\\dots+a_dx_d=0\\) telle que : les centres de gravité de chaque groupe projeté sur cette droite soit au mieux séparé \\(\\Longrightarrow\\) maximiser la distance inter-classe. les observations projetés soient proches de leur centre de gravité projeté \\(\\Longrightarrow\\) minimiser la distance intra-classe. Le compromis entre ces deux distances s’obtient en maximisant le coefficient de Rayleigh qui est le quotient entre ces deux distance : \\[J(a)=\\frac{B(a)}{W(a)}=\\frac{a^tBa}{a^tWa}\\] où \\(B\\) et \\(W\\) sont les matrices inter et intra classes définies pas \\[B=\\frac{1}{n}\\sum_{k=1}^Kn_k(g_k-g)(g_k-g)^t\\quad\\text{et}\\quad W=\\frac{1}{n}\\sum_{k=1}^Kn_kV_k\\quad\\text{avec}\\quad V_k=\\frac{1}{n_k}\\sum_{i:Y_i=k}(X_i-g_k)(X_i-g_k)^t.\\] Ici \\(g\\) désigne le centre de gravité du nuage \\(x_i,i=1,\\dots,n\\) et \\(g_k,k=0,1\\) les centres de gravité des deux groupes. La solution est donnée par un vecteur propre associé à la plus grande valeur propre de \\(W^{-1}B\\). L’approche modèle fait l’hypothèse que les vecteurs \\(X|Y=k,k=0,1\\) sont des vecteurs gaussiens d’espérance \\(\\mu_k\\in\\mathbb R^d\\) et de matrice de variance covariance \\(\\Sigma\\). Ces paramètres sont estimés par maximum de vraisemblance et on déduit les probablités a posteriori par la formule de Bayes : \\[\\mathbf P(Y=k|X=x)=\\frac{\\pi_kf_{X|Y=k}(x)}{f(x)}\\] Le lien entre ces deux approches est établi dans l’exercice 2.4. Nous proposons dans cette partie quelques exercices pour mettre en œuvre et analyser des analyses discriminantes avec R. 2.1 Prise en main : LDA et QDA sur les iris de Fisher On considère les données sur les iris de Fisher. data(iris) A l’aide de la fonction PCA du package FactoMineR, réaliser une ACP en utilisant comme variables actives les 4 variables quantitatives du jeu de données. On mettra la variable Species comme variable qualitative supplémentaire (option quali.sup). Représenter le nuage des individus sur les 2 premiers axes de l’ACP en utilisant une couleur différente pour chaque espèce d’iris (option habillage). A l’aide de la fonction lda du package MASS, effectuer une analyse discriminante linéaire permettant d’expliquer l’espèce par les 4 autres variables explicatives. Représenter le nuage des individus sur les deux premiers axes de l’analyse discriminante linéaire (en utilisant une couleur différente pour chaque espèce d’iris). Rappeler comment sont obtenues les coordonnées des individus sur chaque axe. En déduire une interprétation de la position des individus. Comparer les représentations des questions 2 et 4. Expliquer les sorties des commandes suivantes (mod.lda est l’objet construit avec la fonction lda). score &lt;- predict(mod.lda)$x ldahist(score[,1],iris[,5]) ldahist(score[,2],iris[,5]) ```` Exécuter et analyser les sorties de la commande mod.lda2 &lt;- lda(Species~.,data=iris,CV=TRUE) Comparer, en terme d’erreur de prévision, les performances de LDA et QDA. 2.2 Un cas avec beaucoup de classes On considère les jeux de données Vowel (training et test) qui se trouvent à cet url. On peut les importer avec dapp &lt;- read_csv(&quot;https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.train&quot;)[,-1] dtest &lt;- read_csv(&quot;https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.test&quot;)[,-1] Expliquer le problème. Effectuer une analyse discriminante linéaire (uniquement avec les données d’apprentissage) et visualiser les individus sur les 2 premiers axes de l’analyse discriminante. On pourra utiliser predict. La fonction suivante permet de choisir les axes à visualiser, ainsi que les centres de gravité projetés des groupes. repres_axes &lt;- function(prev,cdg,axe1=1,axe2=2){ cdg &lt;- prev %&gt;% group_by(y) %&gt;% summarise_all(mean) nom1 &lt;- paste(&quot;LD&quot;,as.character(axe1),sep=&quot;&quot;) nom2 &lt;- paste(&quot;LD&quot;,as.character(axe2),sep=&quot;&quot;) ggplot(prev)+aes_string(x=as.name(nom1),y=as.name(nom2))+geom_point(aes(color=y))+geom_point(data=cdg,aes(color=y),shape=17,size=4)+theme_classic() } Étudier la pertinence des axes. Représenter les individus sur le premier plan factoriel de l’ACP, on utilisera une couleur différente pour chaque groupe. On pourra utiliser le package FactoMineR. Comparer cette projection avec celle obtenue par l’analyse discriminante linéaire. Évaluer la performance de la lda sur les données test. Comparer avec l’analyse discriminante quadratique. Expliquer comment on peut faire de la prévision en réduisant la dimension de l’espace des \\(X\\). Proposer une méthode permettant de choisir le meilleur nombre d’axes. On pourra notamment utiliser l’option dimen de la fonction predict.lda. 2.3 Grande dimension : reconnaissance de phonèmes On considère le jeu de données phoneme téléchargeable à l’url https://github.com/cran/ElemStatLearn/blob/master/data/phoneme.RData. load(&quot;data/phoneme.RData&quot;) data(phoneme) donnees &lt;- phoneme[,-258] Expliquer le problème et représenter pour chaque groupe la courbe moyenne. Séparer les données en un échantillon d’apprentissage de taille 3000 et un échantillon test de taille 1509. Effectuer une analyse discriminante linéaire et une analyse discriminante quadratique sur les données d’apprentissage uniquement. Évaluer les performances de ces deux approches sur les données test. Quels peuvent être les intérêts d’effectuer une analyse discriminante régularisée dans ce contexte ? Effectuer une telle analyse à l’aide de la fonction rda du package klaR. Sélectionner les paramètres de régularisation à l’aide du package caret. Comparer le nouveau modèle aux précédents. 2.4 Exercices Exercice 2.1 (Optimalité de la règle de Bayes) On dispose de \\(n\\) observations \\((x_1,y_1),\\dots,(x_n,y_n)\\) telles que \\(x_i\\in\\mathbb R^p\\) et \\(y_i\\in\\{0,1\\}\\) pour \\(i=1,\\dots,n\\). On souhaite expliquer les sorties \\(y_i\\) par les entrées \\(x_i\\). Rappeler la définition d’une règle de prévision. Rappeler la définition de la règle de Bayes \\(g^\\star\\) et de l’erreur de Bayes \\(L^\\star\\). Soit \\(g\\) une règle de décision. Montrer que \\[\\mathbf P(g(X)\\neq Y|X=x)=1-(\\mathbf 1_{g(x)=1}\\eta(x)+\\mathbf 1_{g(x)=0}(1-\\eta(x)))\\] où \\(\\eta(x)=\\mathbf P(Y=1|X=x)\\). En déduire que pour tout \\(x\\in\\mathcal X\\) et pour toute règle \\(g\\) \\[\\mathbf P(g(X)\\neq Y|X=x)-\\mathbf P(g^\\star(X)\\neq Y|X=x)\\geq 0.\\] Conclure. On considère \\((X,Y)\\) un couple aléatoire à valeurs dans \\(\\mathbb R\\times\\{0,1\\}\\) tel que \\[\\begin{equation*} X\\sim\\mathcal U[-2,2]\\quad\\text{et}\\quad (Y|X=x)\\sim\\left\\{ \\begin{array}{ll} \\mathcal B(1/5) &amp; \\textrm{si } x\\leq 0 \\\\ \\mathcal B(9/10) &amp; \\textrm{si } x&gt;0 \\end{array}\\right. \\end{equation*}\\] où \\(\\mathcal U[a,b]\\) désigne la loi uniforme sur \\([a,b]\\) et \\(\\mathcal B(p)\\) la loi de Bernoulli de paramètre \\(p\\). Calculer la règle de Bayes et l’erreur de Bayes. Exercice 2.2 (MV pour LDA) On cherche à expliquer une variable aléatoire \\(Y\\) à valeurs dans \\(\\{0,1\\}\\) par une variable aléatoire \\(X\\) à valeurs dans \\(\\mathbb R\\). Quels sont les paramètres à estimer dans le modèle d’analyse discriminante linéaire. Calculer la vraisemblance conditionnelle à \\(Y\\) et en déduire les estimateurs des paramètres des lois gaussiennes. Comparer les estimateurs obtenus avec ceux du cours. Exercice 2.3 (Fonctions linéaires discriminantes) On cherche à expliquer une variable aléatoire \\(Y\\) à valeurs dans \\(\\{0,1\\}\\) par une variable aléatoire \\(X\\) à valeurs dans \\(\\mathbb R^p\\). Rappeler le modèle d’analyse discriminante linéaire. Soit \\(x\\in\\mathbb R^p\\) un nouvel individu. Montrer que la règle qui consiste à affecter \\(x\\) dans le groupe qui maximise \\(\\mathbf P(Y=k|X=x)\\) est équivalente à la règle qui consiste à affecter \\(x\\) dans le groupe qui maximise les fonctions linéaires discriminantes (on prendra soin de rappeler la définition des fonctions linéaires discriminantes). Exercice 2.4 (Approche géométrique de la LDA) On considère un \\(n\\)-échantillon i.i.d. \\((x_1,y_1),\\dots,(x_n,y_n)\\) où \\(x_i\\) est à valeurs dans \\(\\mathbb R^2\\) et \\(y_i\\) dans \\(\\{0,1\\}\\). On cherche une droite vectorielle \\(a\\) telle que les projections de chaque groupe sur \\(a\\) soient séparées “au mieux”. Dit autrement, on cherche \\(a\\) telle que la distance entre les centres de gravité \\[g_0=\\frac{1}{\\mbox{card}\\{i:y_i=0\\}}\\sum_{i:y_i=0}x_i\\quad\\textrm{et}\\quad g_1=\\frac{1}{\\mbox{card}\\{i:y_i=1\\}}\\sum_{i:y_i=1}x_i\\] projetés sur \\(a\\) soit maximale (cette distance est appelée distance interclasse) ; la distance entre les projections des individus et leur centre de gravité soit minimale (distance interclasse). Pour un vecteur \\(u\\) de \\(\\mathbb R^2\\), on désigne par \\(\\pi_a(u)\\) son projeté sur la droite engendrée par \\(a\\). Sans perte de généralité on supposera dans un premier temps que \\(a\\) est de norme 1. Rappeler les définitions des variances totale \\(V\\), intra \\(W\\) et inter \\(B\\) des observations \\((x_1,y_1),\\dots,(x_n,y_n)\\). Pour \\(u\\) fixé dans \\(\\mathbb R^2\\), exprimer \\(\\pi_a(u)\\) en fonction de \\(u\\) et \\(a\\) et en déduire que \\(\\|\\pi_a(u)\\|^2=a^tuu^ta\\). Exprimer les variances totale \\(V(a)\\), intra \\(W(a)\\) et inter \\(B(a)\\) projetées sur \\(a\\) en fonction des variances calculées à la question 1. On cherche maintenant à maximiser \\[J(a)=\\frac{B(a)}{W(a)}\\] ou encore à \\[\\begin{equation} \\textrm{maximiser }B(a)\\quad\\textrm{sous la contrainte}\\quad W(a)=1. \\tag{2.1} \\end{equation}\\] La méthode des multiplicateurs de Lagrange permet de résoudre un tel problème. La solution du problème de maximisation d’une fonction \\(f(x)\\) sujette à \\(h(x)=0\\) s’obtient en résolvant l’équation \\[\\frac{\\partial L(x,\\lambda)}{\\partial x}=0,\\quad\\textrm{où}\\quad L(x,\\lambda)=f(x)+\\lambda h(x).\\] Montrer que la solution du problème (2.1) est un vecteur propre de \\(W^{-1}B\\) associé à la plus grande valeur propre de \\(W^{-1}B\\). On note \\(a^\\star\\) cette solution. Montrer que \\(a^\\star\\) est colinéaire à \\(W^{-1}(g_1-g_0)\\). On pourra admettre que, dans le cas de 2 groupes, on a \\[B=\\frac{n_0n_1}{n^2}(g_1-g_0)(g_1-g_0)^t.\\] On considère la règle géométrique d’affectation qui consiste à classer un nouvel individu \\(x\\in\\mathbb R^p\\) au groupe 1 si son projeté sur \\(a^\\star\\) est plus proche de \\(\\pi_{a^\\star}(g_1)\\) que de \\(\\pi_{a^\\star}(g_0)\\). Montrer que \\(x\\) sera affecté au groupe 1 si \\[S(x)=x^tW^{-1}(g_1-g_0)&gt;s\\] où on exprimera \\(s\\) en fonction de \\(g_0\\), \\(g_1\\) et \\(W\\). Montrer que cette règle est équivalente à choisir le groupe qui minimise la distance de Mahalanobis \\[d(x,g_k)=(x-g_k)^tW^{-1}(x-g_k),\\quad k=0,1.\\] On revient maintenant à l’approche probabiliste de l’analyse discriminante linéaire vue en cours et on considère la règle d’affectation qui consiste à décider \"groupe 1’’ si \\(\\mathbf P(Y=1|X=x)\\geq 0.5\\). Montrer que dans ce cas, un nouvel individu \\(x\\) est affecter au groupe 1 si : \\[S(x)=x^t\\Sigma^{-1}(\\mu_1-\\mu_0)&gt;\\frac{1}{2}(\\mu_1+\\mu_0)^t\\Sigma^{-1}(\\mu_1-\\mu_0)-\\log\\left(\\frac{\\pi_1}{\\pi_0}\\right).\\] Conclure. "],["arbres.html", "Chapitre 3 Arbres 3.1 Coupures CART en fonction de la nature des variables 3.2 Élagage", " Chapitre 3 Arbres Les méthodes par arbres sont des algorithmes où la prévision s’effectue à partir de moyennes locales. Plus précisément, étant donné un échantillon \\((x_1,y_1)\\dots,(x_n,y_n)\\), l’approche consiste à : construire une partition de l’espace de variables explicatives (\\(\\mathbb R^p\\)) ; prédire la sortie d’une nouvelle observation \\(x\\) en faisant : la moyenne des \\(y_i\\) pour les \\(x_i\\) qui sont dans la même classe que \\(x\\) si on est en régression ; un vote à la majorité parmi les \\(y_i\\) tels que les \\(x_i\\) qui sont dans la même classe que \\(x\\) si on est en classification. Bien entendu toute la difficulté est de trouver la “bonne partition” pour le problème d’intérêt. Il existe un grand nombre d’algorithmes qui permettent de trouver une partition. Le plus connu est l’algorithme CART (Breiman et al. 1984) où la partition est construite par divisions successives au moyen d’hyperplan orthogonaux aux axes de \\(\\mathbb R^p\\). L’algorithme est récursif : il va à chaque étape séparer un groupe d’observations (nœuds) en deux groupes (nœuds fils) en cherchant la meilleure variable et le meilleur seuil de coupure. Ce choix s’effectue à partir d’un critère d’impureté : la meilleure coupure est celle pour laquelle l’impureté des 2 nœuds fils sera minimale. Nous étudions cet algorithme dans cette partie. 3.1 Coupures CART en fonction de la nature des variables Une partition CART s’obtient en séparant les observations en 2 selon une coupure parallèle aux axes puis en itérant ce procédé de séparation binaire sur les deux groupes… Par conséquent la première question à se poser est : pour un ensemble de données \\((x_1,y_1),\\dots,(x_n,y_n)\\) fixé, comment obtenir la meilleure coupure ? Comme souvent ce sont les données qui vont répondre à cette question. La sélection de la meilleur coupure s’effectue en introduisant une fonction d’impureté \\(\\mathcal I\\) qui va mesurer le degrés d’hétérogénéité d’un nœud \\(\\mathcal N\\). Cette fonction prendra de grandes valeurs pour les nœuds hétérogènes (les valeurs de \\(Y\\) diffèrent à l’intérieur du nœud) ; faibles valeurs pour les nœuds homogènes (les valeurs de \\(Y\\) sont proches à l’intérieur du nœud). On utilise souvent comme fonction d’impureté : la variance en régression \\[\\mathcal I(\\mathcal N)=\\frac{1}{|\\mathcal N|}\\sum_{i:x_i\\in\\mathcal N}(y_i-\\overline{y}_\\mathcal N)^2,\\] où \\(\\overline{y} _\\mathcal N\\) désigne la moyenne des \\(y_i\\) dans \\(\\mathcal N\\). l’impureté de Gini en classification binaire \\[\\mathcal I(\\mathcal N)=2p(\\mathcal N)(1-p(\\mathcal N))\\] où \\(p(\\mathcal N)\\) représente la proportion de 1 dans \\(\\mathcal N\\). Les coupures considérées par l’algorithme CART sont des hyperplans orthogonaux aux axes de \\(\\mathbb R^p\\), choisir une coupure revient donc à choisir une variable \\(j\\) parmi les \\(p\\) variables explicatives et un seuil \\(s\\) dans \\(\\mathbb R\\). On peut donc représenter une coupure par un couple \\((j,s)\\). Une fois l’impureté définie, on choisira la coupure \\((j,s)\\) qui maximise le gain d’impureté entre le noeud père et ses deux noeuds fils : \\[\\Delta(\\mathcal I)=\\mathbf P(\\mathcal N)\\mathcal I(\\mathcal N)-(\\mathbf P(\\mathcal N_1(j,s))\\mathcal I(\\mathcal N_1(j,s))+\\mathbf P(\\mathcal N_2(j,s))\\mathcal I(\\mathcal N_2(j,s))\\] où * \\(\\mathcal N_1(j,s)\\) et \\(\\mathcal N_2(j,s)\\) sont les 2 nœuds fils de \\(\\mathcal N\\) engendrés par la coupure \\((j,s)\\) ; * \\(\\mathbf P(\\mathcal N)\\) représente la proportion d’observations dans le nœud \\(\\mathcal N\\). 3.1.1 Arbres de régression On considère le jeu de données suivant où le problème est d’expliquer la variable quantitative \\(Y\\) par la variable quantitative \\(X\\). n &lt;- 50 set.seed(1234) X &lt;- runif(n) set.seed(5678) Y &lt;- 1*X*(X&lt;=0.6)+(-1*X+3.2)*(X&gt;0.6)+rnorm(n,sd=0.1) data1 &lt;- data.frame(X,Y) ggplot(data1)+aes(x=X,y=Y)+geom_point() A l’aide de la fonction rpart du package rpart, construire un arbre permettant d’expliquer \\(Y\\) par \\(X\\). library(rpart) Visualiser l’arbre à l’aide des fonctions prp et rpart.plot du package rpart.plot. Écrire l’estimateur associé à l’arbre. Ajouter sur le graphe de la question 1 la partition définie par l’arbre ainsi que les valeurs prédites. 3.1.2 Arbres de classification On considère les données suivantes où le problème est d’expliquer la variable binaire \\(Y\\) par deux variables quantitatives \\(X_1\\) et \\(X_2\\). n &lt;- 50 set.seed(12345) X1 &lt;- runif(n) set.seed(5678) X2 &lt;- runif(n) Y &lt;- rep(0,n) set.seed(54321) Y[X1&lt;=0.45] &lt;- rbinom(sum(X1&lt;=0.45),1,0.85) set.seed(52432) Y[X1&gt;0.45] &lt;- rbinom(sum(X1&gt;0.45),1,0.15) data2 &lt;- data.frame(X1,X2,Y) ggplot(data2)+aes(x=X1,y=X2,color=Y)+geom_point(size=2)+scale_x_continuous(name=&quot;&quot;)+ scale_y_continuous(name=&quot;&quot;)+theme_classic() Construire un arbre permettant d’expliquer \\(Y\\) par \\(X_1\\) et \\(X_2\\). Représenter l’arbre et identifier l’éventuel problème. Écrire la règle de classification ainsi que la fonction de score définies par l’arbre. Ajouter sur le graphe de la question 1 la partition définie par l’arbre. 3.1.3 Entrée qualitative On considère les données n &lt;- 100 X &lt;- factor(rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;),n)) set.seed(1234) Y[X==&quot;A&quot;] &lt;- rbinom(sum(X==&quot;A&quot;),1,0.9) Y[X==&quot;B&quot;] &lt;- rbinom(sum(X==&quot;B&quot;),1,0.25) Y[X==&quot;C&quot;] &lt;- rbinom(sum(X==&quot;C&quot;),1,0.8) Y[X==&quot;D&quot;] &lt;- rbinom(sum(X==&quot;D&quot;),1,0.2) Y &lt;- as.factor(Y) data3 &lt;- data.frame(X,Y) Construire un arbre permettant d’expliquer \\(Y\\) par \\(X\\). Expliquer la manière dont l’arbre est construit dans ce cadre là. 3.2 Élagage Le procédé de coupe présenté précédemment permet de définir un très grand nombre d’arbres à partir d’un jeu de données (arbre sans coupure, avec une coupure, deux coupures…). Se pose alors la question de trouver le meilleur arbre parmi tous les arbres possibles. Une première idée serait de choisir parmi tous les arbres possibles celui qui optimise un critère de performance. Cette approche, bien que cohérente, n’est généralement pas possible à mettre en œuvre en pratique car le nombre d’arbres à considérer est souvent trop important. La méthode CART propose une procédure permettant de choisir automatiquement un arbre en 3 étapes : On construit un arbre maximal (très profond) \\(\\mathcal T_{max}\\) ; On sélectionne une suite d’arbres emboités : \\[\\mathcal T_{max}=\\mathcal T_0\\supset\\mathcal T_1\\supset\\dots\\supset \\mathcal T_K.\\] La sélection s’effectue en optimisant un critère Cout/complexité qui permet de réguler le compromis entre ajustement et complexité de l’arbre. On sélectionne un arbre dans cette sous-suite en optimisant un critère de performance. Cette approche revient à choisir un sous-arbre de l’arbre \\(\\mathcal T_\\text{max}\\), c’est-à-dire à enlever des branches à \\(T_\\text{max}\\), c’est pourquoi on parle d’élagage. 3.2.1 Élagage pour un problème de régression On considère les données Carseats du package ISLR. library(ISLR) data(Carseats) summary(Carseats) Sales CompPrice Income Min. : 0.000 Min. : 77 Min. : 21.00 1st Qu.: 5.390 1st Qu.:115 1st Qu.: 42.75 Median : 7.490 Median :125 Median : 69.00 Mean : 7.496 Mean :125 Mean : 68.66 3rd Qu.: 9.320 3rd Qu.:135 3rd Qu.: 91.00 Max. :16.270 Max. :175 Max. :120.00 Advertising Population Price Min. : 0.000 Min. : 10.0 Min. : 24.0 1st Qu.: 0.000 1st Qu.:139.0 1st Qu.:100.0 Median : 5.000 Median :272.0 Median :117.0 Mean : 6.635 Mean :264.8 Mean :115.8 3rd Qu.:12.000 3rd Qu.:398.5 3rd Qu.:131.0 Max. :29.000 Max. :509.0 Max. :191.0 ShelveLoc Age Education Urban Bad : 96 Min. :25.00 Min. :10.0 No :118 Good : 85 1st Qu.:39.75 1st Qu.:12.0 Yes:282 Medium:219 Median :54.50 Median :14.0 Mean :53.32 Mean :13.9 3rd Qu.:66.00 3rd Qu.:16.0 Max. :80.00 Max. :18.0 US No :142 Yes:258 On cherche ici à expliquer la variable quantitative Sales par les autres variables. Construire un arbre permettant de répondre au problème. Expliquer les sorties de la fonction printcp appliquée à l’arbre de la question précédente et calculer le dernier terme de la colonne rel error. Construire une suite d’arbres plus grandes en jouant sur les paramètres cp et minsplit de la fonction rpart. Expliquer la sortie de la fonction plotcp appliquée à l’arbre de la question précédente. Sélectionner le “meilleur” arbre dans la suite construite. Visualiser l’arbre choisi (utiliser la fonction prune). On souhaite prédire les valeurs de \\(Y\\) pour de nouveaux individus à partir de l’arbre sélectionné. Pour simplifier on considèrera ces 4 individus : new_ind &lt;- Carseats %&gt;% slice(3,58,185,218) %&gt;% dplyr::select(-Sales) new_ind CompPrice Income Advertising Population Price ShelveLoc 3 113 35 10 269 80 Medium 58 93 91 0 22 117 Bad 185 132 33 7 35 97 Medium 218 106 44 0 481 111 Medium Age Education Urban US 3 59 12 Yes Yes 58 75 11 Yes No 185 60 11 No Yes 218 70 14 No No Calculer les valeurs prédites. Séparer les données en un échantillon d’apprentissage de taille 250 et un échantillon test de taille 150. On considère la suite d’arbres définie par set.seed(4321) tree &lt;- rpart(Sales~.,data=train,cp=0.000001,minsplit=2) Dans cette suite, sélectionner un arbre très simple (avec 2 ou 3 coupures) un arbre très grand l’arbre optimal (avec la procédure d’élagage classique). Calculer l’erreur quadratique de ces 3 arbres en utilisant l’échantillon test. Refaire la comparaison avec une validation croisée 10 blocs. 3.2.2 Élagage en classification binaire et matrice de coût On considère ici les mêmes données que précédemment mais on cherche à expliquer une version binaire de la variable Sales. Cette nouvelle variable, appelée High prend pour valeurs No si Sales est inférieur ou égal à 8, Yes sinon. On travaillera donc avec le jeu data1 défini ci-dessous. High &lt;- ifelse(Carseats$Sales&lt;=8,&quot;No&quot;,&quot;Yes&quot;) data1 &lt;- Carseats %&gt;% dplyr::select(-Sales) %&gt;% mutate(High) Construire un arbre permettant d’expliquer High par les autres variables (sans Sales évidemment !) et expliquer les principales différences par rapport à la partie précédente précédente. Expliquer l’option parms dans la commande : tree1 &lt;- rpart(High~.,data=data1,parms=list(split=&quot;information&quot;)) tree1$parms $prior 1 2 0.59 0.41 $loss [,1] [,2] [1,] 0 1 [2,] 1 0 $split [1] 2 Expliquer les sorties de la fonction printcp sur le premier arbre construit et retrouver la valeur du dernier terme de la colonne rel error. Sélectionner un arbre optimal dans la suite. On considère la suite d’arbres tree2 &lt;- rpart(High~.,data=data1,parms=list(loss=matrix(c(0,5,1,0),ncol=2)), cp=0.01,minsplit=2) Expliquer les sorties des commandes suivantes. On pourra notamment calculer le dernier terme de la colonne rel error de la table cptable. tree2$parms $prior 1 2 0.59 0.41 $loss [,1] [,2] [1,] 0 1 [2,] 5 0 $split [1] 1 printcp(tree2) Classification tree: rpart(formula = High ~ ., data = data1, parms = list(loss = matrix(c(0, 5, 1, 0), ncol = 2)), cp = 0.01, minsplit = 2) Variables actually used in tree construction: [1] Advertising Age CompPrice Education [5] Income Population Price ShelveLoc Root node error: 236/400 = 0.59 n= 400 CP nsplit rel error xerror xstd 1 0.101695 0 1.00000 5.0000 0.20840 2 0.050847 2 0.79661 3.8136 0.20909 3 0.036017 3 0.74576 3.2034 0.20176 4 0.035311 5 0.67373 3.1271 0.20038 5 0.025424 9 0.50847 2.6144 0.19069 6 0.016949 11 0.45763 2.3475 0.18307 7 0.015537 16 0.37288 2.1992 0.17905 8 0.014831 21 0.28814 2.1992 0.17905 9 0.010593 23 0.25847 2.0466 0.17367 10 0.010000 25 0.23729 2.0297 0.17292 Comparer les valeurs ajustées par les deux arbres considérés. Références "],["SVM.html", "Chapitre 4 Support Vector Machine (SVM) 4.1 Cas séparable 4.2 Cas non séparable 4.3 L’astuce du noyau 4.4 Support vector régression 4.5 SVM sur les données spam 4.6 Exercices", " Chapitre 4 Support Vector Machine (SVM) Etant donnée un échantillon \\((x_1,y_1),\\dots,(x_n,y_n)\\) où les \\(x_i\\) sont à valeurs dans \\(\\mathbb R^p\\) et les \\(y_i\\) sont binaires à valeurs dans \\(\\{-1,1\\}\\), l’approche SVM cherche le meilleur hyperplan en terme de séparation des données. Globalement on veut que les 1 se trouvent d’un coté de l’hyperplan et les -1 de l’autre. Dans cette partie on propose d’étudier la mise en œuvre de cet algorithme tout d’abord dans le cas idéal où les données sont séparables puis dans le cas plus réel où elles ne le sont pas. Nous verrons ensuite comment introduire de la non linéarité ne utilisant l’astuce du noyau. 4.1 Cas séparable Le cas séparable est le cas facile : il correspond à la situation où il existe effectivement un (même plusieurs) hyperplan(s) qui sépare(nt) parfaitement les 1 des -1. Il ne se produit quasiment jamais en pratique mais il convient de l’étudier pour comprendre comment est construit l’algorithme. Dans ce cas on cherche l’hyperplan d’équation \\(\\langle w,x\\rangle+b=w^tx+b=0\\) tel que la marge (qui peut être vue comme la distance entre les observations les plus proches de l’hyperplan et l’hyperplan) soit maximale. Mathématiquement le problème se réécrit comme un problème d’optimisation sous contraintes : \\[\\begin{equation} \\min_{w,b}\\frac{1}{2}\\|w\\|^2 \\tag{4.1} \\end{equation}\\] \\[\\text{sous les contraintes } y_i(w^tx_i+b)\\geq 1,\\ i=1,\\dots,n.\\] La solution s’obtient de façon classique en résolvant le problème dual et elle s’écrit comme une combinaison linéaire des \\(x_i\\) \\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\] De plus, les conditions KKT impliquent que pour tout \\(i=1,\\dots,n\\): \\(\\alpha_i^\\star=0\\) ou \\(y_i(x_i^tw+b)-1=0.\\) Ces conditions impliquent que \\(w^\\star\\) s’écrit comme une combinaison linéaire de quelques points, appelés vecteurs supports qui se trouvent sur la marge. Nous proposons maintenant de retrouver ces points et de tracer la marge sur un exemple simple. On considère le nuage de points suivant : n &lt;- 20 set.seed(123) X1 &lt;- scale(runif(n)) set.seed(567) X2 &lt;- scale(runif(n)) Y &lt;- rep(-1,n) Y[X1&gt;X2] &lt;- 1 Y &lt;- as.factor(Y) donnees &lt;- data.frame(X1=X1,X2=X2,Y=Y) p &lt;- ggplot(donnees)+aes(x=X2,y=X1,color=Y)+geom_point() p La fonction svm du package e1071 permet d’ajuster une SVM : library(e1071) mod.svm &lt;- svm(Y~.,data=donnees,kernel=&quot;linear&quot;,cost=10000000000) Récupérer les vecteurs supports et visualiser les sur le graphe (en utilisant une autre couleur par exemple). On les affectera à un data.frame dont les 2 premières colonnes représenteront les valeurs de \\(X_1\\) et \\(X_2\\) des vecteurs supports. ind.svm &lt;- mod.svm$index sv &lt;- donnees %&gt;% slice(ind.svm) ... Retrouver ce graphe à l’aide de la fonction plot. Rappeler la règle de décision associée à la méthode SVM. Donner les estimations des paramètres de la règle de décision sur cet exemple. On pourra notamment regarder la sortie coef de la fonction svm. On dispose d’un nouvel individu \\(x=(-0.5,0.5)\\). Expliquer comment on peut prédire son groupe. Retrouver les résultats de la question précédente à l’aide de la fonction predict. On pourra utiliser l’option decision.values = TRUE. Obtenir les probabilités prédites à l’aide de la fonction predict. On pourra utiliser probability=TRUE dans la fonction svm. 4.2 Cas non séparable Dans la vraie vie, les groupes ne sont généralement pas séparables et il n’existe donc pas de solution au problème (4.1). On va donc autoriser certains points à être : mal classés et/ou bien classés mais à l’intérieur de la marge. Mathématiquement, cela revient à introduire des variables ressorts (slacks variables) \\(\\xi_1,\\dots,\\xi_n\\) positives telles que : \\(\\xi_i\\in [0,1]\\Longrightarrow\\) \\(i\\) bien classé mais dans la région définie par la marge ; \\(\\xi_i&gt;1 \\Longrightarrow\\) \\(i\\) mal classé. Le problème d’optimisation est alors de minimiser en \\((w,b,\\xi)\\) \\[\\frac{1}{2}\\|w\\|^2 +C\\sum_{i=1}^n\\xi_i\\] \\[\\textrm{sous les contraintes } \\left\\{ \\begin{array}{l} y_i(w^tx_i+b)\\geq 1 -\\xi_i \\\\ \\xi_i\\geq 0, i=1,\\dots,n. \\end{array}\\right.\\] Le paramètre \\(C&gt;0\\) est à calibrer et on remarque que le cas séparable correspond à \\(C\\to +\\infty\\). Les solutions de ce nouveau problème d’optimisation s’obtiennent de la même façon que dans le cas séparable, en particulier \\(w^\\star\\) s’écrite toujours comme une combinaison linéaire \\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\] de vecteurs supports sauf qu’on distingue deux types de vecteurs supports (\\(\\alpha_i^\\star&gt;0\\)): ceux sur la frontière définie par la marge : \\(\\xi_i^\\star=0\\) ; ceux en dehors : \\(\\xi_i^\\star&gt;0\\) et \\(\\alpha_i^\\star=C\\). Le choix de \\(C\\) est crucial : ce paramètre régule le compromis biais/variance de la svm : \\(C\\searrow\\): la marge est privilégiée et les \\(\\xi_i\\nearrow\\) \\(\\Longrightarrow\\) beaucoup d’observations dans la marge ou mal classées (et donc beaucoup de vecteurs supports). \\(C\\nearrow\\Longrightarrow\\) \\(\\xi_i\\searrow\\) donc moins d’observations mal classées \\(\\Longrightarrow\\) meilleur ajustement mais petite marge \\(\\Longrightarrow\\) risque de surajustement. On choisit généralement ce paramètre à l’aide des techniques présentées dans le chapitre 1 : choix d’une grille de valeurs de \\(C\\) et d’un critère ; choix d’une méthode de ré-échantillonnage pour estimer le critère ; choix de la valeur de \\(C\\) qui minimise le critère estimé. On considère le jeu de données df3 définie ci-dessous. n &lt;- 1000 set.seed(1234) df &lt;- as.data.frame(matrix(runif(2*n),ncol=2)) df1 &lt;- df %&gt;% filter(V1&lt;=V2)%&gt;% mutate(Y=rbinom(nrow(.),1,0.95)) df2 &lt;- df %&gt;% filter(V1&gt;V2)%&gt;% mutate(Y=rbinom(nrow(.),1,0.05)) df3 &lt;- bind_rows(df1,df2) %&gt;% mutate(Y=as.factor(Y)) ggplot(df3)+aes(x=V2,y=V1,color=Y)+geom_point()+ scale_color_manual(values=c(&quot;#FFFFC8&quot;, &quot;#7D0025&quot;))+ theme(panel.background = element_rect(fill = &quot;#BFD5E3&quot;, colour = &quot;#6D9EC1&quot;,size = 2, linetype = &quot;solid&quot;), panel.grid.major = element_blank(), panel.grid.minor = element_blank()) Ajuster 3 svm en considérant comme valeur de \\(C\\) : 0.000001, 0.1 et 5. On pourra utiliser l’option cost. mod.svm1 &lt;- svm(Y~.,data=df3,kernel=&quot;linear&quot;,...) mod.svm2 &lt;- svm(Y~.,data=df3,kernel=&quot;linear&quot;,...) mod.svm3 &lt;- svm(Y~.,data=df3,kernel=&quot;linear&quot;,...) Calculer les nombres de vecteurs supports pour chaque valeur de \\(C\\). Visualiser les 3 svm obtenues. Interpréter. 4.3 L’astuce du noyau Les SVM présentées précédemment font l’hypothèse que les groupes sont linéairement séparables, ce qui n’est bien entendu pas toujours le cas en pratique. L’astuce du noyau permet de mettre de la non linéarité, elle consiste à : plonger les données dans un nouvel espace appelé espace de représentation ou feature space ; appliquer une svm linéaire dans ce nouvel espace. Le terme astuce vient du fait que ce procédé ne nécessite pas de connaître explicitement ce nouvel espace : pour résoudre le problème d’optimisation dans le feature space on a juste besoin de connaître le noyau associé au feature space. D’un point de vu formel un noyau est une fonction \\[K:\\mathcal X\\times\\mathcal X\\to\\mathbb R\\] dont les propriétés sont proches d’un produit scalaire. Il existe donc tout un tas de noyau avec lesquels on peut faire des SVM, par exemple Linéaire (sur \\(\\mathbb R^d\\)) : \\(K(x,x&#39;)=x^tx&#39;\\). Polynomial (sur \\(\\mathbb R^d\\)) : \\(K(x,x&#39;)=(x^tx&#39;+1)^d\\). Gaussien (Gaussian radial basis function ou RBF) (sur \\(\\mathbb R^d\\)) \\[K(x,x&#39;)=\\exp\\left(-\\frac{\\|x-x&#39;\\|}{2\\sigma^2}\\right).\\] Laplace (sur \\(\\mathbb R\\)) : \\(K(x,x&#39;)=\\exp(-\\gamma|x-x&#39;|)\\). Noyau min (sur \\(\\mathbb R^+\\)) : \\(K(x,x&#39;)=\\min(x,x&#39;)\\). … Bien entendu, en pratique tout le problème va consister à trouver le bon noyau ! On considère le jeu de données suivant où le problème est d’expliquer \\(Y\\) par \\(V1\\) et \\(V2\\). n &lt;- 500 set.seed(13) X &lt;- matrix(runif(n*2,-2,2),ncol=2) %&gt;% as.data.frame() Y &lt;- rep(0,n) cond &lt;- (X$V1^2+X$V2^2)&lt;=2.8 Y[cond] &lt;- rbinom(sum(cond),1,0.9) Y[!cond] &lt;- rbinom(sum(!cond),1,0.1) df &lt;- X %&gt;% mutate(Y=as.factor(Y)) ggplot(df)+aes(x=V2,y=V1,color=Y)+geom_point()+theme_classic() Ajuster une svm linéaire et visualiser l’hyperplan séparateur. Que remarquez-vous ? Exécuter la commande suivante et commenter la sortie. mod.svm1 &lt;- svm(Y~.,data=df,kernel=&quot;radial&quot;,gamma=1,cost=1) plot(mod.svm1,df,grid=250) Faire varier les paramètres gamma et cost. Interpréter (on pourra notamment étudier l’évolution du nombre de vecteurs supports en fonction du paramètre cost). mod.svm2 &lt;- svm(Y~.,data=df,kernel=&quot;radial&quot;,gamma=...,cost=...) mod.svm3 &lt;- svm(Y~.,data=df,kernel=&quot;radial&quot;,gamma=...,cost=...) mod.svm4 &lt;- svm(Y~.,data=df,kernel=&quot;radial&quot;,gamma=...,cost=...) plot(mod.svm2,df,grid=250) plot(mod.svm3,df,grid=250) plot(mod.svm4,df,grid=250) mod.svm2$nSV mod.svm3$nSV mod.svm4$nSV Sélectionner automatiquement ces paramètres. On pourra utiliser la fonction tune en faisant varier C dans c(0.1,1,10,100,1000) et gamma dans c(0.5,1,2,3,4). tune.out &lt;- tune(svm,Y~.,data=...,kernel=&quot;...&quot;, ranges=list(cost=...,gamma=...)) Faire de même avec caret, on utilisera method=“svmRadial” et prob.model=TRUE. C &lt;- c(0.001,0.01,1,10,100,1000) sigma &lt;- c(0.5,1,2,3,4) gr &lt;- expand.grid(C=C,sigma=sigma) ctrl &lt;- trainControl(...) res.caret1 &lt;- train(...,prob.model=TRUE) res.caret1 Visualiser la règle sélectionnée. 4.4 Support vector régression Dans un contexte de régression (lorsque \\(y_i\\in\\mathbb R\\)), on ne recherche plus la l’hyperplan qui va séparer au mieux. On va dans ce cas là cherche à approcher au mieux les valeurs de \\(y_i\\). Cela revient à chercher \\(w\\in\\mathbb R^p\\) et \\(b\\in\\mathbb R\\) tels que \\[|\\langle w,x_i\\rangle+b-y_i|\\leq \\varepsilon\\] avec \\(\\varepsilon&gt;0\\) petit à choisir par l’utilisateur. Par analogie avec la SVM binaire, on va ainsi chercher \\((w,b)\\) qui minimisent \\[\\frac{1}{2}\\|w\\|^2\\] \\[\\textrm{sous les contraintes } |y_i-\\langle w,x_i\\rangle-b|\\leq \\varepsilon,\\ i=1,\\dots,n,\\] Les contraintes impliquent que toute les observations doivent se définir dans une marge ou bande de taille \\(2\\varepsilon\\). Cette hypothèse peut amener l’utilisateur à utiliser des valeurs de \\(\\varepsilon\\) très grandes et empêcher la solution de bien ajuster le nuage de points. Pour pallier à cela, on introduit, comme dans le cas de la SVM binaire, des variables ressorts qui vont autoriser certaines observations à se situer en dehors de la marge. Le problème revient alors à trouver \\((w,b,\\xi,\\xi^\\star)\\) qui minimise \\[\\frac{1}{2}\\|w\\|^2+C\\sum_{i=1}^n(\\xi_i+\\xi_i^\\star)\\] \\[\\textrm{sous les contraintes } \\left\\{ \\begin{array}{l} y_i-\\langle w,x_i\\rangle-b\\leq \\varepsilon+\\xi_i,\\ i=1,\\dots,n,\\\\ \\langle w,x_i\\rangle+b-y_i\\leq \\varepsilon+\\xi_i^\\star,\\ i=1,\\dots,n \\\\ \\xi_i\\geq 0,\\xi_i^\\star\\geq 0,\\ i=1,\\dots,n \\end{array}\\right. \\] Les solutions s’obtiennent exactement de la même façon que dans le cas binaire. On montre notamment que \\(w^\\star\\) s’écrit comme une combinaison linéaire de vecteurs supports : \\[w^\\star=\\sum_{i=1}^n(\\alpha_i^\\star-\\alpha_i)x_i.\\] Les vecteurs supports sont les observations vérifiant \\(\\alpha_i^\\star-\\alpha_i\\neq 0\\). Ici encore il faudra calibrer le paramètre \\(C\\) et on pourra utiliser l’astuce du noyau. On considère le nuage de points \\((x_i,y_i),i=1,\\dots,n\\) définie ci-dessous: set.seed(321) n &lt;- 30 X &lt;- runif(n) eps &lt;- rnorm(n,0,0.2) Y &lt;- 1+X+eps df &lt;- data.frame(X,Y) p1 &lt;- ggplot(df)+aes(x=X,y=Y)+geom_point() p1 On souhaite faire une SVR permettant de prédire \\(Y\\) par \\(X\\). On peut l’obtenir sur R toujours avec la fonction svm de e1071: svr1 &lt;- svm(Y~.,data=df,kernel=&quot;linear&quot;,epsilon=0.5,cost=100,scale=FALSE) On choisit ici exceptionnellement de ne pas réduire les \\(X\\). Écrire une fonction R qui, à partir d’un objet svm, calcule l’équation de la droite de la SVR. Cette fonction pourra également tracer cette doite ainsi que la marge. Comparer la SVR précédente avec celle utilisant epsilon=0.7. On ajoute le point de coordonnées \\((0.05,3)\\) aux données. Discuter de la SVR pour ce nouveau jeu de données en utilisant plusieurs valeurs pour C et epsilon. df1 &lt;- df %&gt;% bind_rows(data.frame(X=0.05,Y=3)) 4.5 SVM sur les données spam On considère le jeu de données spam où le problème est d’expliquer la variable type par les autres. data(spam) summary(spam$type) nonspam spam 2788 1813 On veut comparer plusieurs svm en utilisant le package kernlab. On pourra trouver un descriptif du package à cette adresse https://www.jstatsoft.org/article/view/v011i09. Utiliser la fonction ksvm pour faire une svm linéaire et une svm à noyau gaussien. On prendra comme paramètre 1 pour C et pour le paramètre du noyau gaussien. Évaluer la performance des 2 svm précédentes en calculant l’erreur de classification par validation croisée 5 blocs. Comparer ces deux algorithmes. Refaire la svm à noyau gaussien avec l’option kpar='automatic'. Expliquer. On s’intéresse maintenant à l’AUC. À partir de validation croisée, sélectionner un noyau (linéaire ou gaussien) ainsi que des valeurs de paramètres associés au noyau, sans oublier le paramètre C. On pourra utiliser le package caret et comparer le résultat obtenu à celui d’une forêt aléatoire. 4.6 Exercices Exercice 4.1 (Résolution du problème d’optimisation dans le cas séparable) On considère \\(n\\) observations \\((x_1,y_1),\\dots,(x_n,y_n)\\) telles que \\((x_i,y_i)\\in\\mathbb R^p\\times\\{-1,1\\}\\). On cherche à expliquer la variable \\(Y\\) par \\(X\\). On considère l’algorithme SVM et on se place dans le cas où les données sont séparables. Soit \\(\\mathcal H\\) un hyperplan séparateur d’équation \\(\\langle w,x\\rangle+b=0\\) où \\(w\\in\\mathbb R^p,b\\in\\mathbb R\\). Exprimer la distance entre \\(x_i,i=1,\\dots,n\\) et \\(\\mathcal H\\) en fonction de \\(w\\) et \\(b\\). Expliquer la logique du problème d’optimisation \\[\\max_{w,b,\\|w\\|=1}M\\] \\[\\textrm{sous les contraintes } y_i(\\langle w,x_i\\rangle+b)\\geq M,\\ i=1,\\dots,n.\\] Montrer que ce problème peut se réécrire \\[\\min_{w,b}\\frac{1}{2}\\|w\\|^2\\] \\[\\textrm{sous les contraintes } y_i(\\langle w,x_i\\rangle+b)\\geq 1,\\ i=1,\\dots,n.\\] On rappelle que pour la minimisation d’une fonction \\(h:\\mathbb R^p\\to\\mathbb R\\) sous contraintes affines \\(g_i(u)\\geq 0,i=1,\\dots,n\\), le Lagrangien s’écrit \\[L(u,\\alpha)=h(u)-\\sum_{i=1}^n\\alpha_ig_i(u).\\] Si on désigne par \\(u_\\alpha=\\mathop{\\mathrm{argmin}}_uL(u,\\alpha)\\), la fonction duale est alors donnée par \\[\\theta(\\alpha)=L(u_\\alpha,\\alpha)=\\min_{u\\in\\mathbb R^p}L(u,\\alpha),\\] et le problème dual consiste à maximiser \\(\\theta(\\alpha)\\) sous les contraintes \\(\\alpha_i\\geq 0\\). En désignant par \\(\\alpha^\\star\\) la solution de ce problème, on déduit la solution du problème primal \\(u^\\star=u_{\\alpha^\\star}\\). Les conditions de Karush-Kuhn-Tucker sont données par \\(\\alpha_i^\\star\\geq 0\\). \\(g_i(u_{\\alpha^\\star})\\geq 0\\). \\(\\alpha_i^\\star g_i(u_{\\alpha^\\star})=0\\). Écrire le Lagrangien du problème considéré et en déduire une expression de \\(w\\) en fonction des \\(\\alpha_i\\) et des observations. Écrire la fonction duale. Écrire les conditions KKT et en déduire les solutions \\(w^\\star\\) et \\(b^\\star\\). Interpréter les conditions KKT. Exercice 4.2 (Règle svm à partir de sorties R) On considère \\(n\\) observations \\((x_1,y_1),\\dots,(x_n,y_n)\\) telles que \\((x_i,y_i)\\in\\mathbb R^3\\times\\{-1,1\\}\\). On cherche à expliquer la variable \\(Y\\) par \\(X=(X_1,X_2,X_3)\\). On considère l’algorithme SVM et on se place dans le cas où les données sont séparables. On rappelle que cet algorithme consiste à chercher une droite d’équation \\(w^tx+b=0\\) où \\((w,b)\\in\\mathbb R^3\\times\\mathbb R\\) sont solutions du problème d’optimisation (problème primal) \\[\\min_{w,b}\\frac{1}{2}\\|w\\|^2\\] \\[\\textrm{sous les contraintes } y_i(w^tx_i+b)\\geq 1,\\ i=1,\\dots,n.\\] On désigne par \\(\\alpha_i^\\star,i=1,\\dots,n\\), les solutions du problème dual et par \\((w^\\star,b^\\star)\\) les solutions du problème ci-dessus. Donner la formule permettant de calculer \\(w^\\star\\) en fonction des \\(\\alpha_i^\\star\\). Expliquer comment on classe un nouveau point \\(x\\in\\mathbb R^3\\) par la méthode svm. Les données se trouvent dans un dataframe df. On exécute set.seed(1234) n &lt;- 100 X &lt;- data.frame(X1=runif(n),X2=runif(n),X3=runif(n)) X &lt;- data.frame(X1=scale(runif(n)),X2=scale(runif(n)),X3=scale(runif(n))) Y &lt;- rep(-1,100) Y[X[,1]&lt;X[,2]] &lt;- 1 #Y &lt;- (apply(X,1,sum)&lt;=0) %&gt;% as.numeric() %&gt;% as.factor() df &lt;- data.frame(X,Y=as.factor(Y)) mod.svm &lt;- svm(Y~.,data=df,kernel=&quot;linear&quot;,cost=10000000000) et on obtient df[mod.svm$index,] X1 X2 X3 Y 51 -1.1 -1.0 -1.0 1 92 0.7 0.8 1.1 1 31 0.7 0.5 -1.0 -1 37 -0.5 -0.6 0.3 -1 mod.svm$coefs [,1] [1,] 59 [2,] 49 [3,] -30 [4,] -79 mod.svm$rho [1] -0.5 Calculer les valeurs de \\(w^\\star\\) et \\(b^\\star\\). En déduire la règle de classification. On dispose d’une nouvelle observation \\(x=(1,-0.5,-1)\\). Dans quel groupe (-1 ou 1) l’algorithme affecte cette nouvelle donnée ? "],["agregation.html", "Chapitre 5 Agrégation : forêts aléatoires et gradient boosting 5.1 Forêts aléatoires 5.2 Gradient boosting", " Chapitre 5 Agrégation : forêts aléatoires et gradient boosting Les méthodes par arbres présentées précédemment sont des algorithmes qui possèdent tout un tas de qualités (facile à mettre en œuvre, interprétable…). Ce sont néanmoins rarement les algorithmes qui se révèlent les plus performants. Les méthodes d’agrégation d’arbres présentées dans cette partie sont souvent beaucoup plus pertinentes, notamment en terme de qualité de prédiction. Elles consistent à construire un très grand nombre d’arbres “simples” : \\(g_1,\\dots,g_B\\) et à les agréger en faisant la moyenne : \\[\\frac{1}{B}\\sum_{k=1}^Bg_k(x).\\] Les forêts aléatoires (Breiman 2001) et le gradient boosting (Friedman 2001) utilisent ce procédé d’agrégation. 5.1 Forêts aléatoires L’algorithme des forêts aléatoires consiste à construire des arbres sur des échantillons bootstrap et à les agréger. Il peut s’écrire de la façon suivante : Entrées : \\(x\\in\\mathbb R^d\\) l’observation à prévoir, \\(\\mathcal D_n\\) l’échantillon ; \\(B\\) nombre d’arbres ; \\(n_{max}\\) nombre max d’observations par nœud \\(m\\in\\{1,\\dots,d\\}\\) le nombre de variables candidates pour découper un nœud. Algorithme : pour \\(k=1,\\dots,B\\) : Tirer un échantillon bootstrap dans \\(\\mathcal D_n\\) Construire un arbre CART sur cet échantillon bootstrap, chaque coupure est sélectionnée en minimisant la fonction de coût de CART sur un ensemble de \\(m\\) variables choisies au hasard parmi les \\(d\\). On note \\(T(.,\\theta_k,\\mathcal D_n)\\) l’arbre construit. Sortie : l’estimateur \\(T_B(x)=\\frac{1}{B}\\sum_{k=1}^BT(x,\\theta_k,\\mathcal D_n)\\). Cet algorithme peut être utilisé sur R avec la fonction randomForest du package randomForest. Nous la présentons à travers l’exemple du jeu de données spam du package kernlab. library(kernlab) data(spam) set.seed(1234) spam &lt;- spam[sample(nrow(spam)),] Le problème est d’expliquer la variable binaire type par les autres. A l’aide de la fonction randomForest du package randomForest, ajuster une forêt aléatoire pour répondre au problème posé. Appliquer la fonction plot à l’objet construit avec randomForest et expliquer le graphe obtenu. A quoi peut servir ce graphe en pratique ? Construire la forêt avec mtry=1 et comparer ses performances avec celle construite précédemment. Utiliser la fonction train du package caret pour choisir le paramètre mtry dans la grille seq(1,30,by=5). Construire la forêt avec le paramètre mtry sélectionné. Calculer l’importance des variables et représenter ces importance à l’aide d’un diagramme en barres. La fonction ranger du package ranger permet également de calculer des forêts aléatoires. Comparer les temps de calcul de cette fonction avec randomForest 5.2 Gradient boosting Les algorithmes de gradient boosting permettent de minimiser des pertes empiriques de la forme \\[\\frac{1}{n}\\sum_{i=1}^n\\ell(y_i,f(x_i)).\\] où \\(\\ell:\\mathbb R\\times\\mathbb R\\to\\mathbb R\\) est une fonction de coût convexe en son second argument. Il existe plusieurs type d’algorithmes boosting. Un des plus connus et utilisés a été proposé par Friedman (2001), c’est la version que nous étudions dans cette partie. Cette approche propose de chercher la meilleure combinaison linéaire d’arbres binaires, c’est-à-dire que l’on recherche \\(g(x)=\\sum_{m=1}^M\\alpha_mh_m(x)\\) qui minimise \\[\\mathcal R_n(g)=\\frac{1}{n}\\sum_{i=1}^n\\ell(y_i,g(x_i)).\\] Optimiser sur toutes les combinaisons d’arbres binaires se révélant souvent trop compliqué, Friedman (2001) utilise une descente de gradient pour construire la combinaison d’abres de façon récursive. L’algorithme est le suivant : Entrées : \\(d_n=(x_1,y_1),\\dots,(x_n,y_n)\\) l’échantillon, \\(\\lambda\\) un paramètre de régularisation tel que \\(0&lt;\\lambda\\leq 1\\). \\(M\\in\\mathbb N\\) le nombre d’itérations. paramètres de l’arbre (nombre de coupures…) Itérations : Initialisation : \\(g_0(.)=\\mathop{\\mathrm{argmin}}_c \\frac{1}{n}\\sum_{i=1}^n \\ell(y_i,c)\\) Pour \\(m=1\\) à \\(M\\) : Calculer l’opposé du gradient \\(-\\frac{\\partial}{\\partial g(x_i)}\\ell(y_i,g(x_i))\\) et l’évaluer aux points \\(g_{m-1}(x_i)\\) : \\[U_i=-\\frac{\\partial}{\\partial g(x_i)}\\ell(y_i,g(x_i)) _{\\Big |g(x_i)=g_{m-1}(x_i)},\\quad i=1,\\dots,n.\\] Ajuster un arbre sur l’échantillon \\((x_1,U_1),\\dots,(x_n,U_n)\\), on le note \\(h_m\\). Mise à jour : \\(g_m(x)=g_{m-1}(x)+\\lambda h_m(x)\\). Sortie : la suite \\((g_m(x))_m\\). Sur R On peut utiliser différents packages pour faire du gradient boosting. Nous utilisons ici le package gbm (Ridgeway 2006). 5.2.1 Un exemple simple en régression On considère un jeu de données \\((x_i,y_i),i=1,\\dots,200\\) issu d’un modèle de régression \\[y_i=m(x_i)+\\varepsilon_i\\] où la vraie fonction de régression est la fonction sinus (mais on va faire comme si on ne le savait pas). x &lt;- seq(-2*pi,2*pi,by=0.01) y &lt;- sin(x) set.seed(1234) X &lt;- runif(200,-2*pi,2*pi) Y &lt;- sin(X)+rnorm(200,sd=0.2) df1 &lt;- data.frame(X,Y) df2 &lt;- data.frame(X=x,Y=y) p1 &lt;- ggplot(df1)+aes(x=X,y=Y)+geom_point()+geom_line(data=df2,size=1)+xlab(&quot;&quot;)+ylab(&quot;&quot;) p1 Rappeler ce que siginifie le \\(L_2\\)-boosting. A l’aide de la fonction gbm du package gbm construire un algorithme de \\(L_2\\)-boosting. On utilisera 500000 itérations et gardera les autres valeurs par défaut de paramètres. Visualiser l’estimateur à la première itération. On pourra faire un predict avec l’option n.trees. Faire de même pour les itérations 1000 et 500000. Sélectionner le nombre d’itérations par la procédure de votre choix. 5.2.2 Adaboost et logitboost pour la classification binaire. On considère le jeu de données spam du package kernlab. library(kernlab) data(spam) set.seed(1234) spam &lt;- spam[sample(nrow(spam)),] Exécuter la commande model_ada1 &lt;- gbm(type~.,data=spam,distribution=&quot;adaboost&quot;,interaction.depth=2, shrinkage=0.05,n.trees=500) Proposer une correction permettant de faire fonctionner l’algorithme. Expliciter le modèle ajusté par la commande précédente. Effectuer un summary du modèle ajusté. Expliquer la sortie. Utiliser la fonction vip du package vip pour retrouver ce sorties. Sélectionner le nombre d’itérations pour l’algorithme adaboost en faisant de la validation croisée 5 blocs. Faire la même procédure en changeant la valeur du paramètre shrinkage. Interpréter. Expliquer la différence entre adaboost et logitboost et précisez comment on peut mettre en œuvre ce dernier algorithme. 5.2.3 Exercices Rappeler la fonction de risque adaboost. Montrer que le risque est minimum en \\[f^\\star(x)=\\frac{1}{2}\\log\\frac{\\mathbf P(Y=1|X=x)}{\\mathbf P(Y=-1|X=x)}.\\] Mêmes questions pour le risque logitboost. Références "],["deep.html", "Chapitre 6 Réseaux de neurones avec Keras", " Chapitre 6 Réseaux de neurones avec Keras Nous présentons ici une introduction au réseau de neurones à l’aide du package keras. On pourra trouver une documentation complète ainsi qu’un très bon tutoriel aux adresses suivantes https://keras.rstudio.com et https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/. On commence par charger la librairie library(keras) #install_keras() 1 seule fois sur la machine On va utiliser des réseaux de neurones pour le jeu de données spam où le problème est d’expliquer la variable binaires typepar les 57 autres variables du jeu de données : library(kernlab) data(spam) spamX &lt;- as.matrix(spam[,-58]) #spamY &lt;- to_categorical(as.numeric(spam$type)-1, 2) spamY &lt;- as.numeric(spam$type)-1 On sépare les données en un échantillon d’apprentissage et un échantillon test set.seed(5678) perm &lt;- sample(4601,3000) appX &lt;- spamX[perm,] appY &lt;- spamY[perm] validX &lt;- spamX[-perm,] validY &lt;- spamY[-perm] A l’aide des données d’apprentissage, entrainer un perceptron simple avec une fonction d’activation sigmoïde. On utilisera 30 epochs et des batchs de taille 5. #Définition du modèle percep.sig &lt;- keras_model_sequential() percep.sig %&gt;% layer_dense(units=...,input_shape = ...,activation=&quot;...&quot;) summary(percep.sig) percep.sig %&gt;% compile( loss=&quot;binary_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=&quot;accuracy&quot; ) #Entrainement p.sig &lt;- percep.sig %&gt;% fit( x=..., y=..., epochs=..., batch_size=..., validation_split=..., verbose=0 ) Faire de même avec la fonction d’activation softmax. On utilisera pour cela 2 neurones avec une sortie \\(Y\\) possédant la forme suivante. spamY1 &lt;- to_categorical(as.numeric(spam$type)-1, 2) appY1 &lt;- spamY1[perm,] validY1 &lt;- spamY1[-perm,] Comparer les performances des deux perceptrons sur les données de validation à l’aide de la fonction evaluate. Construire un ou deux réseaux avec deux couches cachées. On pourra faire varier les nombre de neurones dans ces couches. Comparer les performances des réseaux construits. "],["dondes.html", "Chapitre 7 Données déséquilibrées 7.1 Critères de performance pour données déséquilibrées 7.2 Ré-équilibrage 7.3 Exercices supplémentaires", " Chapitre 7 Données déséquilibrées On parle de données déséquilibrées lorsque les deux modalités de la variable cible \\(Y\\) ne sont pas représentées de façon égale dans l’échantillon, ou plus précisément lorsqu’une des deux modalités est fortement majoritaire. Ce contexte est fréquemment rencontré en pratique, on peut citer les cas de détection de fraudes (peu de fraudeurs), de la présence d’une maladie rare (peu de patients atteints), du risque de crédit (peu de mauvais payeurs)… Les algorithmes standards peuvent être mis en difficultés et de nouvelles stratégies doivent être élaborées. Les stratégies classiques permettant de répondre à ce problème consistent à utiliser des critères de performance adaptés au déséquilibre ; ré-échantillonner les données pour se rapprocher d’une situation d’équilibre. Nous présentons ces stratégies à travers quelques exercices. 7.1 Critères de performance pour données déséquilibrées La notion de risque en machine learning est capitale puisque c’est à partir de l’estimation de ces risques que l’on calibre des algorithmes et que l’on choisit un algorithme de prévision. En présence de données déséquilibré, il convient de choisir un risque adapté. En effet, il est le plus souvent important de parvenir à bien identifier des individus de la classe minoritaire. Des critères tels que l’accuracy ou l’erreur de classification ne sont pas pertinents pour ce cadre. On va privilégier des critères comme le balanced accuracy \\[\\text{Bal Acc}=\\frac{1}{2}\\mathbf P(g(X)=1|Y=1)+\\frac{1}{2}\\mathbf P(g(X)=-1|Y=-1)=\\frac{\\text{TPR+TNR}}{2}.\\] le \\(F_1\\)-score \\[F_1=2\\,\\frac{\\text{Precision }\\times\\text{Recall}}{\\text{Precision }+\\text{Recall}},\\] avec \\[\\text{Precision}=\\mathbf P(Y=1|g(X)=1)\\quad\\text{et}\\quad\\text{Recall}=\\mathbf P(g(X)=1|Y=1).\\] le kappa de Cohen \\[\\kappa=\\frac{\\mathbf P(a)-\\mathbf P(e)}{1-\\mathbf P(e)}\\] où \\(\\mathbf P(a)\\) représente l’accuracy et \\(\\mathbf P(e)\\) l’accuracy sous une hypothèse d’indépendance. la courbe ROC et l’AUC… Comme d’habitude, ces critères sont inconnus et doivent être estimés par des méthodes de ré-échantillonnage de type validation croisée. Exercice 7.1 (Calculer des critères) Générer un vecteur d’observations Y de taille 500 selon une loi de Bernoulli de paramètre 0.05. Générer un vecteur de prévisions P1 de taille 500 selon une loi de Bernoulli de paramètre 0.01. Générer un vecteur de prévision P2 de taille 500 tel que \\[\\mathcal L(P2|Y=0)=\\mathcal B(0.10)\\quad\\text{et}\\quad \\mathcal L(P2|Y=1)=\\mathcal B(0.85).\\] Dresser les tables de contingence de P1 et P2 à l’aide de table. Commenter. Pour P2, calculer, avec les fonctions usuelles de R, l’accuracy, le recall et la précision. En déduire le F1-score. Même question pour le \\(\\kappa\\) de Cohen. Retrouver ces indicateurs à l’aide de la fonction confusionMatrix de caret puis comparer les prévisions P1 et P2. 7.2 Ré-équilibrage En complément du choix d’un critère pertinent, il peut être intéressant de tenter de ré-équilibrer l’échantillon pour aider les algorithmes à mieux détecter les individus de la classe minoritaire. Les méthodes classiques consistent à créer de nouvelles observations de la classe minoritaire (oversampling) et/ou supprimer des individus de la classe minoritaire (undersampling). Exercice 7.2 (Quelques algorithmes de ré-équilibrage) On considère le jeu de données df ci-dessous où on cherche à prédire Y par X1 et X2. n &lt;- 2000 set.seed(1234) X1 &lt;- runif(n) set.seed(5678) X2 &lt;- runif(n) set.seed(9012) R1 &lt;- X1&lt;=0.25 R2 &lt;- (X1&gt;0.25 &amp; X2&gt;=0.75) R3 &lt;- (X1&gt;0.25 &amp; X2&lt;0.75) Y &lt;- rep(0,n) Y[R1] &lt;- rbinom(sum(R1),1,0.75) Y[R2] &lt;- rbinom(sum(R2),1,0.75) Y[R3] &lt;- rbinom(sum(R3),1,0.25) df1 &lt;- data.frame(X1,X2,Y) df1$Y &lt;- factor(df1$Y) indDY1 &lt;- which(df1$Y==1) df1.1 &lt;- df1[-indDY1[1:650],] df1.2 &lt;- df1.1[sample(nrow(df1.1),1000),] df &lt;- df1.2[sample(nrow(df1.2),100),] rownames(df) &lt;- NULL p1 &lt;- ggplot(df)+aes(x=X1,y=X2,color=Y)+geom_point() p1 On a ici 4 fois plus d’observations dans le groupe 0. summary(df$Y) 0 1 80 20 On commence par faire du oversampling avec la fonction RandOverClassif. Effectuer le ré-échantillonnage et expliquer. Corriger les paramètres de la fonction de manière à avoir 80 observations dans le groupe 0 et 60 dans le groupe 1. On s’intéresse maintenant à l’algorithme SMOTE Exécuter la fonction SmoteClassif avec k=3 et les les paramètres par défaut Visualiser les observations smote. Corriger les paramètres de la fonction de manière à avoir 80 observations dans le groupe 0 et 60 dans le groupe 1. On souhaite maintenant ré-équilibrer par random undersampling. Utiliser la fonction RandUnderClassif pour effectuer un tel ré-équilibrage. Ici encore on pourra faire varier les paramètres. On passe maintenant à l’algorithme Tomek. Sans utiliser la fonction TomekClassif identifier les paires d’observations qui ont un lien de Tomek. On pourra utiliser la fonction nng du package cccd. Retrouver ces paires à l’aide de la fonction Tomek LinK. Visualiser les observations supprimées. On prendra soin d’expliquer l’option rem de TomekClassif. Exercice 7.3 (Comparaison de méthodes de ré-équilibrage) On considère 3 jeux de données df1, df2 et df3. n &lt;- 2000 set.seed(12345) X1 &lt;- runif(n) set.seed(5678) X2 &lt;- runif(n) set.seed(9012) R1 &lt;- X1&lt;=0.25 R2 &lt;- (X1&gt;0.25 &amp; X2&gt;=0.75) R3 &lt;- (X1&gt;0.25 &amp; X2&lt;0.75) Y &lt;- rep(0,n) Y[R1] &lt;- rbinom(sum(R1),1,0.75) Y[R2] &lt;- rbinom(sum(R2),1,0.75) Y[R3] &lt;- rbinom(sum(R3),1,0.25) df1 &lt;- data.frame(X1,X2,Y) df1$Y &lt;- factor(df1$Y) indDY1 &lt;- which(df1$Y==1) df2 &lt;- df1[-indDY1[1:400],] df3 &lt;- df1[-indDY1[1:700],] df1 &lt;- df1[sample(nrow(df1),1000),] df2 &lt;- df2[sample(nrow(df2),1000),] df3 &lt;- df3[sample(nrow(df3),1000),] Comparer la distribution de Y pour ces trois jeux de données et visualiser les observations. On sépare ces 3 échantillons en un échantillon d’apprentissage et un échantillon test. set.seed(123) a1 &lt;- createDataPartition(1:nrow(df1),p=2/3) a2 &lt;- createDataPartition(1:nrow(df2),p=2/3) a3 &lt;- createDataPartition(1:nrow(df3),p=2/3) train1 &lt;- df1[a1$Resample1,] train2 &lt;- df2[a2$Resample1,] train3 &lt;- df3[a3$Resample1,] test1 &lt;- df1[-a1$Resample1,] test2 &lt;- df2[-a2$Resample1,] test3 &lt;- df3[-a3$Resample1,] Ajuster une forêt aléatoire sur les 3 échantillon d’apprentissage, calculer les labels prédits sur les échantillons tests et estimer les différents indicateurs vus en cours à l’aide de confusionMatrix. On considère uniquement l’échantillon df3. Refaire l’analyse précédente en utilisant des techniques de ré-échantillonnage. 7.3 Exercices supplémentaires Exercice 7.4 (Echantillonnage rétrospectif) Dans le cadre de l’échantillonnage rétrospectif pour le modèle logistique vu en cours, démontrer la propriété qui lie le modèle logistique initial au modèle ré-équilibré. Exercice 7.5 (Echantillonnage rétrospectif) Une étude cas/témoins est réalisée pour mesurer l’effet du tabac sur une pathologie. Pour ce faire, on choisit \\(n_1=250\\) patients atteints de la pathologie (cas) et \\(n_0=250\\) patients sains (témoins). Les résultats de l’étude sont présentés ci-dessous Fumeur Non fumeur Non malade 48 202 Malade 208 42 A partir des données obtenues, estimer à l’aide d’un modèle logistique la probabilité d’être atteint pour un fumeur, puis pour un non fumeur. Comment interpréter ces deux probabilités ? Est-ce qu’elles estiment la probabilité d’être atteint pour un individu quelconque dans la population ? Des études précédentes ont montré que cinq individus sur mille sont atteints par la pathologie dans la population entière. En utilisant la propriété de l’exercice précédent, en déduire les probabilités d’être atteint pour un fumeur et un non fumeur dans la population. "],["comp-algo.html", "Chapitre 8 Comparaison d’algorithmes", " Chapitre 8 Comparaison d’algorithmes Les chapitres précédents ont présenté plusieurs algorithmes permettant de répondre à un problème posé, le plus souvent de classification supervisée. Se pose bien entendu la question de choisir un unique algorithme. Etant donné un échantillon \\(\\mathcal D_n=\\{(x_1,y_1),\\dots,(x_n,y_y)\\}\\) on rappelle qu’un algorithme de prévision est une fonction \\[g:\\mathcal X\\times(\\mathcal X\\times \\mathcal Y)^n\\to\\mathcal Y\\] qui, à une nouvelle observation \\(x\\in\\mathcal X\\) renverra la prévision \\(g(x,\\mathcal D_n)\\) calculée à partir de l’échantillon \\(\\mathcal D_n\\). Cette fonction \\(g\\) peut contenir tout un tas d’étapes comme : la gestion des données manquantes une procédure de choix de variables une méthode pour ré-équilibrer les données des procédures pour calibrer des paramètres (qui peuvent éventuellement inclure des validations croisées) … Le machine learning se focalisant sur la capacité d’un algorithme à bien prédire, les stratégies classiques pour choisir un algorithme vont (une fois de plus) consister à évaluer le pouvoir prédictif de chaque algorithme. Il n’y a rien de bien nouveau puisque cela va reposer sur les techniques présentées aux chapitres 1 : choisir un ou plusieurs critères (erreur de classification, AUC, \\(F_1\\)-score…) choisir une procédure de ré-échantillonnage pour estimer ce critère (validation hold-out, validation croisée, OOB…). Nous proposons de développer une stratégie pour choisir un algorithme sur le jeu de données Internet Advertisements Data Set disponible sur cette page https://archive.ics.uci.edu/ml/datasets/internet+advertisements. Le problème est d’identifier la présence d’une image publicitaire sur des pages webs. Il comporte ad.data &lt;- read.table(&quot;data/ad_data.txt&quot;,header=FALSE,sep=&quot;,&quot;,dec=&quot;.&quot;,na.strings = &quot;?&quot;,strip.white = TRUE) dim(ad.data) [1] 3279 1559 Ce jeu de données contient 1558 variables explicatives, ces variables contiennent différentes caractériques de la page web (voir le site où sont présentées les données pour plus d’information). La dernière variable est la variable à expliquer, elle vaut ad. si présence d’une publicité, nonad. sinon. names(ad.data)[ncol(ad.data)] &lt;- &quot;Y&quot; ad.data$Y &lt;- as.factor(ad.data$Y) summary(ad.data$Y) ad. nonad. 459 2820 Ce jeu de données contient des données manquantes. sum(is.na(ad.data)) [1] 2729 On peut les visualiser avec library(visdat) vis_miss(ad.data[,1:30]) On remarque que : 920 lignes 4 colonnes (les 4 premières) ont au moins une valeur manquante. apply(is.na(ad.data),1,any) %&gt;% sum() [1] 920 var.na &lt;- apply(is.na(ad.data),2,any) names(ad.data)[var.na] [1] &quot;V1&quot; &quot;V2&quot; &quot;V3&quot; &quot;V4&quot; On choisit de retirer ces 4 variables de l’analyse (il faudrait peut-être réfléchir un peu plus…). ad.data1 &lt;- ad.data[,var.na==FALSE] dim(ad.data1) [1] 3279 1555 sum(is.na(ad.data1)) [1] 0 On se retrouve donc en présence de 3279 individus et 1554 variables explicatives. On construit la matrice des X et le vecteur des Y qui sont nécessaires pour certaines fonctions comme glmnet : X.ad &lt;- model.matrix(Y~.,data=ad.data1)[,-1] Y.ad &lt;- ad.data1$Y et on transforme la variable cible en 0-1 pour utiliser gbm: ad.data2 &lt;- ad.data1 %&gt;% mutate(Y=recode(Y,&quot;ad.&quot;=0,&quot;nonad.&quot;=1)) On souhaite comparer les algorithmes présentés précédemment. Ils nécessitent les packages suivants library(e1071) library(caret) library(rpart) library(glmnet) library(ranger) library(gbm) On commence tout d’abord par représenter un algorithme par une fonction R qui admettra en entrée un jeu de données et renverra une unique prévision pour de nouveaux individus. On illustre ces fonctions pour prédire ce nouvel individu. newX &lt;- ad.data1[1000,] newX.X &lt;- matrix(X.ad[1000,],nrow=1) On stockera les prévisions dans l’objet suivant prev &lt;- tibble(algo=c(&quot;SVM&quot;,&quot;arbre&quot;,&quot;ridge&quot;,&quot;lasso&quot;,&quot;foret&quot;,&quot;ada&quot;,&quot;logit&quot;),prev=0) SVM à noyau gaussien où le choix des paramètres du noyau se fait par validation croisée 4 blocs : prev.svm &lt;- function(df,newX){ C &lt;- c(0.01,1,10) sigma &lt;- c(0.1,1,3) gr &lt;- expand.grid(C=C,sigma=sigma) ctrl &lt;- trainControl(method=&quot;cv&quot;,number=4) cl &lt;- makePSOCKcluster(3) registerDoParallel(cl) res.svm &lt;- train(Y~.,data=df,method=&quot;svmRadial&quot;,trControl=ctrl, tuneGrid=gr,prob.model=TRUE) stopCluster(cl) predict(res.svm,newX,type=&quot;prob&quot;)[2] } prev[1,2] &lt;- prev.svm(ad.data1,newX) Arbre de classification où l’élagage est fait selon la procédure CART présentée dans le chapitre 3. prev.arbre &lt;- function(df,newX){ arbre &lt;- rpart(Y~.,data=df,cp=1e-8,minsplit=2) cp_opt &lt;- arbre$cptable %&gt;% as.data.frame() %&gt;% filter(xerror==min(xerror)) %&gt;% dplyr::select(CP) %&gt;% slice(1) %&gt;% as.numeric() arbre.opt &lt;- prune(arbre,cp=cp_opt) predict(arbre,newdata=newX,type=&quot;prob&quot;)[,2] } prev[2,2] &lt;- prev.arbre(ad.data1,newX) Lasso et Ridge où le paramètre de régularisation est choisi par validation croisée 10 blocs en minimisant la déviance binomiale : prev.ridge &lt;- function(df.X,df.Y,newX){ ridge &lt;- cv.glmnet(df.X,df.Y,family=&quot;binomial&quot;,alpha=0) as.vector(predict(ridge,newx = newX,type=&quot;response&quot;)) } prev.lasso &lt;- function(df.X,df.Y,newX){ lasso &lt;- cv.glmnet(df.X,df.Y,family=&quot;binomial&quot;,alpha=1) as.vector(predict(lasso,newx = newX,type=&quot;response&quot;)) } prev[3,2] &lt;- prev.ridge(X.ad,Y.ad,newX.X) prev[4,2] &lt;- prev.lasso(X.ad,Y.ad,newX.X) Forêt aléatoire avec les paramètres par défaut : prev.foret &lt;- function(df,newX){ foret &lt;- ranger(Y~.,data=df,probability=TRUE) predict(foret,data=newX,type=&quot;response&quot;)$predictions[,2] } prev[5,2] &lt;- prev.foret(ad.data1,newX) Adaboost et logitboost avec le nombre d’itérations choisi par validation croisée 5 blocs : prev.ada &lt;- function(df,newX){ ada &lt;- gbm(Y~.,data=df,distribution=&quot;adaboost&quot;,interaction.depth=2, bag.fraction=1,cv.folds = 5,n.trees=500) nb.it &lt;- gbm.perf(ada,plot.it=FALSE) predict(ada,newdata=newX,n.trees=nb.it,type=&quot;response&quot;) } prev.logit &lt;- function(df,newX){ logit &lt;- gbm(Y~.,data=df,distribution=&quot;bernoulli&quot;,interaction.depth=2, bag.fraction=1,cv.folds = 5,n.trees=500) nb.it &lt;- gbm.perf(logit,plot.it=FALSE) predict(logit,newdata=newX,n.trees=nb.it,type=&quot;response&quot;) } prev[6,2] &lt;- prev.ada(ad.data2,newX) prev[7,2] &lt;- prev.logit(ad.data2,newX) On peut visualiser la prévision de chaque algorithme prev # A tibble: 7 x 2 algo prev &lt;chr&gt; &lt;dbl&gt; 1 SVM 0.950 2 arbre 0.990 3 ridge 0.984 4 lasso 0.980 5 foret 0.979 6 ada 0.974 7 logit 0.983 Exercice 8.1 (Choix d’un algorithme par validation croisée) Choisir un algorithme parmi les précédents en utilisant comme critère l’erreur de classification ainsi que la courbe ROC et l’AUC. On pourra faire une validation croisée 10 blocs (même si ça peut être un peu long…). Exercice 8.2 (Choix d’un algorithme de ré-équilibrage par validation croisée) On considère le même jeu de données que précédemment. Choisir un algorithme de ré-équilibrage par validation croisée. Il s’agira de combiner des méthodes de ré-équilibrage (random over/under sampling, smote, tomek…) avec des algorithmes de prévision de machine learning. On pourra se restreindre au modèle logistique avec calcul des estimateurs par maximum de vraisemblance, ridge, lasso… "],["références.html", "Références", " Références "]]
