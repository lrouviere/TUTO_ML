<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapitre 3 Arbres | Machine learning</title>
  <meta name="description" content="Chapitre 3 Arbres | Machine learning" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapitre 3 Arbres | Machine learning" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapitre 3 Arbres | Machine learning" />
  
  
  

<meta name="author" content="Laurent Rouvière" />


<meta name="date" content="2020-11-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="SVM.html"/>
<link rel="next" href="agregation.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.2/htmlwidgets.js"></script>
<link href="libs/vis-4.20.1/vis.css" rel="stylesheet" />
<script src="libs/vis-4.20.1/vis.min.js"></script>
<script src="libs/visNetwork-binding-2.0.9/visNetwork.js"></script>
<script src="libs/FileSaver-1.1.20151003/FileSaver.min.js"></script>
<script src="libs/Blob-1.0/Blob.js"></script>
<script src="libs/canvas-toBlob-1.0/canvas-toBlob.js"></script>
<script src="libs/html2canvas-0.5.0/html2canvas.js"></script>
<script src="libs/jspdf-1.3.2/jspdf.debug.js"></script>
<link href="libs/jquery-sparkline-2.1.2/jquery.sparkline.css" rel="stylesheet" />
<script src="libs/jquery-sparkline-2.1.2/jquery.sparkline.js"></script>
<script src="libs/sparkline-binding-2.0/sparkline.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning<br> <br> L. Rouvière</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Présentation</a></li>
<li class="chapter" data-level="1" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>1</b> Estimation du risque avec caret</a><ul>
<li class="chapter" data-level="1.1" data-path="caret.html"><a href="caret.html#notion-de-risque-en-apprentissage-supervisé"><i class="fa fa-check"></i><b>1.1</b> Notion de risque en apprentissage supervisé</a></li>
<li class="chapter" data-level="1.2" data-path="caret.html"><a href="caret.html#la-validation-croisée"><i class="fa fa-check"></i><b>1.2</b> La validation croisée</a></li>
<li class="chapter" data-level="1.3" data-path="caret.html"><a href="caret.html#le-package-caret"><i class="fa fa-check"></i><b>1.3</b> Le package caret</a></li>
<li class="chapter" data-level="1.4" data-path="caret.html"><a href="caret.html#compléments"><i class="fa fa-check"></i><b>1.4</b> Compléments</a><ul>
<li class="chapter" data-level="1.4.1" data-path="caret.html"><a href="caret.html#calcul-parallèle"><i class="fa fa-check"></i><b>1.4.1</b> Calcul parallèle</a></li>
<li class="chapter" data-level="1.4.2" data-path="caret.html"><a href="caret.html#répéter-les-méthodes-de-rééchantillonnage"><i class="fa fa-check"></i><b>1.4.2</b> Répéter les méthodes de rééchantillonnage</a></li>
<li class="chapter" data-level="1.4.3" data-path="caret.html"><a href="caret.html#modifier-le-risque"><i class="fa fa-check"></i><b>1.4.3</b> Modifier le risque</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="SVM.html"><a href="SVM.html"><i class="fa fa-check"></i><b>2</b> Support Vector Machine (SVM)</a><ul>
<li class="chapter" data-level="2.1" data-path="SVM.html"><a href="SVM.html#cas-séparable"><i class="fa fa-check"></i><b>2.1</b> Cas séparable</a></li>
<li class="chapter" data-level="2.2" data-path="SVM.html"><a href="SVM.html#cas-non-séparable"><i class="fa fa-check"></i><b>2.2</b> Cas non séparable</a></li>
<li class="chapter" data-level="2.3" data-path="SVM.html"><a href="SVM.html#lastuce-du-noyau"><i class="fa fa-check"></i><b>2.3</b> L’astuce du noyau</a></li>
<li class="chapter" data-level="2.4" data-path="SVM.html"><a href="SVM.html#support-vector-régression"><i class="fa fa-check"></i><b>2.4</b> Support vector régression</a></li>
<li class="chapter" data-level="2.5" data-path="SVM.html"><a href="SVM.html#svm-sur-les-données-spam"><i class="fa fa-check"></i><b>2.5</b> SVM sur les données spam</a></li>
<li class="chapter" data-level="2.6" data-path="SVM.html"><a href="SVM.html#exercices"><i class="fa fa-check"></i><b>2.6</b> Exercices</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="arbres.html"><a href="arbres.html"><i class="fa fa-check"></i><b>3</b> Arbres</a><ul>
<li class="chapter" data-level="3.1" data-path="arbres.html"><a href="arbres.html#coupures-cart-en-fonction-de-la-nature-des-variables"><i class="fa fa-check"></i><b>3.1</b> Coupures CART en fonction de la nature des variables</a><ul>
<li class="chapter" data-level="3.1.1" data-path="arbres.html"><a href="arbres.html#arbres-de-régression"><i class="fa fa-check"></i><b>3.1.1</b> Arbres de régression</a></li>
<li class="chapter" data-level="3.1.2" data-path="arbres.html"><a href="arbres.html#arbres-de-classification"><i class="fa fa-check"></i><b>3.1.2</b> Arbres de classification</a></li>
<li class="chapter" data-level="3.1.3" data-path="arbres.html"><a href="arbres.html#entrée-qualitative"><i class="fa fa-check"></i><b>3.1.3</b> Entrée qualitative</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="arbres.html"><a href="arbres.html#élagage"><i class="fa fa-check"></i><b>3.2</b> Élagage</a><ul>
<li class="chapter" data-level="3.2.1" data-path="arbres.html"><a href="arbres.html#élagage-pour-un-problème-de-régression"><i class="fa fa-check"></i><b>3.2.1</b> Élagage pour un problème de régression</a></li>
<li class="chapter" data-level="3.2.2" data-path="arbres.html"><a href="arbres.html#élagage-en-classification-binaire-et-matrice-de-coût"><i class="fa fa-check"></i><b>3.2.2</b> Élagage en classification binaire et matrice de coût</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="agregation.html"><a href="agregation.html"><i class="fa fa-check"></i><b>4</b> Agrégation : forêts aléatoires et gradient boosting</a><ul>
<li class="chapter" data-level="4.1" data-path="agregation.html"><a href="agregation.html#forets"><i class="fa fa-check"></i><b>4.1</b> Forêts aléatoires</a></li>
<li class="chapter" data-level="4.2" data-path="agregation.html"><a href="agregation.html#boosting"><i class="fa fa-check"></i><b>4.2</b> Gradient boosting</a><ul>
<li class="chapter" data-level="4.2.1" data-path="agregation.html"><a href="agregation.html#un-exemple-simple-en-régression"><i class="fa fa-check"></i><b>4.2.1</b> Un exemple simple en régression</a></li>
<li class="chapter" data-level="4.2.2" data-path="agregation.html"><a href="agregation.html#adaboost-et-logitboost-pour-la-classification-binaire."><i class="fa fa-check"></i><b>4.2.2</b> Adaboost et logitboost pour la classification binaire.</a></li>
<li class="chapter" data-level="4.2.3" data-path="agregation.html"><a href="agregation.html#exo:grad-boost"><i class="fa fa-check"></i><b>4.2.3</b> Exercices</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="deep.html"><a href="deep.html"><i class="fa fa-check"></i><b>5</b> Réseaux de neurones avec Keras</a></li>
<li class="chapter" data-level="6" data-path="dondes.html"><a href="dondes.html"><i class="fa fa-check"></i><b>6</b> Données déséquilibrées</a><ul>
<li class="chapter" data-level="6.1" data-path="dondes.html"><a href="dondes.html#critères-de-performance-pour-données-déséquilibrées"><i class="fa fa-check"></i><b>6.1</b> Critères de performance pour données déséquilibrées</a></li>
<li class="chapter" data-level="6.2" data-path="dondes.html"><a href="dondes.html#ré-équilibrage"><i class="fa fa-check"></i><b>6.2</b> Ré-équilibrage</a></li>
<li class="chapter" data-level="6.3" data-path="dondes.html"><a href="dondes.html#exercices-supplémentaires"><i class="fa fa-check"></i><b>6.3</b> Exercices supplémentaires</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="comp-algo.html"><a href="comp-algo.html"><i class="fa fa-check"></i><b>7</b> Comparaison d’algorithmes</a></li>
<li class="chapter" data-level="" data-path="références.html"><a href="références.html"><i class="fa fa-check"></i>Références</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="arbres" class="section level1">
<h1><span class="header-section-number">Chapitre 3</span> Arbres</h1>
<p>Les méthodes par arbres sont des algorithmes où la prévision s’effectue à partir de <strong>moyennes locales</strong>. Plus précisément, étant donné un échantillon <span class="math inline">\((x_1,y_1)\dots,(x_n,y_n)\)</span>, l’approche consiste à :</p>
<ul>
<li>construire une partition de l’espace de variables explicatives (<span class="math inline">\(\mathbb R^p\)</span>) ;</li>
<li>prédire la sortie d’une nouvelle observation <span class="math inline">\(x\)</span> en faisant :
<ul>
<li>la moyenne des <span class="math inline">\(y_i\)</span> pour les <span class="math inline">\(x_i\)</span> qui sont dans la même classe que <span class="math inline">\(x\)</span> si on est en régression ;</li>
<li>un vote à la majorité parmi les <span class="math inline">\(y_i\)</span> tels que les <span class="math inline">\(x_i\)</span> qui sont dans la même classe que <span class="math inline">\(x\)</span> si on est en classification.</li>
</ul></li>
</ul>
<p>Bien entendu toute la difficulté est de trouver la “bonne partition” pour le problème d’intérêt. Il existe un grand nombre d’algorithmes qui permettent de trouver une partition. Le plus connu est l’algorithme <strong>CART</strong> <span class="citation">(Breiman et al. <a href="#ref-brfrolst84" role="doc-biblioref">1984</a>)</span> où la partition est construite par <strong>divisions successives</strong> au moyen d’hyperplan orthogonaux aux axes de <span class="math inline">\(\mathbb R^p\)</span>. L’algorithme est récursif : il va à chaque étape séparer un groupe d’observations (<strong>nœuds</strong>) en deux groupes (<strong>nœuds fils</strong>) en cherchant la meilleure variable et le meilleur seuil de coupure. Ce choix s’effectue à partir d’un critère <strong>d’impureté</strong> : la meilleure coupure est celle pour laquelle l’impureté des 2 nœuds fils sera minimale. Nous étudions cet algorithme dans cette partie.</p>
<div id="coupures-cart-en-fonction-de-la-nature-des-variables" class="section level2">
<h2><span class="header-section-number">3.1</span> Coupures CART en fonction de la nature des variables</h2>
<p>Une partition CART s’obtient en séparant les observations en 2 selon une coupure parallèle aux axes puis en itérant ce procédé de séparation binaire sur les deux groupes… Par conséquent la première question à se poser est : pour un ensemble de données <span class="math inline">\((x_1,y_1),\dots,(x_n,y_n)\)</span> fixé, comment obtenir la meilleure coupure ?</p>
<p>Comme souvent ce sont les données qui vont répondre à cette question. La sélection de la meilleur coupure s’effectue en introduisant une <strong>fonction d’impureté <span class="math inline">\(\mathcal I\)</span></strong> qui va mesurer le degrés d’hétérogénéité d’un nœud <span class="math inline">\(\mathcal N\)</span>. Cette fonction prendra de</p>
<ul>
<li>grandes valeurs pour les nœuds hétérogènes (les valeurs de <span class="math inline">\(Y\)</span> diffèrent à l’intérieur du nœud) ;</li>
<li>faibles valeurs pour les nœuds homogènes (les valeurs de <span class="math inline">\(Y\)</span> sont proches à l’intérieur du nœud).</li>
</ul>
<p>On utilise souvent comme fonction d’impureté :</p>
<ul>
<li>la <strong>variance</strong> en régression
<span class="math display">\[\mathcal I(\mathcal N)=\frac{1}{|\mathcal N|}\sum_{i:x_i\in\mathcal N}(y_i-\overline{y}_\mathcal N)^2,\]</span>
où <span class="math inline">\(\overline{y} _\mathcal N\)</span> désigne la moyenne des <span class="math inline">\(y_i\)</span> dans <span class="math inline">\(\mathcal N\)</span>.</li>
<li>l’impureté de <strong>Gini</strong> en classification binaire
<span class="math display">\[\mathcal I(\mathcal N)=2p(\mathcal N)(1-p(\mathcal N))\]</span>
où <span class="math inline">\(p(\mathcal N)\)</span> représente la proportion de 1 dans <span class="math inline">\(\mathcal N\)</span>.</li>
</ul>
<p>Les coupures considérées par l’algorithme CART sont des hyperplans orthogonaux aux axes de <span class="math inline">\(\mathbb R^p\)</span>, choisir une coupure revient donc à choisir une variable <span class="math inline">\(j\)</span> parmi les <span class="math inline">\(p\)</span> variables explicatives et un seuil <span class="math inline">\(s\)</span> dans <span class="math inline">\(\mathbb R\)</span>. On peut donc représenter une coupure par un couple <span class="math inline">\((j,s)\)</span>.
Une fois l’impureté définie, on choisira la coupure <span class="math inline">\((j,s)\)</span> qui <strong>maximise le gain d’impureté</strong> entre le noeud père et ses deux noeuds fils :
<span class="math display">\[\Delta(\mathcal I)=\mathbf P(\mathcal N)\mathcal I(\mathcal N)-(\mathbf P(\mathcal N_1(j,s))\mathcal I(\mathcal N_1(j,s))+\mathbf P(\mathcal N_2(j,s))\mathcal I(\mathcal N_2(j,s))\]</span>
où
* <span class="math inline">\(\mathcal N_1(j,s)\)</span> et <span class="math inline">\(\mathcal N_2(j,s)\)</span> sont les 2 nœuds fils de <span class="math inline">\(\mathcal N\)</span> engendrés par la coupure <span class="math inline">\((j,s)\)</span> ;
* <span class="math inline">\(\mathbf P(\mathcal N)\)</span> représente la proportion d’observations dans le nœud <span class="math inline">\(\mathcal N\)</span>.</p>
<div id="arbres-de-régression" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Arbres de régression</h3>
<p>On considère le jeu de données suivant où le problème est d’expliquer la variable quantitative <span class="math inline">\(Y\)</span> par la variable quantitative <span class="math inline">\(X\)</span>.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="arbres.html#cb80-1"></a>n &lt;-<span class="st"> </span><span class="dv">50</span></span>
<span id="cb80-2"><a href="arbres.html#cb80-2"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb80-3"><a href="arbres.html#cb80-3"></a>X &lt;-<span class="st"> </span><span class="kw">runif</span>(n)</span>
<span id="cb80-4"><a href="arbres.html#cb80-4"></a><span class="kw">set.seed</span>(<span class="dv">5678</span>)</span>
<span id="cb80-5"><a href="arbres.html#cb80-5"></a>Y &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">*</span>X<span class="op">*</span>(X<span class="op">&lt;=</span><span class="fl">0.6</span>)<span class="op">+</span>(<span class="op">-</span><span class="dv">1</span><span class="op">*</span>X<span class="fl">+3.2</span>)<span class="op">*</span>(X<span class="op">&gt;</span><span class="fl">0.6</span>)<span class="op">+</span><span class="kw">rnorm</span>(n,<span class="dt">sd=</span><span class="fl">0.1</span>)</span>
<span id="cb80-6"><a href="arbres.html#cb80-6"></a>data1 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(X,Y)</span>
<span id="cb80-7"><a href="arbres.html#cb80-7"></a><span class="kw">ggplot</span>(data1)<span class="op">+</span><span class="kw">aes</span>(<span class="dt">x=</span>X,<span class="dt">y=</span>Y)<span class="op">+</span><span class="kw">geom_point</span>()</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-123-1.png" width="672" style="display: block; margin: auto;" /></p>
<ol style="list-style-type: decimal">
<li><p>A l’aide de la fonction <strong>rpart</strong> du package <strong>rpart</strong>, construire un arbre permettant d’expliquer <span class="math inline">\(Y\)</span> par <span class="math inline">\(X\)</span>.</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="arbres.html#cb81-1"></a><span class="kw">library</span>(rpart)</span></code></pre></div>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="arbres.html#cb82-1"></a>tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(Y<span class="op">~</span>X,<span class="dt">data=</span>data1)</span></code></pre></div></li>
<li><p>Visualiser l’arbre à l’aide des fonctions <strong>prp</strong> et <strong>rpart.plot</strong> du package <strong>rpart.plot</strong>.</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="arbres.html#cb83-1"></a><span class="kw">library</span>(rpart.plot)</span>
<span id="cb83-2"><a href="arbres.html#cb83-2"></a><span class="kw">prp</span>(tree)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-126-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="arbres.html#cb84-1"></a><span class="kw">rpart.plot</span>(tree)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-126-2.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>Écrire l’estimateur associé à l’arbre.</p>
<div class="corR">
<p>
On a un modèle de régression <span class="math display"><span class="math display">\[Y=m(X)+\varepsilon\]</span></span> où la fonction de régression (inconnue) <span class="math inline"><span class="math inline">\(m(x)\)</span></span> est estimée par
</p>
<p>
<span class="math display"><span class="math display">\[\widehat m(x)=0.31\, \mathbf{1}_{x&lt;0.58}+2.4\,\mathbf{1}_{x\geq 0.58}.\]</span></span>
</p>
</div></li>
<li><p>Ajouter sur le graphe de la question 1 la partition définie par l’arbre ainsi que les valeurs prédites.</p>
<div class="corR">
<p>
On obtient une partition avec 2 nœuds terminaux. Cette partition peut être résumée par la question : “est-ce que <span class="math inline"><span class="math inline">\(X\)</span></span> est plus petit que 0.58 ?”.
</p>
</div>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="arbres.html#cb85-1"></a>df1 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.58</span>),<span class="dt">xend=</span><span class="kw">c</span>(<span class="fl">0.58</span>,<span class="dv">1</span>),<span class="dt">y=</span><span class="kw">c</span>(<span class="fl">0.31</span>,<span class="fl">2.41</span>),<span class="dt">yend=</span><span class="kw">c</span>(<span class="fl">0.31</span>,<span class="fl">2.41</span>))</span>
<span id="cb85-2"><a href="arbres.html#cb85-2"></a><span class="kw">ggplot</span>(data1)<span class="op">+</span><span class="kw">aes</span>(<span class="dt">x=</span>X,<span class="dt">y=</span>Y)<span class="op">+</span><span class="kw">geom_point</span>()<span class="op">+</span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="fl">0.58</span>,<span class="dt">size=</span><span class="dv">1</span>,<span class="dt">color=</span><span class="st">&quot;blue&quot;</span>)<span class="op">+</span></span>
<span id="cb85-3"><a href="arbres.html#cb85-3"></a><span class="st">  </span><span class="kw">geom_segment</span>(<span class="dt">data=</span>df1,<span class="kw">aes</span>(<span class="dt">x=</span>x,<span class="dt">y=</span>y,<span class="dt">xend=</span>xend,<span class="dt">yend=</span>yend),<span class="dt">size=</span><span class="dv">1</span>,<span class="dt">color=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-129-1.png" width="672" style="display: block; margin: auto;" /></p></li>
</ol>
</div>
<div id="arbres-de-classification" class="section level3">
<h3><span class="header-section-number">3.1.2</span> Arbres de classification</h3>
<p>On considère les données suivantes où le problème est d’expliquer la variable binaire <span class="math inline">\(Y\)</span> par deux variables quantitatives <span class="math inline">\(X_1\)</span> et <span class="math inline">\(X_2\)</span>.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="arbres.html#cb86-1"></a>n &lt;-<span class="st"> </span><span class="dv">50</span></span>
<span id="cb86-2"><a href="arbres.html#cb86-2"></a><span class="kw">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb86-3"><a href="arbres.html#cb86-3"></a>X1 &lt;-<span class="st"> </span><span class="kw">runif</span>(n)</span>
<span id="cb86-4"><a href="arbres.html#cb86-4"></a><span class="kw">set.seed</span>(<span class="dv">5678</span>)</span>
<span id="cb86-5"><a href="arbres.html#cb86-5"></a>X2 &lt;-<span class="st"> </span><span class="kw">runif</span>(n)</span>
<span id="cb86-6"><a href="arbres.html#cb86-6"></a>Y &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,n)</span>
<span id="cb86-7"><a href="arbres.html#cb86-7"></a><span class="kw">set.seed</span>(<span class="dv">54321</span>)</span>
<span id="cb86-8"><a href="arbres.html#cb86-8"></a>Y[X1<span class="op">&lt;=</span><span class="fl">0.45</span>] &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="kw">sum</span>(X1<span class="op">&lt;=</span><span class="fl">0.45</span>),<span class="dv">1</span>,<span class="fl">0.85</span>)</span>
<span id="cb86-9"><a href="arbres.html#cb86-9"></a><span class="kw">set.seed</span>(<span class="dv">52432</span>)</span>
<span id="cb86-10"><a href="arbres.html#cb86-10"></a>Y[X1<span class="op">&gt;</span><span class="fl">0.45</span>] &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="kw">sum</span>(X1<span class="op">&gt;</span><span class="fl">0.45</span>),<span class="dv">1</span>,<span class="fl">0.15</span>)</span>
<span id="cb86-11"><a href="arbres.html#cb86-11"></a>data2 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(X1,X2,Y)</span>
<span id="cb86-12"><a href="arbres.html#cb86-12"></a><span class="kw">ggplot</span>(data2)<span class="op">+</span><span class="kw">aes</span>(<span class="dt">x=</span>X1,<span class="dt">y=</span>X2,<span class="dt">color=</span>Y)<span class="op">+</span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="dv">2</span>)<span class="op">+</span><span class="kw">scale_x_continuous</span>(<span class="dt">name=</span><span class="st">&quot;&quot;</span>)<span class="op">+</span></span>
<span id="cb86-13"><a href="arbres.html#cb86-13"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">name=</span><span class="st">&quot;&quot;</span>)<span class="op">+</span><span class="kw">theme_classic</span>()</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-130-1.png" width="672" style="display: block; margin: auto;" /></p>
<ol style="list-style-type: decimal">
<li><p>Construire un arbre permettant d’expliquer <span class="math inline">\(Y\)</span> par <span class="math inline">\(X_1\)</span> et <span class="math inline">\(X_2\)</span>. Représenter l’arbre et identifier l’éventuel problème.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="arbres.html#cb87-1"></a>tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>data2)</span>
<span id="cb87-2"><a href="arbres.html#cb87-2"></a><span class="kw">rpart.plot</span>(tree)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-131-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="corR">
<p>
On observe que l’arbre construit est un arbre de <strong>régression, pas de classification</strong>. Cela vient du fait que <span class="math inline"><span class="math inline">\(Y\)</span></span> est considérée par <strong>R</strong> comme une variable quantitative, il faut la convertir en facteur.
</p>
</div>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="arbres.html#cb88-1"></a>data2<span class="op">$</span>Y &lt;-<span class="st"> </span><span class="kw">as.factor</span>(data2<span class="op">$</span>Y)</span>
<span id="cb88-2"><a href="arbres.html#cb88-2"></a>tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>data2)</span>
<span id="cb88-3"><a href="arbres.html#cb88-3"></a><span class="kw">rpart.plot</span>(tree)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-133-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="corR">
<p>
Tout est OK maintenant !
</p>
</div></li>
<li><p>Écrire la règle de classification ainsi que la fonction de score définies par l’arbre.</p>
<div class="corR">
<p>
La règle de classification est <span class="math display"><span class="math display">\[\widehat g(x)=\mathbf{1}_{X_1&lt;0.44}.\]</span></span>
</p>
<p>
La fonction de score est donnée par <span class="math display"><span class="math display">\[\widehat S(x)=\widehat P(Y=1|X=x)=0.83\mathbf{1}_{X_1&lt;0.44}+0.07\mathbf{1}_{X_1\geq 0.44}.\]</span></span>
</p>
</div></li>
<li><p>Ajouter sur le graphe de la question 1 la partition définie par l’arbre.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="arbres.html#cb89-1"></a><span class="kw">ggplot</span>(data2)<span class="op">+</span><span class="kw">aes</span>(<span class="dt">x=</span>X1,<span class="dt">y=</span>X2,<span class="dt">color=</span>Y,<span class="dt">shape=</span>Y)<span class="op">+</span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="dv">2</span>)<span class="op">+</span></span>
<span id="cb89-2"><a href="arbres.html#cb89-2"></a><span class="st">  </span><span class="kw">theme_classic</span>()<span class="op">+</span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="fl">0.44</span>,<span class="dt">size=</span><span class="dv">1</span>,<span class="dt">color=</span><span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-136-1.png" width="672" style="display: block; margin: auto;" /></p></li>
</ol>
</div>
<div id="entrée-qualitative" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Entrée qualitative</h3>
<p>On considère les données</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="arbres.html#cb90-1"></a>n &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb90-2"><a href="arbres.html#cb90-2"></a>X &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">rep</span>(<span class="kw">c</span>(<span class="st">&quot;A&quot;</span>,<span class="st">&quot;B&quot;</span>,<span class="st">&quot;C&quot;</span>,<span class="st">&quot;D&quot;</span>),n))</span>
<span id="cb90-3"><a href="arbres.html#cb90-3"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb90-4"><a href="arbres.html#cb90-4"></a>Y[X<span class="op">==</span><span class="st">&quot;A&quot;</span>] &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="kw">sum</span>(X<span class="op">==</span><span class="st">&quot;A&quot;</span>),<span class="dv">1</span>,<span class="fl">0.9</span>)</span>
<span id="cb90-5"><a href="arbres.html#cb90-5"></a>Y[X<span class="op">==</span><span class="st">&quot;B&quot;</span>] &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="kw">sum</span>(X<span class="op">==</span><span class="st">&quot;B&quot;</span>),<span class="dv">1</span>,<span class="fl">0.25</span>)</span>
<span id="cb90-6"><a href="arbres.html#cb90-6"></a>Y[X<span class="op">==</span><span class="st">&quot;C&quot;</span>] &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="kw">sum</span>(X<span class="op">==</span><span class="st">&quot;C&quot;</span>),<span class="dv">1</span>,<span class="fl">0.8</span>)</span>
<span id="cb90-7"><a href="arbres.html#cb90-7"></a>Y[X<span class="op">==</span><span class="st">&quot;D&quot;</span>] &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="kw">sum</span>(X<span class="op">==</span><span class="st">&quot;D&quot;</span>),<span class="dv">1</span>,<span class="fl">0.2</span>)</span>
<span id="cb90-8"><a href="arbres.html#cb90-8"></a>Y &lt;-<span class="st"> </span><span class="kw">as.factor</span>(Y)</span>
<span id="cb90-9"><a href="arbres.html#cb90-9"></a>data3 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(X,Y)</span></code></pre></div>
<ol style="list-style-type: decimal">
<li><p>Construire un arbre permettant d’expliquer <span class="math inline">\(Y\)</span> par <span class="math inline">\(X\)</span>.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="arbres.html#cb91-1"></a>tree3 &lt;-<span class="st"> </span><span class="kw">rpart</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>data3)</span>
<span id="cb91-2"><a href="arbres.html#cb91-2"></a><span class="kw">rpart.plot</span>(tree3)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-138-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>Expliquer la manière dont l’arbre est construit dans ce cadre là.</p>
<div class="corR">
<p>
La variable étant qualitative, on ne cherche pas un seuil de coupure pour diviser un nœud en 2. On va ici considérer toutes les partitions binaires de l’ensemble <span class="math inline"><span class="math inline">\(\{A,B,C,D\}\)</span></span>. La meilleure partition est <span class="math inline"><span class="math inline">\(\{\{A,C\},\{B,D\}\}\)</span></span>.
</p>
</div></li>
</ol>
</div>
</div>
<div id="élagage" class="section level2">
<h2><span class="header-section-number">3.2</span> Élagage</h2>
<p>Le procédé de coupe présenté précédemment permet de définir un très grand nombre d’arbres à partir d’un jeu de données (arbre sans coupure, avec une coupure, deux coupures…). Se pose alors la question de trouver le <strong>meilleur arbre</strong> parmi tous les arbres possibles. Une première idée serait de choisir parmi tous les arbres possibles celui qui optimise un critère de performance. Cette approche, bien que cohérente, n’est généralement pas possible à mettre en œuvre en pratique car le nombre d’arbres à considérer est souvent trop important.</p>
<p>La méthode CART propose une procédure permettant de choisir automatiquement un arbre en 3 étapes :</p>
<ul>
<li>On construit un <strong>arbre maximal</strong> (très profond) <span class="math inline">\(\mathcal T_{max}\)</span> ;</li>
<li>On sélectionne une <strong>suite d’arbres emboités</strong> :
<span class="math display">\[\mathcal T_{max}=\mathcal T_0\supset\mathcal T_1\supset\dots\supset \mathcal T_K.\]</span>
La sélection s’effectue en optimisant un critère <strong>Cout/complexité</strong> qui permet de réguler le compromis entre <strong>ajustement</strong> et <strong>complexité</strong> de l’arbre.</li>
<li>On <strong>sélectionne un arbre</strong> dans cette sous-suite en optimisant un critère de performance.</li>
</ul>
<p>Cette approche revient à choisir un sous-arbre de l’arbre <span class="math inline">\(\mathcal T_\text{max}\)</span>, c’est-à-dire à enlever des branches à <span class="math inline">\(T_\text{max}\)</span>, c’est pourquoi on parle <strong>d’élagage</strong>.</p>
<div id="élagage-pour-un-problème-de-régression" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Élagage pour un problème de régression</h3>
<p>On considère les données <strong>Carseats</strong> du package <strong>ISLR</strong>.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="arbres.html#cb92-1"></a><span class="kw">library</span>(ISLR)</span>
<span id="cb92-2"><a href="arbres.html#cb92-2"></a><span class="kw">data</span>(Carseats)</span>
<span id="cb92-3"><a href="arbres.html#cb92-3"></a><span class="kw">summary</span>(Carseats)</span>
<span id="cb92-4"><a href="arbres.html#cb92-4"></a>     Sales          CompPrice       Income      </span>
<span id="cb92-5"><a href="arbres.html#cb92-5"></a> Min.   <span class="op">:</span><span class="st"> </span><span class="fl">0.000</span>   Min.   <span class="op">:</span><span class="st"> </span><span class="dv">77</span>   Min.   <span class="op">:</span><span class="st"> </span><span class="fl">21.00</span>  </span>
<span id="cb92-6"><a href="arbres.html#cb92-6"></a> 1st Qu.<span class="op">:</span><span class="st"> </span><span class="fl">5.390</span>   1st Qu.<span class="op">:</span><span class="dv">115</span>   1st Qu.<span class="op">:</span><span class="st"> </span><span class="fl">42.75</span>  </span>
<span id="cb92-7"><a href="arbres.html#cb92-7"></a> Median <span class="op">:</span><span class="st"> </span><span class="fl">7.490</span>   Median <span class="op">:</span><span class="dv">125</span>   Median <span class="op">:</span><span class="st"> </span><span class="fl">69.00</span>  </span>
<span id="cb92-8"><a href="arbres.html#cb92-8"></a> Mean   <span class="op">:</span><span class="st"> </span><span class="fl">7.496</span>   Mean   <span class="op">:</span><span class="dv">125</span>   Mean   <span class="op">:</span><span class="st"> </span><span class="fl">68.66</span>  </span>
<span id="cb92-9"><a href="arbres.html#cb92-9"></a> 3rd Qu.<span class="op">:</span><span class="st"> </span><span class="fl">9.320</span>   3rd Qu.<span class="op">:</span><span class="dv">135</span>   3rd Qu.<span class="op">:</span><span class="st"> </span><span class="fl">91.00</span>  </span>
<span id="cb92-10"><a href="arbres.html#cb92-10"></a> Max.   <span class="op">:</span><span class="fl">16.270</span>   Max.   <span class="op">:</span><span class="dv">175</span>   Max.   <span class="op">:</span><span class="fl">120.00</span>  </span>
<span id="cb92-11"><a href="arbres.html#cb92-11"></a>  Advertising       Population        Price      </span>
<span id="cb92-12"><a href="arbres.html#cb92-12"></a> Min.   <span class="op">:</span><span class="st"> </span><span class="fl">0.000</span>   Min.   <span class="op">:</span><span class="st"> </span><span class="fl">10.0</span>   Min.   <span class="op">:</span><span class="st"> </span><span class="fl">24.0</span>  </span>
<span id="cb92-13"><a href="arbres.html#cb92-13"></a> 1st Qu.<span class="op">:</span><span class="st"> </span><span class="fl">0.000</span>   1st Qu.<span class="op">:</span><span class="fl">139.0</span>   1st Qu.<span class="op">:</span><span class="fl">100.0</span>  </span>
<span id="cb92-14"><a href="arbres.html#cb92-14"></a> Median <span class="op">:</span><span class="st"> </span><span class="fl">5.000</span>   Median <span class="op">:</span><span class="fl">272.0</span>   Median <span class="op">:</span><span class="fl">117.0</span>  </span>
<span id="cb92-15"><a href="arbres.html#cb92-15"></a> Mean   <span class="op">:</span><span class="st"> </span><span class="fl">6.635</span>   Mean   <span class="op">:</span><span class="fl">264.8</span>   Mean   <span class="op">:</span><span class="fl">115.8</span>  </span>
<span id="cb92-16"><a href="arbres.html#cb92-16"></a> 3rd Qu.<span class="op">:</span><span class="fl">12.000</span>   3rd Qu.<span class="op">:</span><span class="fl">398.5</span>   3rd Qu.<span class="op">:</span><span class="fl">131.0</span>  </span>
<span id="cb92-17"><a href="arbres.html#cb92-17"></a> Max.   <span class="op">:</span><span class="fl">29.000</span>   Max.   <span class="op">:</span><span class="fl">509.0</span>   Max.   <span class="op">:</span><span class="fl">191.0</span>  </span>
<span id="cb92-18"><a href="arbres.html#cb92-18"></a>  ShelveLoc        Age          Education    Urban    </span>
<span id="cb92-19"><a href="arbres.html#cb92-19"></a> Bad   <span class="op">:</span><span class="st"> </span><span class="dv">96</span>   Min.   <span class="op">:</span><span class="fl">25.00</span>   Min.   <span class="op">:</span><span class="fl">10.0</span>   No <span class="op">:</span><span class="dv">118</span>  </span>
<span id="cb92-20"><a href="arbres.html#cb92-20"></a> Good  <span class="op">:</span><span class="st"> </span><span class="dv">85</span>   1st Qu.<span class="op">:</span><span class="fl">39.75</span>   1st Qu.<span class="op">:</span><span class="fl">12.0</span>   Yes<span class="op">:</span><span class="dv">282</span>  </span>
<span id="cb92-21"><a href="arbres.html#cb92-21"></a> Medium<span class="op">:</span><span class="dv">219</span>   Median <span class="op">:</span><span class="fl">54.50</span>   Median <span class="op">:</span><span class="fl">14.0</span>            </span>
<span id="cb92-22"><a href="arbres.html#cb92-22"></a>              Mean   <span class="op">:</span><span class="fl">53.32</span>   Mean   <span class="op">:</span><span class="fl">13.9</span>            </span>
<span id="cb92-23"><a href="arbres.html#cb92-23"></a>              3rd Qu.<span class="op">:</span><span class="fl">66.00</span>   3rd Qu.<span class="op">:</span><span class="fl">16.0</span>            </span>
<span id="cb92-24"><a href="arbres.html#cb92-24"></a>              Max.   <span class="op">:</span><span class="fl">80.00</span>   Max.   <span class="op">:</span><span class="fl">18.0</span>            </span>
<span id="cb92-25"><a href="arbres.html#cb92-25"></a>   US     </span>
<span id="cb92-26"><a href="arbres.html#cb92-26"></a> No <span class="op">:</span><span class="dv">142</span>  </span>
<span id="cb92-27"><a href="arbres.html#cb92-27"></a> Yes<span class="op">:</span><span class="dv">258</span>  </span>
<span id="cb92-28"><a href="arbres.html#cb92-28"></a>          </span>
<span id="cb92-29"><a href="arbres.html#cb92-29"></a>          </span>
<span id="cb92-30"><a href="arbres.html#cb92-30"></a>          </span>
<span id="cb92-31"><a href="arbres.html#cb92-31"></a>          </span></code></pre></div>
<p>On cherche ici à expliquer la variable quantitative <strong>Sales</strong> par les autres variables.</p>
<ol style="list-style-type: decimal">
<li><p>Construire un arbre permettant de répondre au problème.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="arbres.html#cb93-1"></a>tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(Sales<span class="op">~</span>.,<span class="dt">data=</span>Carseats)</span>
<span id="cb93-2"><a href="arbres.html#cb93-2"></a><span class="kw">rpart.plot</span>(tree)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-141-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>Expliquer les sorties de la fonction <strong>printcp</strong> appliquée à l’arbre de la question précédente et calculer le dernier terme de la colonne <strong>rel error</strong>.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="arbres.html#cb94-1"></a><span class="kw">printcp</span>(tree)</span>
<span id="cb94-2"><a href="arbres.html#cb94-2"></a></span>
<span id="cb94-3"><a href="arbres.html#cb94-3"></a>Regression tree<span class="op">:</span></span>
<span id="cb94-4"><a href="arbres.html#cb94-4"></a><span class="kw">rpart</span>(<span class="dt">formula =</span> Sales <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> Carseats)</span>
<span id="cb94-5"><a href="arbres.html#cb94-5"></a></span>
<span id="cb94-6"><a href="arbres.html#cb94-6"></a>Variables actually used <span class="cf">in</span> tree construction<span class="op">:</span></span>
<span id="cb94-7"><a href="arbres.html#cb94-7"></a>[<span class="dv">1</span>] Advertising Age         CompPrice   Income     </span>
<span id="cb94-8"><a href="arbres.html#cb94-8"></a>[<span class="dv">5</span>] Population  Price       ShelveLoc  </span>
<span id="cb94-9"><a href="arbres.html#cb94-9"></a></span>
<span id="cb94-10"><a href="arbres.html#cb94-10"></a>Root node error<span class="op">:</span><span class="st"> </span><span class="fl">3182.3</span><span class="op">/</span><span class="dv">400</span> =<span class="st"> </span><span class="fl">7.9557</span></span>
<span id="cb94-11"><a href="arbres.html#cb94-11"></a></span>
<span id="cb94-12"><a href="arbres.html#cb94-12"></a>n=<span class="st"> </span><span class="dv">400</span> </span>
<span id="cb94-13"><a href="arbres.html#cb94-13"></a></span>
<span id="cb94-14"><a href="arbres.html#cb94-14"></a>         CP nsplit rel error  xerror     xstd</span>
<span id="cb94-15"><a href="arbres.html#cb94-15"></a><span class="dv">1</span>  <span class="fl">0.250510</span>      <span class="dv">0</span>   <span class="fl">1.00000</span> <span class="fl">1.00492</span> <span class="fl">0.069530</span></span>
<span id="cb94-16"><a href="arbres.html#cb94-16"></a><span class="dv">2</span>  <span class="fl">0.105073</span>      <span class="dv">1</span>   <span class="fl">0.74949</span> <span class="fl">0.75877</span> <span class="fl">0.051613</span></span>
<span id="cb94-17"><a href="arbres.html#cb94-17"></a><span class="dv">3</span>  <span class="fl">0.051121</span>      <span class="dv">2</span>   <span class="fl">0.64442</span> <span class="fl">0.68283</span> <span class="fl">0.046333</span></span>
<span id="cb94-18"><a href="arbres.html#cb94-18"></a><span class="dv">4</span>  <span class="fl">0.045671</span>      <span class="dv">3</span>   <span class="fl">0.59330</span> <span class="fl">0.64240</span> <span class="fl">0.043550</span></span>
<span id="cb94-19"><a href="arbres.html#cb94-19"></a><span class="dv">5</span>  <span class="fl">0.033592</span>      <span class="dv">4</span>   <span class="fl">0.54763</span> <span class="fl">0.60051</span> <span class="fl">0.041716</span></span>
<span id="cb94-20"><a href="arbres.html#cb94-20"></a><span class="dv">6</span>  <span class="fl">0.024063</span>      <span class="dv">5</span>   <span class="fl">0.51403</span> <span class="fl">0.58903</span> <span class="fl">0.039691</span></span>
<span id="cb94-21"><a href="arbres.html#cb94-21"></a><span class="dv">7</span>  <span class="fl">0.023948</span>      <span class="dv">6</span>   <span class="fl">0.48997</span> <span class="fl">0.59472</span> <span class="fl">0.039561</span></span>
<span id="cb94-22"><a href="arbres.html#cb94-22"></a><span class="dv">8</span>  <span class="fl">0.022163</span>      <span class="dv">7</span>   <span class="fl">0.46602</span> <span class="fl">0.58972</span> <span class="fl">0.039539</span></span>
<span id="cb94-23"><a href="arbres.html#cb94-23"></a><span class="dv">9</span>  <span class="fl">0.016043</span>      <span class="dv">8</span>   <span class="fl">0.44386</span> <span class="fl">0.58329</span> <span class="fl">0.039731</span></span>
<span id="cb94-24"><a href="arbres.html#cb94-24"></a><span class="dv">10</span> <span class="fl">0.014027</span>      <span class="dv">9</span>   <span class="fl">0.42782</span> <span class="fl">0.57392</span> <span class="fl">0.038516</span></span>
<span id="cb94-25"><a href="arbres.html#cb94-25"></a><span class="dv">11</span> <span class="fl">0.013145</span>     <span class="dv">11</span>   <span class="fl">0.39976</span> <span class="fl">0.57780</span> <span class="fl">0.038529</span></span>
<span id="cb94-26"><a href="arbres.html#cb94-26"></a><span class="dv">12</span> <span class="fl">0.012711</span>     <span class="dv">12</span>   <span class="fl">0.38662</span> <span class="fl">0.58719</span> <span class="fl">0.038339</span></span>
<span id="cb94-27"><a href="arbres.html#cb94-27"></a><span class="dv">13</span> <span class="fl">0.012147</span>     <span class="dv">13</span>   <span class="fl">0.37391</span> <span class="fl">0.58970</span> <span class="fl">0.038419</span></span>
<span id="cb94-28"><a href="arbres.html#cb94-28"></a><span class="dv">14</span> <span class="fl">0.011888</span>     <span class="dv">14</span>   <span class="fl">0.36176</span> <span class="fl">0.58850</span> <span class="fl">0.038291</span></span>
<span id="cb94-29"><a href="arbres.html#cb94-29"></a><span class="dv">15</span> <span class="fl">0.010778</span>     <span class="dv">15</span>   <span class="fl">0.34987</span> <span class="fl">0.58673</span> <span class="fl">0.038383</span></span>
<span id="cb94-30"><a href="arbres.html#cb94-30"></a><span class="dv">16</span> <span class="fl">0.010506</span>     <span class="dv">16</span>   <span class="fl">0.33909</span> <span class="fl">0.57818</span> <span class="fl">0.038886</span></span>
<span id="cb94-31"><a href="arbres.html#cb94-31"></a><span class="dv">17</span> <span class="fl">0.010000</span>     <span class="dv">17</span>   <span class="fl">0.32859</span> <span class="fl">0.57320</span> <span class="fl">0.038277</span></span></code></pre></div>
<div class="corR">
<p>
<p>On peut lire des informations sur la suite d’arbres emboîtés, cette suite est de longueur 17 ici. Dans le dernier tableau, chaque ligne représente un arbre de la suite et on a dans les colonnes :</p>
</p>
<ul>
<li>
<p><strong>CP</strong> : le paramètre de complexité, plus il est petit plus l’arbre est profond ;</p>
</li>
<li>
<p><strong>nsplit</strong> : nombre de coupures de l’arbre ;</p>
</li>
<li>
<p><strong>rel error</strong> contient l’erreur calculée sur les données d’apprentissage. Cette erreur décroit lorsque la complexité augmente et peut être interprétée comme une erreur d’ajustement ;</p>
</li>
<li>
<p><strong>xerror</strong> : contient l’erreur calculée par validation croisée. Elle peut être interprétée comme une erreur de prévision ;</p>
</li>
<li>
<p><strong>xstd</strong> correspond à l’écart type estimé de l’erreur.</p>
</li>
</ul>
<p>
<p>Les types d’erreurs dépendent du problème considéré. Vu qu’on est ici sur un problème de régression, c’est l’<strong>erreur quadratique moyenne</strong> qui est considérée. De plus ces erreurs sont normalisées par rapport à l’erreur de l’arbre racine (sans coupure). Ainsi on retrouve l’erreur demandée avec</p>
</p>
</div>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="arbres.html#cb95-1"></a> Carseats <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">fitted=</span><span class="kw">predict</span>(tree)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb95-2"><a href="arbres.html#cb95-2"></a><span class="st">   </span><span class="kw">summarise</span>(<span class="dt">MSE=</span><span class="kw">mean</span>((fitted<span class="op">-</span>Sales)<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">mean</span>((Sales<span class="op">-</span><span class="kw">mean</span>(Sales))<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb95-3"><a href="arbres.html#cb95-3"></a>         MSE</span>
<span id="cb95-4"><a href="arbres.html#cb95-4"></a> <span class="dv">1</span> <span class="fl">0.3285866</span></span>
<span id="cb95-5"><a href="arbres.html#cb95-5"></a> <span class="co">#mean((predict(tree)-Carseats$Sales)^2)/mean((Carseats$Sales-mean(Carseats$Sales))^2)</span></span></code></pre></div></li>
<li><p>Construire une suite d’arbres plus grandes en jouant sur les paramètres <code>cp</code> et <code>minsplit</code> de la fonction <strong>rpart</strong>.</p>
<div class="corR">
<p>
Il suffit de diminuer les valeurs par défaut de ces paramètres.
</p>
</div>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="arbres.html#cb96-1"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb96-2"><a href="arbres.html#cb96-2"></a>tree1 &lt;-<span class="st"> </span><span class="kw">rpart</span>(Sales<span class="op">~</span>.,<span class="dt">data=</span>Carseats,<span class="dt">cp=</span><span class="fl">0.00001</span>,<span class="dt">minsplit=</span><span class="dv">2</span>)</span>
<span id="cb96-3"><a href="arbres.html#cb96-3"></a><span class="kw">printcp</span>(tree1)</span></code></pre></div>
<pre style="max-height: 500px;"><code>
Regression tree:
rpart(formula = Sales ~ ., data = Carseats, cp = 1e-05, minsplit = 2)

Variables actually used in tree construction:
 [1] Advertising Age         CompPrice   Education  
 [5] Income      Population  Price       ShelveLoc  
 [9] Urban       US         

Root node error: 3182.3/400 = 7.9557

n= 400 

            CP nsplit  rel error  xerror     xstd
1   2.5051e-01      0 1.00000000 1.00632 0.069635
2   1.0507e-01      1 0.74948961 0.75985 0.051802
3   5.1121e-02      2 0.64441706 0.67592 0.044633
4   4.5671e-02      3 0.59329646 0.67720 0.043488
5   3.3592e-02      4 0.54762521 0.64640 0.043209
6   2.4063e-02      5 0.51403284 0.64674 0.041192
7   2.3948e-02      6 0.48997005 0.63825 0.041103
8   2.2163e-02      7 0.46602225 0.63234 0.040983
9   1.6043e-02      8 0.44385897 0.61886 0.039262
10  1.4027e-02      9 0.42781645 0.61392 0.039072
11  1.3145e-02     11 0.39976237 0.61848 0.039060
12  1.2711e-02     12 0.38661699 0.62070 0.039293
13  1.2147e-02     13 0.37390609 0.62158 0.039768
14  1.1888e-02     14 0.36175900 0.62655 0.039848
15  1.0778e-02     15 0.34987122 0.61565 0.039116
16  1.0506e-02     16 0.33909277 0.62089 0.039292
17  1.0301e-02     17 0.32858663 0.62477 0.039223
18  9.8052e-03     18 0.31828518 0.62074 0.039205
19  9.5324e-03     20 0.29867475 0.62340 0.039696
20  9.3098e-03     21 0.28914234 0.61989 0.039722
21  8.6039e-03     22 0.27983257 0.62566 0.040168
22  8.5728e-03     23 0.27122871 0.61686 0.039659
23  7.7737e-03     25 0.25408305 0.62076 0.040607
24  7.4353e-03     26 0.24630936 0.61125 0.040480
25  6.2838e-03     28 0.23143882 0.59597 0.039395
26  6.1242e-03     29 0.22515504 0.60994 0.040169
27  5.6953e-03     30 0.21903085 0.59896 0.039173
28  5.5687e-03     31 0.21333555 0.60151 0.039820
29  5.4134e-03     32 0.20776686 0.60041 0.039844
30  5.1373e-03     33 0.20235343 0.58408 0.039291
31  4.9581e-03     34 0.19721608 0.58631 0.039265
32  4.8270e-03     35 0.19225798 0.58969 0.039393
33  4.5558e-03     36 0.18743102 0.59070 0.039253
34  4.5456e-03     37 0.18287525 0.58833 0.038997
35  4.3739e-03     38 0.17832965 0.58757 0.038982
36  4.3307e-03     39 0.17395578 0.58716 0.038985
37  4.2485e-03     40 0.16962503 0.58706 0.039096
38  4.0980e-03     41 0.16537650 0.58472 0.039006
39  4.0525e-03     42 0.16127847 0.58935 0.039188
40  4.0054e-03     43 0.15722596 0.58756 0.038706
41  3.6917e-03     44 0.15322052 0.60472 0.039435
42  3.6352e-03     45 0.14952883 0.60179 0.039308
43  3.5301e-03     46 0.14589367 0.60395 0.039286
44  3.5196e-03     47 0.14236356 0.60402 0.039279
45  2.8653e-03     48 0.13884396 0.59319 0.038874
46  2.8565e-03     49 0.13597868 0.58540 0.039159
47  2.8565e-03     50 0.13312217 0.58540 0.039159
48  2.7253e-03     51 0.13026571 0.58760 0.039209
49  2.6841e-03     52 0.12754044 0.58585 0.038937
50  2.6829e-03     54 0.12217220 0.58743 0.038915
51  2.6660e-03     55 0.11948928 0.58794 0.038911
52  2.4588e-03     56 0.11682326 0.58713 0.038864
53  2.3693e-03     57 0.11436443 0.57598 0.038151
54  2.3018e-03     58 0.11199508 0.57746 0.038203
55  2.2746e-03     60 0.10739152 0.58523 0.039585
56  2.2540e-03     61 0.10511688 0.58489 0.039595
57  2.1781e-03     62 0.10286290 0.58466 0.039588
58  2.1645e-03     63 0.10068483 0.58575 0.039509
59  2.0950e-03     64 0.09852033 0.58152 0.039361
60  2.0945e-03     65 0.09642538 0.58236 0.039446
61  2.0740e-03     66 0.09433084 0.58431 0.039597
62  1.8864e-03     67 0.09225680 0.57892 0.039320
63  1.8413e-03     68 0.09037038 0.58456 0.039520
64  1.7921e-03     69 0.08852905 0.58578 0.040068
65  1.7167e-03     70 0.08673697 0.58533 0.039995
66  1.6766e-03     71 0.08502031 0.58336 0.039558
67  1.6704e-03     72 0.08334367 0.58559 0.039491
68  1.6064e-03     73 0.08167332 0.58367 0.039470
69  1.6055e-03     74 0.08006697 0.58229 0.039276
70  1.5103e-03     75 0.07846149 0.58881 0.039911
71  1.4967e-03     76 0.07695120 0.58862 0.039908
72  1.4907e-03     77 0.07545453 0.59042 0.040029
73  1.4007e-03     78 0.07396387 0.60029 0.040368
74  1.4002e-03     79 0.07256317 0.60033 0.040358
75  1.3613e-03     80 0.07116301 0.60705 0.040742
76  1.3589e-03     81 0.06980172 0.61439 0.041431
77  1.3462e-03     82 0.06844282 0.61457 0.041431
78  1.3351e-03     83 0.06709659 0.61405 0.041405
79  1.3304e-03     84 0.06576144 0.61487 0.041409
80  1.3146e-03     85 0.06443102 0.61487 0.041409
81  1.2795e-03     86 0.06311644 0.61217 0.041323
82  1.2412e-03     87 0.06183696 0.61153 0.041238
83  1.2373e-03     88 0.06059575 0.61610 0.041280
84  1.2135e-03     89 0.05935843 0.61519 0.041307
85  1.2002e-03     91 0.05693148 0.61097 0.041152
86  1.1269e-03     92 0.05573126 0.61178 0.041184
87  1.0919e-03     93 0.05460435 0.60862 0.041145
88  1.0898e-03     94 0.05351243 0.60925 0.041145
89  1.0864e-03     95 0.05242260 0.60925 0.041145
90  1.0646e-03     96 0.05133621 0.60693 0.041083
91  1.0116e-03     97 0.05027156 0.60260 0.040185
92  9.5940e-04     98 0.04925996 0.60122 0.040328
93  8.9105e-04     99 0.04830056 0.60234 0.040289
94  8.8465e-04    100 0.04740951 0.60105 0.040619
95  8.7611e-04    101 0.04652486 0.60055 0.040661
96  8.5644e-04    102 0.04564875 0.60111 0.040661
97  8.4568e-04    103 0.04479231 0.60150 0.040657
98  8.3004e-04    104 0.04394663 0.60427 0.040867
99  8.0748e-04    105 0.04311659 0.60590 0.040864
100 7.9944e-04    106 0.04230912 0.60627 0.040864
101 7.5680e-04    107 0.04150968 0.61302 0.041688
102 7.4082e-04    108 0.04075288 0.61164 0.041485
103 7.4043e-04    109 0.04001206 0.61183 0.041479
104 7.3510e-04    110 0.03927163 0.61163 0.041483
105 7.0107e-04    111 0.03853653 0.61182 0.041563
106 6.9184e-04    112 0.03783546 0.60947 0.041716
107 6.7585e-04    113 0.03714362 0.60947 0.041716
108 6.7373e-04    114 0.03646776 0.60689 0.041731
109 6.7173e-04    115 0.03579403 0.60689 0.041731
110 6.6783e-04    116 0.03512230 0.60591 0.041707
111 6.6518e-04    117 0.03445448 0.60613 0.041700
112 6.6451e-04    118 0.03378929 0.60613 0.041700
113 6.0900e-04    119 0.03312478 0.60732 0.041781
114 6.0343e-04    120 0.03251578 0.61077 0.042036
115 5.9465e-04    121 0.03191235 0.61269 0.042102
116 5.8550e-04    123 0.03072304 0.61208 0.042081
117 5.8340e-04    124 0.03013754 0.61187 0.042084
118 5.6972e-04    125 0.02955414 0.61258 0.042084
119 5.6433e-04    126 0.02898442 0.61258 0.042084
120 5.6323e-04    127 0.02842009 0.61228 0.042075
121 5.4821e-04    128 0.02785686 0.60943 0.042014
122 5.4339e-04    131 0.02621222 0.60980 0.042021
123 5.1968e-04    132 0.02566882 0.60983 0.042171
124 5.0869e-04    133 0.02514915 0.60906 0.042168
125 5.0157e-04    134 0.02464045 0.60828 0.042297
126 4.7302e-04    135 0.02413889 0.61003 0.042276
127 4.6969e-04    136 0.02366587 0.60911 0.042228
128 4.6775e-04    137 0.02319618 0.61118 0.042218
129 4.6669e-04    138 0.02272842 0.61118 0.042218
130 4.5761e-04    139 0.02226174 0.60991 0.042232
131 4.5283e-04    140 0.02180413 0.60956 0.042235
132 4.5270e-04    141 0.02135130 0.61176 0.042356
133 4.5251e-04    142 0.02089861 0.61176 0.042356
134 4.4875e-04    143 0.02044610 0.61176 0.042356
135 4.4874e-04    144 0.01999735 0.61164 0.042360
136 4.4666e-04    145 0.01954861 0.61164 0.042360
137 4.3805e-04    146 0.01910194 0.61410 0.042468
138 4.2159e-04    147 0.01866389 0.61468 0.042470
139 4.1179e-04    148 0.01824230 0.61626 0.042531
140 3.8646e-04    149 0.01783051 0.61657 0.042546
141 3.6959e-04    150 0.01744404 0.61985 0.042911
142 3.3035e-04    151 0.01707446 0.62146 0.043372
143 3.0799e-04    152 0.01674411 0.62258 0.043333
144 3.0672e-04    153 0.01643612 0.62274 0.043330
145 3.0672e-04    154 0.01612940 0.62274 0.043330
146 3.0672e-04    155 0.01582268 0.62274 0.043330
147 3.0544e-04    156 0.01551596 0.62274 0.043330
148 3.0094e-04    157 0.01521052 0.62395 0.043352
149 2.9757e-04    158 0.01490958 0.62467 0.043363
150 2.8981e-04    159 0.01461201 0.62274 0.043380
151 2.8923e-04    160 0.01432220 0.62270 0.043354
152 2.8782e-04    161 0.01403296 0.62400 0.043499
153 2.8635e-04    162 0.01374515 0.62400 0.043499
154 2.8189e-04    163 0.01345879 0.62219 0.043487
155 2.8173e-04    164 0.01317690 0.62253 0.043478
156 2.6988e-04    165 0.01289517 0.62531 0.043675
157 2.6283e-04    166 0.01262530 0.62452 0.043671
158 2.5737e-04    167 0.01236246 0.62258 0.043382
159 2.5139e-04    168 0.01210509 0.62028 0.043359
160 2.5003e-04    169 0.01185370 0.61871 0.043243
161 2.3771e-04    170 0.01160367 0.61747 0.043178
162 2.3512e-04    171 0.01136596 0.61853 0.043182
163 2.2600e-04    172 0.01113084 0.61800 0.043165
164 2.1796e-04    173 0.01090483 0.61542 0.043149
165 2.1590e-04    174 0.01068688 0.61466 0.043133
166 2.1121e-04    175 0.01047098 0.61339 0.043099
167 2.0973e-04    176 0.01025977 0.61238 0.043036
168 2.0949e-04    178 0.00984031 0.61238 0.043036
169 2.0779e-04    179 0.00963081 0.61220 0.043040
170 2.0120e-04    180 0.00942302 0.61280 0.043025
171 2.0025e-04    181 0.00922182 0.61280 0.043025
172 1.9247e-04    182 0.00902157 0.61353 0.043060
173 1.8668e-04    183 0.00882910 0.61383 0.043085
174 1.7976e-04    184 0.00864242 0.61349 0.043056
175 1.6630e-04    185 0.00846266 0.61532 0.043131
176 1.6596e-04    186 0.00829637 0.61615 0.043142
177 1.6594e-04    187 0.00813041 0.61615 0.043142
178 1.6347e-04    188 0.00796447 0.61623 0.043140
179 1.6290e-04    189 0.00780100 0.61623 0.043140
180 1.5712e-04    190 0.00763810 0.61644 0.043133
181 1.5619e-04    191 0.00748098 0.61562 0.043119
182 1.5210e-04    192 0.00732479 0.61504 0.043100
183 1.4745e-04    193 0.00717270 0.61484 0.043106
184 1.4354e-04    194 0.00702525 0.61434 0.043095
185 1.3883e-04    195 0.00688171 0.61338 0.043090
186 1.3883e-04    196 0.00674288 0.61357 0.043102
187 1.3613e-04    197 0.00660405 0.61349 0.043104
188 1.3589e-04    198 0.00646792 0.61374 0.043123
189 1.3299e-04    199 0.00633203 0.61225 0.043031
190 1.3241e-04    200 0.00619904 0.61244 0.043040
191 1.3011e-04    201 0.00606664 0.61182 0.043038
192 1.2674e-04    202 0.00593652 0.61207 0.043052
193 1.2674e-04    203 0.00580978 0.61250 0.043040
194 1.2167e-04    204 0.00568304 0.61345 0.043093
195 1.2167e-04    205 0.00556136 0.61264 0.043092
196 1.2105e-04    206 0.00543969 0.61264 0.043092
197 1.1352e-04    207 0.00531864 0.61255 0.043081
198 1.0898e-04    208 0.00520512 0.61236 0.043083
199 1.0860e-04    209 0.00509614 0.61259 0.043076
200 1.0592e-04    210 0.00498754 0.61259 0.043076
201 1.0265e-04    211 0.00488162 0.61474 0.043307
202 9.6794e-05    212 0.00477896 0.61439 0.043163
203 9.5532e-05    213 0.00468217 0.61428 0.043175
204 9.4042e-05    214 0.00458664 0.61450 0.043168
205 9.1257e-05    215 0.00449260 0.61509 0.043172
206 9.0753e-05    216 0.00440134 0.61548 0.043188
207 8.9624e-05    217 0.00431059 0.61573 0.043180
208 8.8270e-05    218 0.00422096 0.61566 0.043182
209 8.7486e-05    219 0.00413269 0.61545 0.043188
210 8.3729e-05    220 0.00404521 0.61466 0.043180
211 8.1451e-05    221 0.00396148 0.61426 0.043159
212 7.9204e-05    222 0.00388003 0.61366 0.043149
213 7.7471e-05    224 0.00372162 0.61346 0.043143
214 7.6989e-05    225 0.00364415 0.61346 0.043143
215 7.4805e-05    227 0.00349017 0.61288 0.043144
216 7.2925e-05    228 0.00341536 0.61356 0.043182
217 7.2160e-05    229 0.00334244 0.61239 0.043184
218 7.1694e-05    230 0.00327028 0.61239 0.043184
219 6.9264e-05    231 0.00319859 0.61315 0.043444
220 6.8065e-05    232 0.00312932 0.61316 0.043445
221 6.8065e-05    233 0.00306126 0.61363 0.043442
222 6.7977e-05    234 0.00299319 0.61363 0.043442
223 6.6383e-05    235 0.00292522 0.61341 0.043441
224 6.6383e-05    236 0.00285883 0.61341 0.043441
225 6.6383e-05    237 0.00279245 0.61341 0.043441
226 6.6203e-05    238 0.00272607 0.61341 0.043441
227 6.5697e-05    239 0.00265986 0.61341 0.043441
228 6.5373e-05    240 0.00259417 0.61341 0.043441
229 6.4356e-05    241 0.00252879 0.61327 0.043444
230 6.3372e-05    242 0.00246444 0.61244 0.043314
231 6.2228e-05    243 0.00240107 0.61268 0.043307
232 6.2225e-05    244 0.00233884 0.61268 0.043307
233 6.0397e-05    245 0.00227661 0.61266 0.043308
234 5.8464e-05    246 0.00221622 0.61302 0.043300
235 5.8137e-05    248 0.00209929 0.61288 0.043302
236 5.4694e-05    249 0.00204115 0.61454 0.043304
237 5.2855e-05    251 0.00193176 0.61433 0.043357
238 5.1331e-05    252 0.00187891 0.61341 0.043318
239 5.1048e-05    253 0.00182758 0.61222 0.043262
240 4.9324e-05    255 0.00172548 0.61222 0.043262
241 4.9278e-05    256 0.00167616 0.61214 0.043261
242 4.9278e-05    257 0.00162688 0.61214 0.043261
243 4.9273e-05    258 0.00157760 0.61214 0.043261
244 4.5298e-05    259 0.00152833 0.61225 0.043257
245 4.3577e-05    260 0.00148303 0.61182 0.043250
246 4.3370e-05    261 0.00143945 0.61153 0.043256
247 4.2422e-05    262 0.00139608 0.61153 0.043256
248 4.0867e-05    263 0.00135366 0.61087 0.043246
249 3.9280e-05    264 0.00131279 0.61059 0.043234
250 3.7840e-05    265 0.00127351 0.61039 0.043239
251 3.7840e-05    266 0.00123567 0.61013 0.043231
252 3.7840e-05    267 0.00119783 0.61013 0.043231
253 3.6955e-05    268 0.00115999 0.61040 0.043226
254 3.5847e-05    269 0.00112304 0.61040 0.043226
255 3.5216e-05    270 0.00108719 0.61040 0.043226
256 3.4708e-05    271 0.00105197 0.61069 0.043225
257 3.4032e-05    272 0.00101727 0.61083 0.043221
258 3.3519e-05    273 0.00098323 0.61077 0.043223
259 3.3247e-05    274 0.00094971 0.61077 0.043223
260 2.9981e-05    275 0.00091647 0.61135 0.043216
261 2.9052e-05    276 0.00088649 0.61163 0.043224
262 2.7245e-05    277 0.00085744 0.61167 0.043189
263 2.5663e-05    278 0.00083019 0.61090 0.043097
264 2.5663e-05    279 0.00080453 0.61090 0.043097
265 2.2814e-05    280 0.00077886 0.61105 0.043094
266 2.2688e-05    281 0.00075605 0.61153 0.043120
267 2.2128e-05    282 0.00073336 0.61091 0.043128
268 2.1877e-05    283 0.00071123 0.61118 0.043154
269 2.1510e-05    284 0.00068936 0.61118 0.043154
270 2.0132e-05    285 0.00066785 0.61155 0.043169
271 2.0132e-05    286 0.00064772 0.61209 0.043176
272 1.8231e-05    287 0.00062758 0.61222 0.043173
273 1.8163e-05    288 0.00060935 0.61289 0.043189
274 1.7618e-05    289 0.00059119 0.61289 0.043189
275 1.7618e-05    290 0.00057357 0.61289 0.043189
276 1.7608e-05    291 0.00055595 0.61289 0.043189
277 1.7110e-05    292 0.00053834 0.61261 0.043152
278 1.5272e-05    293 0.00052123 0.61323 0.043154
279 1.5099e-05    294 0.00050596 0.61262 0.043101
280 1.4162e-05    296 0.00047576 0.61252 0.043100
281 1.4162e-05    297 0.00046160 0.61237 0.043096
282 1.4141e-05    298 0.00044744 0.61237 0.043096
283 1.4141e-05    300 0.00041916 0.61237 0.043096
284 1.3214e-05    301 0.00040502 0.61212 0.043066
285 1.3214e-05    302 0.00039180 0.61186 0.043063
286 1.3093e-05    303 0.00037859 0.61186 0.043063
287 1.2318e-05    304 0.00036550 0.61172 0.043054
288 1.2318e-05    305 0.00035318 0.61141 0.043044
289 1.1454e-05    306 0.00034086 0.61140 0.043044
290 1.1082e-05    307 0.00032941 0.61149 0.043050
291 1.0621e-05    308 0.00031832 0.61200 0.043160
292 1.0000e-05    312 0.00027584 0.61200 0.043160</code></pre>
<div class="corR">
<p>
<p>On obtient ici une suite de près de 300 arbres. On remarque que</p>
</p>
<ul>
<li>
<p>l’erreur d’ajustement ne cesse de décroitre, ceci est logique vu le procédé de construction : on ajuste de mieux en mieux lorsqu’on augmente le nombre de coupures ;</p>
</li>
<li>
<p>l’erreur de prévision décroit avant de d’augmenter à nouveau. C’est le phénomène bien connu du <strong>sur-apprentissage</strong>.</p>
</li>
</ul>
</div></li>
<li><p>Expliquer la sortie de la fonction <strong>plotcp</strong> appliquée à l’arbre de la question précédente.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="arbres.html#cb98-1"></a><span class="kw">plotcp</span>(tree1)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-148-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="corR">
<p>
On obtient un graphe qui permet de visualiser l’erreur quadratique calculée par validation croisée (erreur de prévision) en fonction du paramètre <strong>cp</strong> ou <strong>nsplit</strong>.
</p>
</div></li>
<li><p>Sélectionner le “meilleur” arbre dans la suite construite.</p>
<div class="corR">
<p>
La manière classique revient à choisir l’arbre qui a la plus petite erreur de prévision. Cela revient à aller chercher dans le tableau de la fonction <strong>printcp</strong> l’arbre qiu possède la plus petite erreur de prévision. On peut obtenir la valeur optimale de <strong>cp</strong> avec
</p>
</div>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="arbres.html#cb99-1"></a>cp_opt &lt;-<span class="st"> </span>tree1<span class="op">$</span>cptable <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(xerror<span class="op">==</span><span class="kw">min</span>(xerror)) <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(CP) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.numeric</span>()</span>
<span id="cb99-2"><a href="arbres.html#cb99-2"></a>cp_opt</span>
<span id="cb99-3"><a href="arbres.html#cb99-3"></a>[<span class="dv">1</span>] <span class="fl">0.002369349</span></span></code></pre></div></li>
<li><p>Visualiser l’arbre choisi (utiliser la fonction <strong>prune</strong>).</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="arbres.html#cb100-1"></a>tree_opt &lt;-<span class="st"> </span><span class="kw">prune</span>(tree,<span class="dt">cp=</span>cp_opt)</span>
<span id="cb100-2"><a href="arbres.html#cb100-2"></a><span class="kw">rpart.plot</span>(tree_opt)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-152-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="corR">
<p>
La fonction <strong>visTree</strong> du package <strong>visNetwork</strong> permet de donner une visualisation interactive de l’arbre.
</p>
</div>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="arbres.html#cb101-1"></a><span class="kw">library</span>(visNetwork)</span>
<span id="cb101-2"><a href="arbres.html#cb101-2"></a><span class="kw">visTree</span>(tree_opt)</span></code></pre></div>
<div id="htmlwidget-cbf142802bb0a0a3728c" style="width:100%;height:600px;" class="visNetwork html-widget"></div>
    <script type="application/json" data-for="htmlwidget-cbf142802bb0a0a3728c">{"x":{"nodes":{"id":[1,2,4,8,16,17,9,18,36,37,19,38,39,78,79,5,10,20,40,41,21,11,22,23,46,47,3,6,12,24,25,50,51,13,7],"label":["ShelveLoc","Price","ShelveLoc","Population","3.767","5.386","Advertising","Price","4.987","6.627","CompPrice","6.23","Price","6.732","8.93","Age","Income","ShelveLoc","5.787","7.59","10.081","Income","7.988","ShelveLoc","8.397","10.73","Price","Advertising","Price","7.152","Income","7.603","9.829","11.921","12.188"],"level":[1,2,3,4,5,5,4,5,6,6,5,6,6,7,7,3,4,5,6,6,5,4,5,5,6,6,2,3,4,5,5,6,6,4,3],"color":["#F1B8C2","#E6BFA6","#F1B8C2","#CBC999","#E6E0F8","#D4B5EA","#A9D1A5","#E6BFA6","#D8C0ED","#C695DF","#8DD4C1","#CA9FE3","#E6BFA6","#C592DF","#AC59CC","#91D1DD","#B4C7ED","#F1B8C2","#CFABE6","#BB7CD7","#A03BC2","#B4C7ED","#B771D4","#F1B8C2","#B267D0","#992ABD","#E6BFA6","#A9D1A5","#E6BFA6","#C087DB","#B4C7ED","#BB7BD7","#A341C4","#8B0AB3","#8904B1"],"value":[400,315,207,61,25,36,146,77,34,43,69,19,50,28,22,108,65,56,20,36,9,43,13,30,9,21,85,57,48,12,36,9,27,9,28],"shape":["dot","dot","dot","dot","square","square","dot","dot","square","square","dot","square","dot","square","square","dot","dot","dot","square","square","square","dot","square","dot","square","square","dot","dot","dot","square","dot","square","square","square","square"],"title":["<div style=\"text-align:center;\">N : <b>100%<\/b> (400)<br>Complexity : <b>0.251<\/b><br>Mean : <b>7.496<\/b><br>Variance : <b>7.976<\/b><\/div>","<div style=\"text-align:center;\">N : <b>78.7%<\/b> (315)<br>Complexity : <b>0.105<\/b><br>Mean : <b>6.763<\/b><br>Variance : <b>5.922<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Bad, Medium<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>51.7%<\/b> (207)<br>Complexity : <b>0.046<\/b><br>Mean : <b>6.019<\/b><br>Variance : <b>4.644<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Bad, Medium<br><b> Price <\/b> >= 105.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>15.2%<\/b> (61)<br>Complexity : <b>0.012<\/b><br>Mean : <b>4.722<\/b><br>Variance : <b>4.014<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Bad<br><b> Price <\/b> >= 105.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>6.2%<\/b> (25)<br>Complexity : <b>0.007<\/b><br>Mean : <b>3.767<\/b><br>Variance : <b>3.676<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Bad<br><b> Price <\/b> >= 105.5<br><b> Population <\/b> < 196.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>9%<\/b> (36)<br>Complexity : <b>0.008<\/b><br>Mean : <b>5.386<\/b><br>Variance : <b>3.255<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Bad<br><b> Price <\/b> >= 105.5<br><b> Population <\/b> >= 196.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>36.5%<\/b> (146)<br>Complexity : <b>0.022<\/b><br>Mean : <b>6.56<\/b><br>Variance : <b>3.934<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Medium<br><b> Price <\/b> >= 105.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>19.2%<\/b> (77)<br>Complexity : <b>0.016<\/b><br>Mean : <b>5.902<\/b><br>Variance : <b>3.686<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Medium<br><b> Price <\/b> >= 105.5<br><b> Advertising <\/b> < 5.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>8.5%<\/b> (34)<br>Complexity : <b>0.01<\/b><br>Mean : <b>4.987<\/b><br>Variance : <b>4.047<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Medium<br><b> Price <\/b> >= 127<br><b> Advertising <\/b> < 5.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>10.8%<\/b> (43)<br>Complexity : <b>0.009<\/b><br>Mean : <b>6.627<\/b><br>Variance : <b>2.274<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Medium<br><b>  <\/b> 105.5 <= <b>Price<\/b> < 127<br><b> Advertising <\/b> < 5.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>17.2%<\/b> (69)<br>Complexity : <b>0.014<\/b><br>Mean : <b>7.295<\/b><br>Variance : <b>3.232<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Medium<br><b> Price <\/b> >= 105.5<br><b> Advertising <\/b> >= 5.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>4.8%<\/b> (19)<br>Complexity : <b>0.01<\/b><br>Mean : <b>6.23<\/b><br>Variance : <b>2.241<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Medium<br><b> Price <\/b> >= 105.5<br><b> Advertising <\/b> >= 5.5<br><b> CompPrice <\/b> < 121.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>12.5%<\/b> (50)<br>Complexity : <b>0.014<\/b><br>Mean : <b>7.699<\/b><br>Variance : <b>3.055<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Medium<br><b> Price <\/b> >= 105.5<br><b> Advertising <\/b> >= 5.5<br><b> CompPrice <\/b> >= 121.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>7%<\/b> (28)<br>Complexity : <b>0.006<\/b><br>Mean : <b>6.732<\/b><br>Variance : <b>2.666<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Medium<br><b> Price <\/b> >= 127<br><b> Advertising <\/b> >= 5.5<br><b> CompPrice <\/b> >= 121.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>5.5%<\/b> (22)<br>Complexity : <b>0.01<\/b><br>Mean : <b>8.93<\/b><br>Variance : <b>0.865<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Medium<br><b>  <\/b> 105.5 <= <b>Price<\/b> < 127<br><b> Advertising <\/b> >= 5.5<br><b> CompPrice <\/b> >= 121.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>27%<\/b> (108)<br>Complexity : <b>0.034<\/b><br>Mean : <b>8.189<\/b><br>Variance : <b>5.314<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Bad, Medium<br><b> Price <\/b> < 105.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>16.2%<\/b> (65)<br>Complexity : <b>0.024<\/b><br>Mean : <b>7.38<\/b><br>Variance : <b>4.735<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Bad, Medium<br><b> Price <\/b> < 105.5<br><b> Age <\/b> >= 54.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>14%<\/b> (56)<br>Complexity : <b>0.013<\/b><br>Mean : <b>6.946<\/b><br>Variance : <b>3.692<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Bad, Medium<br><b> Price <\/b> < 105.5<br><b> Age <\/b> >= 54.5<br><b> Income <\/b> < 105.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>5%<\/b> (20)<br>Complexity : <b>0.01<\/b><br>Mean : <b>5.787<\/b><br>Variance : <b>4.051<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Bad<br><b> Price <\/b> < 105.5<br><b> Age <\/b> >= 54.5<br><b> Income <\/b> < 105.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>9%<\/b> (36)<br>Complexity : <b>0.007<\/b><br>Mean : <b>7.59<\/b><br>Variance : <b>2.407<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Medium<br><b> Price <\/b> < 105.5<br><b> Age <\/b> >= 54.5<br><b> Income <\/b> < 105.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>2.2%<\/b> (9)<br>Complexity : <b>0.01<\/b><br>Mean : <b>10.081<\/b><br>Variance : <b>2.977<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Bad, Medium<br><b> Price <\/b> < 105.5<br><b> Age <\/b> >= 54.5<br><b> Income <\/b> >= 105.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>10.8%<\/b> (43)<br>Complexity : <b>0.012<\/b><br>Mean : <b>9.413<\/b><br>Variance : <b>3.778<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Bad, Medium<br><b> Price <\/b> < 105.5<br><b> Age <\/b> < 54.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>3.2%<\/b> (13)<br>Complexity : <b>0.01<\/b><br>Mean : <b>7.988<\/b><br>Variance : <b>1.604<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Bad, Medium<br><b> Price <\/b> < 105.5<br><b> Age <\/b> < 54.5<br><b> Income <\/b> < 57.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>7.5%<\/b> (30)<br>Complexity : <b>0.011<\/b><br>Mean : <b>10.03<\/b><br>Variance : <b>3.503<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Bad, Medium<br><b> Price <\/b> < 105.5<br><b> Age <\/b> < 54.5<br><b> Income <\/b> >= 57.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>2.2%<\/b> (9)<br>Complexity : <b>0.01<\/b><br>Mean : <b>8.397<\/b><br>Variance : <b>2.845<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Bad<br><b> Price <\/b> < 105.5<br><b> Age <\/b> < 54.5<br><b> Income <\/b> >= 57.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>5.2%<\/b> (21)<br>Complexity : <b>0.005<\/b><br>Mean : <b>10.73<\/b><br>Variance : <b>2.227<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Medium<br><b> Price <\/b> < 105.5<br><b> Age <\/b> < 54.5<br><b> Income <\/b> >= 57.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>21.2%<\/b> (85)<br>Complexity : <b>0.051<\/b><br>Mean : <b>10.214<\/b><br>Variance : <b>6.256<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Good<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>14.2%<\/b> (57)<br>Complexity : <b>0.024<\/b><br>Mean : <b>9.244<\/b><br>Variance : <b>4.951<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Good<br><b> Price <\/b> >= 109.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>12%<\/b> (48)<br>Complexity : <b>0.013<\/b><br>Mean : <b>8.743<\/b><br>Variance : <b>3.945<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Good<br><b> Price <\/b> >= 109.5<br><b> Advertising <\/b> < 13.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>3%<\/b> (12)<br>Complexity : <b>0.01<\/b><br>Mean : <b>7.152<\/b><br>Variance : <b>3.332<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Good<br><b> Price <\/b> >= 142.5<br><b> Advertising <\/b> < 13.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>9%<\/b> (36)<br>Complexity : <b>0.011<\/b><br>Mean : <b>9.273<\/b><br>Variance : <b>3.095<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Good<br><b>  <\/b> 109.5 <= <b>Price<\/b> < 142.5<br><b> Advertising <\/b> < 13.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>2.2%<\/b> (9)<br>Complexity : <b>0.01<\/b><br>Mean : <b>7.603<\/b><br>Variance : <b>1.228<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Good<br><b>  <\/b> 109.5 <= <b>Price<\/b> < 142.5<br><b> Advertising <\/b> < 13.5<br><b> Income <\/b> < 40.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>6.8%<\/b> (27)<br>Complexity : <b>0.005<\/b><br>Mean : <b>9.829<\/b><br>Variance : <b>2.502<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Good<br><b>  <\/b> 109.5 <= <b>Price<\/b> < 142.5<br><b> Advertising <\/b> < 13.5<br><b> Income <\/b> >= 40.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>2.2%<\/b> (9)<br>Complexity : <b>0.01<\/b><br>Mean : <b>11.921<\/b><br>Variance : <b>1.909<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Good<br><b> Price <\/b> >= 109.5<br><b> Advertising <\/b> >= 13.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>","<div style=\"text-align:center;\">N : <b>7%<\/b> (28)<br>Complexity : <b>0.007<\/b><br>Mean : <b>12.188<\/b><br>Variance : <b>3.17<\/b><hr class = \"rPartvisNetwork\">\n<div class =\"showOnMe2\"><div style=\"text-align:center;\"><U style=\"color:blue;\" class = \"classActivePointer\">Rules<\/U><\/div>\n<div class=\"showMeRpartTTp2\" style=\"display:none;\">\n<b> ShelveLoc <\/b> Good<br><b> Price <\/b> < 109.5<\/script><script type=\"text/javascript\">$(document).ready(function(){\n$(\".showOnMe2\").click(function(){\n$(\".showMeRpartTTp2\").toggle();\n$.sparkline_display_visible();\n});\n  });<\/script><\/div><\/div>\n\n<\/div>"],"fixed":[true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true],"colorClust":["#BC7ED8","#C491DE","#CDA5E5","#DBC7EF","#E6E0F8","#D4B5EA","#C797E0","#CEA8E5","#D8C0ED","#C695DF","#BF83DA","#CA9FE3","#BA79D6","#C592DF","#AC59CC","#B56CD2","#BE81D9","#C28CDD","#CFABE6","#BB7CD7","#A03BC2","#A74CC8","#B771D4","#A03CC3","#B267D0","#992ABD","#9E37C1","#A950C9","#AF5ECE","#C087DB","#A950C9","#BB7BD7","#A341C4","#8B0AB3","#8904B1"],"labelClust":[7.496,6.763,6.019,4.722,3.767,5.386,6.56,5.902,4.987,6.627,7.295,6.23,7.699,6.732,8.93,8.189,7.38,6.946,5.787,7.59,10.081,9.413,7.988,10.03,8.397,10.73,10.214,9.244,8.743,7.152,9.273,7.603,9.829,11.921,12.188],"Leaf":[0,0,0,0,1,1,0,0,1,1,0,1,0,1,1,0,0,0,1,1,1,0,1,0,1,1,0,0,0,1,0,1,1,1,1],"font.size":[16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16],"scaling.min":[22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5],"scaling.max":[22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5,22.5]},"edges":{"id":["edge1","edge2","edge3","edge4","edge5","edge6","edge7","edge8","edge9","edge10","edge11","edge12","edge13","edge14","edge15","edge16","edge17","edge18","edge19","edge20","edge21","edge22","edge23","edge24","edge25","edge26","edge27","edge28","edge29","edge30","edge31","edge32","edge33","edge34"],"from":[1,2,4,8,8,4,9,18,18,9,19,19,39,39,2,5,10,20,20,10,5,11,11,23,23,1,3,6,12,12,25,25,6,3],"to":[2,4,8,16,17,9,18,36,37,19,38,39,78,79,5,10,20,40,41,21,11,22,23,46,47,3,6,12,24,25,50,51,13,7],"label":["Bad, Me...",">= 105.5","Bad","< 196.5",">= 196.5","Medium","< 5.5",">= 127","< 127",">= 5.5","< 121.5",">= 121.5",">= 127","< 127","< 105.5",">= 54.5","< 105.5","Bad","Medium",">= 105.5","< 54.5","< 57.5",">= 57.5","Bad","Medium","Good",">= 109.5","< 13.5",">= 142.5","< 142.5","< 40.5",">= 40.5",">= 13.5","< 109.5"],"value":[315,207,61,25,36,146,77,34,43,69,19,50,28,22,108,65,56,20,36,9,43,13,30,9,21,85,57,48,12,36,9,27,9,28],"title":["<div style=\"text-align:center;\"><b>ShelveLoc<\/b><\/div><div style=\"text-align:center;\">Bad<\/div><div style=\"text-align:center;\">Medium<\/div>","<div style=\"text-align:center;\"><b>Price<\/b><\/div><div style=\"text-align:center;\">>=105.5<\/div>","<div style=\"text-align:center;\"><b>ShelveLoc<\/b><\/div><div style=\"text-align:center;\">Bad<\/div>","<div style=\"text-align:center;\"><b>Population<\/b><\/div><div style=\"text-align:center;\"><196.5<\/div>","<div style=\"text-align:center;\"><b>Population<\/b><\/div><div style=\"text-align:center;\">>=196.5<\/div>","<div style=\"text-align:center;\"><b>ShelveLoc<\/b><\/div><div style=\"text-align:center;\">Medium<\/div>","<div style=\"text-align:center;\"><b>Advertising<\/b><\/div><div style=\"text-align:center;\"><5.5<\/div>","<div style=\"text-align:center;\"><b>Price<\/b><\/div><div style=\"text-align:center;\">>=127<\/div>","<div style=\"text-align:center;\"><b>Price<\/b><\/div><div style=\"text-align:center;\"><127<\/div>","<div style=\"text-align:center;\"><b>Advertising<\/b><\/div><div style=\"text-align:center;\">>=5.5<\/div>","<div style=\"text-align:center;\"><b>CompPrice<\/b><\/div><div style=\"text-align:center;\"><121.5<\/div>","<div style=\"text-align:center;\"><b>CompPrice<\/b><\/div><div style=\"text-align:center;\">>=121.5<\/div>","<div style=\"text-align:center;\"><b>Price<\/b><\/div><div style=\"text-align:center;\">>=127<\/div>","<div style=\"text-align:center;\"><b>Price<\/b><\/div><div style=\"text-align:center;\"><127<\/div>","<div style=\"text-align:center;\"><b>Price<\/b><\/div><div style=\"text-align:center;\"><105.5<\/div>","<div style=\"text-align:center;\"><b>Age<\/b><\/div><div style=\"text-align:center;\">>=54.5<\/div>","<div style=\"text-align:center;\"><b>Income<\/b><\/div><div style=\"text-align:center;\"><105.5<\/div>","<div style=\"text-align:center;\"><b>ShelveLoc<\/b><\/div><div style=\"text-align:center;\">Bad<\/div>","<div style=\"text-align:center;\"><b>ShelveLoc<\/b><\/div><div style=\"text-align:center;\">Medium<\/div>","<div style=\"text-align:center;\"><b>Income<\/b><\/div><div style=\"text-align:center;\">>=105.5<\/div>","<div style=\"text-align:center;\"><b>Age<\/b><\/div><div style=\"text-align:center;\"><54.5<\/div>","<div style=\"text-align:center;\"><b>Income<\/b><\/div><div style=\"text-align:center;\"><57.5<\/div>","<div style=\"text-align:center;\"><b>Income<\/b><\/div><div style=\"text-align:center;\">>=57.5<\/div>","<div style=\"text-align:center;\"><b>ShelveLoc<\/b><\/div><div style=\"text-align:center;\">Bad<\/div>","<div style=\"text-align:center;\"><b>ShelveLoc<\/b><\/div><div style=\"text-align:center;\">Medium<\/div>","<div style=\"text-align:center;\"><b>ShelveLoc<\/b><\/div><div style=\"text-align:center;\">Good<\/div>","<div style=\"text-align:center;\"><b>Price<\/b><\/div><div style=\"text-align:center;\">>=109.5<\/div>","<div style=\"text-align:center;\"><b>Advertising<\/b><\/div><div style=\"text-align:center;\"><13.5<\/div>","<div style=\"text-align:center;\"><b>Price<\/b><\/div><div style=\"text-align:center;\">>=142.5<\/div>","<div style=\"text-align:center;\"><b>Price<\/b><\/div><div style=\"text-align:center;\"><142.5<\/div>","<div style=\"text-align:center;\"><b>Income<\/b><\/div><div style=\"text-align:center;\"><40.5<\/div>","<div style=\"text-align:center;\"><b>Income<\/b><\/div><div style=\"text-align:center;\">>=40.5<\/div>","<div style=\"text-align:center;\"><b>Advertising<\/b><\/div><div style=\"text-align:center;\">>=13.5<\/div>","<div style=\"text-align:center;\"><b>Price<\/b><\/div><div style=\"text-align:center;\"><109.5<\/div>"],"color":["#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7","#8181F7"],"font.size":[14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14],"font.align":["horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal","horizontal"],"smooth.enabled":[true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true,true],"smooth.type":["cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier","cubicBezier"],"smooth.roundness":[0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5]},"nodesToDataframe":true,"edgesToDataframe":true,"options":{"width":"100%","height":"100%","nodes":{"shape":"dot"},"manipulation":{"enabled":false},"layout":{"hierarchical":{"enabled":true,"direction":"UD"}},"interaction":{"dragNodes":false,"selectConnectedEdges":false,"tooltipDelay":500},"edges":{"scaling":{"label":{"enabled":false}}}},"groups":null,"width":"100%","height":"600px","idselection":{"enabled":false,"style":"width: 150px; height: 26px","useLabels":true,"main":"Select by id"},"byselection":{"enabled":false,"style":"width: 150px; height: 26px","multiple":false,"hideColor":"rgba(200,200,200,0.5)","highlight":false},"main":{"text":"","style":"font-family:Georgia, Times New Roman, Times, serif;font-weight:bold;font-size:20px;text-align:center;"},"submain":{"text":"","style":"font-family:Georgia, Times New Roman, Times, serif;font-size:12px;text-align:center;"},"footer":{"text":"","style":"font-family:Georgia, Times New Roman, Times, serif;font-size:12px;text-align:center;"},"background":"rgba(0, 0, 0, 0)","highlight":{"enabled":true,"hoverNearest":false,"degree":{"from":50000,"to":0},"algorithm":"hierarchical","hideColor":"rgba(200,200,200,0.5)","labelOnly":true},"collapse":{"enabled":true,"fit":true,"resetHighlight":true,"clusterOptions":{"fixed":true,"physics":false},"keepCoord":true,"labelSuffix":"(cluster)"},"tooltipStay":300,"tooltipStyle":"position: fixed;visibility:hidden;padding: 5px;\n                      white-space: nowrap;\n                      font-family: cursive;font-size:12px;font-color:purple;background-color: #E6E6E6;\n                      border-radius: 15px;","OnceEvents":{"stabilized":"function() { \n        this.setOptions({layout:{hierarchical:false}, physics:{solver:'barnesHut', enabled:true, stabilization : false}, nodes : {physics : false, fixed : true}});\n    }"},"legend":{"width":0.1,"useGroups":false,"position":"left","ncol":1,"stepX":100,"stepY":100,"zoom":true,"nodes":{"label":["Advertising","Age","CompPrice","Income","Population","Price","ShelveLoc"],"color":["#A9D1A5","#91D1DD","#8DD4C1","#B4C7ED","#CBC999","#E6BFA6","#F1B8C2"],"shape":["dot","dot","dot","dot","dot","dot","dot"],"size":[22,22,22,22,22,22,22],"Leaf":[0,0,0,0,0,0,0],"font.size":[16,16,16,16,16,16,16],"id":[10000,10001,10002,10003,10004,10005,10006]},"nodesToDataframe":true},"tree":{"updateShape":true,"shapeVar":"dot","shapeY":"square","colorVar":{"variable":["ShelveLoc","Price","Population","Advertising","CompPrice","Age","Income"],"color":["#F1B8C2","#E6BFA6","#CBC999","#A9D1A5","#8DD4C1","#91D1DD","#B4C7ED"]},"colorY":{"colRamp":["function (x) ","roundcolor(cbind(palette[[1L]](x), palette[[2L]](x), palette[[3L]](x), ","    if (alpha) palette[[4L]](x))) * 255"],"colorTerm":["#BC7ED8","#C491DE","#CDA5E5","#DBC7EF","#E6E0F8","#D4B5EA","#C797E0","#CEA8E5","#D8C0ED","#C695DF","#BF83DA","#CA9FE3","#BA79D6","#C592DF","#AC59CC","#B56CD2","#BE81D9","#C28CDD","#CFABE6","#BB7CD7","#A03BC2","#A74CC8","#B771D4","#A03CC3","#B267D0","#992ABD","#9E37C1","#A950C9","#AF5ECE","#C087DB","#A950C9","#BB7BD7","#A341C4","#8B0AB3","#8904B1"],"colorY":["#E6E0F8","#8904B1"],"vardecidedClust":[7.496,6.763,6.019,4.722,3.767,5.386,6.56,5.902,4.987,6.627,7.295,6.23,7.699,6.732,8.93,8.189,7.38,6.946,5.787,7.59,10.081,9.413,7.988,10.03,8.397,10.73,10.214,9.244,8.743,7.152,9.273,7.603,9.829,11.921,12.188]}},"export":{"type":"png","css":"float:right;-webkit-border-radius: 10;\n                  -moz-border-radius: 10;\n                  border-radius: 10px;\n                  font-family: Arial;\n                  color: #ffffff;\n                  font-size: 12px;\n                  background: #090a0a;\n                  padding: 4px 8px 4px 4px;\n                  text-decoration: none;","background":"#fff","name":"network.png","label":"Export as png"}},"evals":["OnceEvents.stabilized"],"jsHooks":[]}</script>
<div class="corR">
<p>
Une <strong>application Shiny</strong> est également proposée pour visualiser les arbres
</p>
</div>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="arbres.html#cb102-1"></a><span class="kw">visTreeEditor</span>(Carseats)</span></code></pre></div></li>
<li><p>On souhaite prédire les valeurs de <span class="math inline">\(Y\)</span> pour de nouveaux individus à partir de l’arbre sélectionné. Pour simplifier on considèrera ces 4 individus :</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="arbres.html#cb103-1"></a>new_ind &lt;-<span class="st"> </span>Carseats <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="dv">3</span>,<span class="dv">58</span>,<span class="dv">185</span>,<span class="dv">218</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>Sales)</span>
<span id="cb103-2"><a href="arbres.html#cb103-2"></a>new_ind</span>
<span id="cb103-3"><a href="arbres.html#cb103-3"></a>    CompPrice Income Advertising Population Price ShelveLoc</span>
<span id="cb103-4"><a href="arbres.html#cb103-4"></a><span class="dv">3</span>         <span class="dv">113</span>     <span class="dv">35</span>          <span class="dv">10</span>        <span class="dv">269</span>    <span class="dv">80</span>    Medium</span>
<span id="cb103-5"><a href="arbres.html#cb103-5"></a><span class="dv">58</span>         <span class="dv">93</span>     <span class="dv">91</span>           <span class="dv">0</span>         <span class="dv">22</span>   <span class="dv">117</span>       Bad</span>
<span id="cb103-6"><a href="arbres.html#cb103-6"></a><span class="dv">185</span>       <span class="dv">132</span>     <span class="dv">33</span>           <span class="dv">7</span>         <span class="dv">35</span>    <span class="dv">97</span>    Medium</span>
<span id="cb103-7"><a href="arbres.html#cb103-7"></a><span class="dv">218</span>       <span class="dv">106</span>     <span class="dv">44</span>           <span class="dv">0</span>        <span class="dv">481</span>   <span class="dv">111</span>    Medium</span>
<span id="cb103-8"><a href="arbres.html#cb103-8"></a>    Age Education Urban  US</span>
<span id="cb103-9"><a href="arbres.html#cb103-9"></a><span class="dv">3</span>    <span class="dv">59</span>        <span class="dv">12</span>   Yes Yes</span>
<span id="cb103-10"><a href="arbres.html#cb103-10"></a><span class="dv">58</span>   <span class="dv">75</span>        <span class="dv">11</span>   Yes  No</span>
<span id="cb103-11"><a href="arbres.html#cb103-11"></a><span class="dv">185</span>  <span class="dv">60</span>        <span class="dv">11</span>    No Yes</span>
<span id="cb103-12"><a href="arbres.html#cb103-12"></a><span class="dv">218</span>  <span class="dv">70</span>        <span class="dv">14</span>    No  No</span></code></pre></div>
<p>Calculer les valeurs prédites.</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="arbres.html#cb104-1"></a><span class="kw">predict</span>(tree_opt,<span class="dt">newdata=</span>new_ind)</span>
<span id="cb104-2"><a href="arbres.html#cb104-2"></a>       <span class="dv">3</span>       <span class="dv">58</span>      <span class="dv">185</span>      <span class="dv">218</span> </span>
<span id="cb104-3"><a href="arbres.html#cb104-3"></a><span class="fl">7.590278</span> <span class="fl">3.767200</span> <span class="fl">7.590278</span> <span class="fl">6.626512</span> </span></code></pre></div></li>
<li><p>Séparer les données en un échantillon d’apprentissage de taille 250 et un échantillon test de taille 150.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="arbres.html#cb105-1"></a>n.train &lt;-<span class="st"> </span><span class="dv">250</span></span>
<span id="cb105-2"><a href="arbres.html#cb105-2"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb105-3"><a href="arbres.html#cb105-3"></a>perm &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(Carseats))</span>
<span id="cb105-4"><a href="arbres.html#cb105-4"></a>train &lt;-<span class="st"> </span>Carseats[perm[<span class="dv">1</span><span class="op">:</span>n.train],]</span>
<span id="cb105-5"><a href="arbres.html#cb105-5"></a>test &lt;-<span class="st"> </span>Carseats[<span class="op">-</span>perm[<span class="dv">1</span><span class="op">:</span>n.train],]</span></code></pre></div></li>
<li><p>On considère la suite d’arbres définie par</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="arbres.html#cb106-1"></a><span class="kw">set.seed</span>(<span class="dv">4321</span>)</span>
<span id="cb106-2"><a href="arbres.html#cb106-2"></a>tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(Sales<span class="op">~</span>.,<span class="dt">data=</span>train,<span class="dt">cp=</span><span class="fl">0.000001</span>,<span class="dt">minsplit=</span><span class="dv">2</span>)</span></code></pre></div>
<p>Dans cette suite, sélectionner</p>
<ul>
<li>un arbre très simple (avec 2 ou 3 coupures)</li>
<li>un arbre très grand</li>
<li>l’arbre optimal (avec la procédure d’élagage classique).</li>
</ul>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="arbres.html#cb107-1"></a><span class="kw">printcp</span>(tree)</span></code></pre></div>
<pre style="max-height: 500px;"><code>
Regression tree:
rpart(formula = Sales ~ ., data = train, cp = 1e-06, minsplit = 2)

Variables actually used in tree construction:
 [1] Advertising Age         CompPrice   Education  
 [5] Income      Population  Price       ShelveLoc  
 [9] Urban       US         

Root node error: 1930.1/250 = 7.7203

n= 250 

            CP nsplit  rel error  xerror     xstd
1   2.1456e-01      0 1.0000e+00 1.00199 0.090218
2   9.9792e-02      1 7.8544e-01 0.86595 0.075513
3   5.5822e-02      2 6.8565e-01 0.82348 0.071817
4   5.5012e-02      3 6.2983e-01 0.76327 0.065209
5   4.7593e-02      4 5.7481e-01 0.75011 0.065969
6   3.2780e-02      5 5.2722e-01 0.69666 0.062659
7   3.2081e-02      6 4.9444e-01 0.71690 0.062718
8   2.8747e-02      7 4.6236e-01 0.71639 0.062790
9   2.7988e-02      8 4.3361e-01 0.69569 0.059348
10  1.8568e-02      9 4.0563e-01 0.70266 0.064934
11  1.8305e-02     10 3.8706e-01 0.75844 0.071881
12  1.7705e-02     11 3.6875e-01 0.75332 0.071355
13  1.6028e-02     12 3.5105e-01 0.77667 0.075496
14  1.4152e-02     13 3.3502e-01 0.76655 0.074752
15  1.4119e-02     14 3.2087e-01 0.80299 0.082942
16  1.1545e-02     15 3.0675e-01 0.82136 0.082988
17  1.1033e-02     16 2.9520e-01 0.80368 0.081778
18  1.0407e-02     17 2.8417e-01 0.80205 0.081219
19  9.6380e-03     18 2.7377e-01 0.80105 0.081149
20  9.4448e-03     19 2.6413e-01 0.79823 0.081147
21  9.2825e-03     21 2.4524e-01 0.79320 0.081143
22  8.7958e-03     22 2.3596e-01 0.76562 0.078130
23  8.7574e-03     23 2.2716e-01 0.76141 0.078054
24  7.9616e-03     24 2.1840e-01 0.76444 0.077936
25  7.0728e-03     25 2.1044e-01 0.77626 0.077783
26  7.0288e-03     27 1.9630e-01 0.77706 0.078603
27  6.7205e-03     28 1.8927e-01 0.77012 0.077833
28  6.5421e-03     29 1.8255e-01 0.76052 0.075687
29  6.4728e-03     30 1.7600e-01 0.77588 0.075711
30  5.7670e-03     31 1.6953e-01 0.77776 0.075870
31  5.0693e-03     32 1.6376e-01 0.79694 0.077306
32  4.9069e-03     34 1.5363e-01 0.78951 0.077248
33  4.7845e-03     35 1.4872e-01 0.78954 0.077227
34  4.7623e-03     36 1.4393e-01 0.79479 0.078655
35  4.7423e-03     38 1.3441e-01 0.79479 0.078655
36  4.3579e-03     39 1.2967e-01 0.78334 0.078557
37  4.3530e-03     40 1.2531e-01 0.78187 0.078607
38  4.1413e-03     41 1.2096e-01 0.78700 0.078606
39  4.0455e-03     42 1.1681e-01 0.78351 0.078553
40  3.9302e-03     43 1.1277e-01 0.76858 0.075026
41  3.8957e-03     45 1.0491e-01 0.76858 0.075026
42  3.8803e-03     46 1.0101e-01 0.77054 0.075020
43  3.8596e-03     47 9.7133e-02 0.77054 0.075020
44  3.5520e-03     48 9.3273e-02 0.76480 0.074222
45  3.5181e-03     49 8.9721e-02 0.75251 0.074226
46  3.4216e-03     50 8.6203e-02 0.75336 0.074228
47  3.1866e-03     51 8.2782e-02 0.75473 0.074308
48  3.1193e-03     52 7.9595e-02 0.75621 0.074297
49  3.0949e-03     53 7.6476e-02 0.75747 0.074346
50  2.8538e-03     54 7.3381e-02 0.76454 0.074387
51  2.7245e-03     55 7.0527e-02 0.75920 0.074369
52  2.6778e-03     56 6.7802e-02 0.75964 0.074366
53  2.2840e-03     57 6.5125e-02 0.75580 0.074519
54  2.1373e-03     58 6.2841e-02 0.74584 0.074550
55  2.1338e-03     59 6.0703e-02 0.73895 0.074268
56  1.9958e-03     60 5.8570e-02 0.73653 0.074241
57  1.9324e-03     61 5.6574e-02 0.73663 0.073524
58  1.8577e-03     62 5.4641e-02 0.73454 0.073556
59  1.7446e-03     63 5.2784e-02 0.73777 0.073803
60  1.7300e-03     64 5.1039e-02 0.73668 0.073751
61  1.7199e-03     65 4.9309e-02 0.73848 0.073725
62  1.6642e-03     67 4.5869e-02 0.73425 0.073646
63  1.5818e-03     68 4.4205e-02 0.72999 0.073374
64  1.4176e-03     69 4.2623e-02 0.74065 0.074144
65  1.2535e-03     70 4.1205e-02 0.73915 0.074086
66  1.2528e-03     71 3.9952e-02 0.73727 0.074038
67  1.2241e-03     72 3.8699e-02 0.73623 0.074052
68  1.1710e-03     73 3.7475e-02 0.74071 0.074539
69  1.0861e-03     74 3.6304e-02 0.73962 0.074496
70  1.0751e-03     75 3.5218e-02 0.73945 0.074716
71  1.0619e-03     76 3.4143e-02 0.73833 0.074740
72  1.0396e-03     77 3.3081e-02 0.73833 0.074740
73  1.0031e-03     78 3.2041e-02 0.73390 0.073412
74  9.8653e-04     79 3.1038e-02 0.73390 0.073412
75  9.7982e-04     80 3.0052e-02 0.73362 0.073421
76  9.6068e-04     81 2.9072e-02 0.73336 0.073431
77  9.0157e-04     82 2.8111e-02 0.73346 0.073239
78  8.5140e-04     83 2.7210e-02 0.74355 0.073608
79  8.2985e-04     84 2.6358e-02 0.74289 0.073577
80  7.6578e-04     85 2.5529e-02 0.75270 0.074401
81  7.5439e-04     87 2.3997e-02 0.75068 0.071516
82  7.1170e-04     88 2.3243e-02 0.74411 0.071166
83  6.8926e-04     89 2.2531e-02 0.74202 0.071059
84  6.7218e-04     90 2.1842e-02 0.73282 0.070730
85  6.7025e-04     91 2.1169e-02 0.73262 0.070738
86  6.6158e-04     92 2.0499e-02 0.73262 0.070738
87  6.3418e-04     93 1.9838e-02 0.73337 0.070711
88  5.8450e-04     94 1.9203e-02 0.73500 0.071237
89  5.6893e-04     95 1.8619e-02 0.73223 0.070717
90  5.5273e-04     96 1.8050e-02 0.73659 0.070702
91  5.4467e-04     97 1.7497e-02 0.73718 0.070776
92  5.4467e-04     98 1.6953e-02 0.73718 0.070776
93  5.3718e-04     99 1.6408e-02 0.73718 0.070776
94  5.2351e-04    100 1.5871e-02 0.73718 0.070776
95  5.0781e-04    101 1.5347e-02 0.73857 0.071077
96  4.9273e-04    102 1.4839e-02 0.73897 0.071303
97  4.7591e-04    103 1.4347e-02 0.73723 0.071291
98  4.7283e-04    104 1.3871e-02 0.73289 0.071245
99  4.7167e-04    105 1.3398e-02 0.73289 0.071245
100 4.6792e-04    106 1.2926e-02 0.73367 0.071229
101 4.4890e-04    107 1.2458e-02 0.73271 0.071251
102 4.4497e-04    108 1.2009e-02 0.73500 0.071336
103 4.3662e-04    109 1.1565e-02 0.73626 0.071314
104 4.0289e-04    110 1.1128e-02 0.73827 0.071318
105 3.9917e-04    111 1.0725e-02 0.73714 0.071225
106 3.5812e-04    112 1.0326e-02 0.73757 0.071280
107 3.4136e-04    113 9.9677e-03 0.73935 0.071322
108 3.4092e-04    114 9.6264e-03 0.73935 0.071322
109 3.1735e-04    115 9.2854e-03 0.73935 0.071322
110 2.9187e-04    116 8.9681e-03 0.74282 0.072752
111 2.8919e-04    117 8.6762e-03 0.74209 0.072720
112 2.6331e-04    118 8.3870e-03 0.74465 0.072682
113 2.5547e-04    119 8.1237e-03 0.74431 0.072695
114 2.5495e-04    120 7.8683e-03 0.74576 0.072656
115 2.3380e-04    121 7.6133e-03 0.74654 0.072644
116 2.2890e-04    122 7.3795e-03 0.74668 0.072724
117 2.2642e-04    123 7.1506e-03 0.74690 0.072727
118 2.0984e-04    124 6.9242e-03 0.74265 0.072544
119 2.0200e-04    125 6.7143e-03 0.74425 0.072512
120 1.9608e-04    126 6.5123e-03 0.74161 0.072178
121 1.8717e-04    127 6.3163e-03 0.74511 0.072464
122 1.8717e-04    128 6.1291e-03 0.74799 0.072449
123 1.8580e-04    129 5.9419e-03 0.74770 0.072459
124 1.8476e-04    130 5.7561e-03 0.74770 0.072459
125 1.7793e-04    131 5.5714e-03 0.74719 0.072478
126 1.7444e-04    132 5.3934e-03 0.75014 0.072719
127 1.7244e-04    133 5.2190e-03 0.75150 0.072759
128 1.5738e-04    134 5.0466e-03 0.75072 0.072755
129 1.5505e-04    135 4.8892e-03 0.75222 0.072736
130 1.5275e-04    136 4.7341e-03 0.75193 0.072741
131 1.4819e-04    137 4.5814e-03 0.75415 0.072836
132 1.4370e-04    138 4.4332e-03 0.75422 0.072833
133 1.4148e-04    139 4.2895e-03 0.75413 0.072836
134 1.3805e-04    140 4.1480e-03 0.75413 0.072836
135 1.3430e-04    141 4.0100e-03 0.74921 0.071582
136 1.2773e-04    142 3.8757e-03 0.74657 0.071549
137 1.2334e-04    143 3.7479e-03 0.74713 0.071529
138 1.2312e-04    144 3.6246e-03 0.74737 0.071512
139 1.2024e-04    145 3.5015e-03 0.74737 0.071512
140 1.1979e-04    146 3.3812e-03 0.74737 0.071512
141 1.1629e-04    147 3.2615e-03 0.74785 0.071532
142 1.1285e-04    148 3.1452e-03 0.74861 0.071529
143 1.1222e-04    149 3.0323e-03 0.74823 0.071534
144 1.1026e-04    150 2.9201e-03 0.74823 0.071534
145 1.0832e-04    151 2.8098e-03 0.74823 0.071534
146 1.0640e-04    152 2.7015e-03 0.74763 0.071529
147 1.0640e-04    153 2.5951e-03 0.74763 0.071529
148 1.0611e-04    154 2.4887e-03 0.74763 0.071529
149 1.0260e-04    155 2.3826e-03 0.74763 0.071529
150 9.6395e-05    156 2.2800e-03 0.74796 0.071519
151 9.1612e-05    157 2.1836e-03 0.74794 0.071510
152 9.1612e-05    158 2.0920e-03 0.74575 0.071262
153 9.0178e-05    159 2.0004e-03 0.74575 0.071262
154 8.8088e-05    160 1.9102e-03 0.74505 0.071262
155 8.2225e-05    161 1.8221e-03 0.74505 0.071262
156 8.1241e-05    162 1.7399e-03 0.74505 0.071262
157 7.8365e-05    163 1.6587e-03 0.74505 0.071262
158 7.7933e-05    164 1.5803e-03 0.74444 0.071272
159 7.6301e-05    165 1.5024e-03 0.74439 0.071302
160 7.2769e-05    166 1.4261e-03 0.74487 0.071275
161 6.7381e-05    167 1.3533e-03 0.74537 0.071259
162 6.4264e-05    168 1.2859e-03 0.74567 0.071240
163 6.2200e-05    169 1.2216e-03 0.74575 0.071237
164 6.2200e-05    170 1.1594e-03 0.74484 0.071204
165 5.9104e-05    172 1.0350e-03 0.74682 0.071255
166 5.7226e-05    173 9.7594e-04 0.74664 0.071254
167 5.2975e-05    174 9.1872e-04 0.74630 0.071256
168 5.0154e-05    175 8.6574e-04 0.74784 0.071591
169 4.5138e-05    176 8.1559e-04 0.74775 0.071593
170 4.3548e-05    177 7.7045e-04 0.74783 0.071590
171 3.7343e-05    178 7.2690e-04 0.74733 0.071588
172 3.3574e-05    179 6.8956e-04 0.74901 0.071885
173 3.2132e-05    181 6.2241e-04 0.74965 0.071873
174 3.1933e-05    182 5.9028e-04 0.74965 0.071873
175 3.1735e-05    183 5.5835e-04 0.74965 0.071873
176 2.8211e-05    184 5.2661e-04 0.74871 0.071893
177 2.6528e-05    186 4.7019e-04 0.74910 0.071893
178 2.4895e-05    187 4.4366e-04 0.74849 0.071891
179 2.3315e-05    189 3.9387e-04 0.74884 0.071879
180 2.2406e-05    190 3.7055e-04 0.74863 0.071879
181 2.1787e-05    191 3.4815e-04 0.74883 0.071893
182 2.0310e-05    192 3.2636e-04 0.74905 0.071891
183 1.8885e-05    193 3.0605e-04 0.74848 0.071856
184 1.7512e-05    194 2.8717e-04 0.74842 0.071854
185 1.7486e-05    195 2.6965e-04 0.74764 0.071829
186 1.6191e-05    196 2.5217e-04 0.74786 0.071823
187 1.5967e-05    198 2.1978e-04 0.74741 0.071800
188 1.4922e-05    199 2.0382e-04 0.74741 0.071800
189 1.3816e-05    200 1.8890e-04 0.74746 0.071784
190 1.3704e-05    201 1.7508e-04 0.74738 0.071787
191 1.3704e-05    202 1.6138e-04 0.74738 0.071787
192 1.3134e-05    203 1.4767e-04 0.74738 0.071787
193 1.2538e-05    204 1.3454e-04 0.74713 0.071769
194 1.2469e-05    205 1.2200e-04 0.74686 0.071769
195 9.3520e-06    206 1.0953e-04 0.74739 0.071770
196 9.3520e-06    207 1.0018e-04 0.74683 0.071766
197 8.3935e-06    208 9.0826e-05 0.74686 0.071762
198 7.4868e-06    210 7.4039e-05 0.74709 0.071770
199 6.2951e-06    211 6.6552e-05 0.74743 0.071763
200 5.0775e-06    212 6.0257e-05 0.74757 0.071758
201 4.9739e-06    213 5.5179e-05 0.74752 0.071756
202 4.9739e-06    214 5.0205e-05 0.74752 0.071756
203 4.7731e-06    215 4.5232e-05 0.74752 0.071756
204 4.1967e-06    217 3.5685e-05 0.74745 0.071757
205 4.1492e-06    218 3.1488e-05 0.74741 0.071759
206 3.7304e-06    219 2.7339e-05 0.74701 0.071761
207 3.1346e-06    220 2.3609e-05 0.74663 0.071759
208 2.4956e-06    221 2.0474e-05 0.74656 0.071748
209 2.2106e-06    222 1.7979e-05 0.74660 0.071747
210 2.2106e-06    223 1.5768e-05 0.74670 0.071746
211 1.6580e-06    224 1.3557e-05 0.74670 0.071746
212 1.4594e-06    226 1.0241e-05 0.74634 0.071706
213 1.2694e-06    227 8.7821e-06 0.74652 0.071710
214 1.2694e-06    229 6.2433e-06 0.74652 0.071710
215 1.0000e-06    230 4.9739e-06 0.74652 0.071710</code></pre>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="arbres.html#cb109-1"></a>simple.tree &lt;-<span class="st"> </span><span class="kw">prune</span>(tree,<span class="dt">cp=</span><span class="fl">0.1</span>)</span>
<span id="cb109-2"><a href="arbres.html#cb109-2"></a>large.tree &lt;-<span class="st"> </span><span class="kw">prune</span>(tree,<span class="dt">cp=</span><span class="fl">1e-6</span>)</span>
<span id="cb109-3"><a href="arbres.html#cb109-3"></a><span class="co">#cp_opt &lt;- tree$cptable[which.min(tree$cptable[,&quot;xerror&quot;]),&quot;CP&quot;]</span></span>
<span id="cb109-4"><a href="arbres.html#cb109-4"></a>cp_opt &lt;-<span class="st"> </span>tree<span class="op">$</span>cptable <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(xerror<span class="op">==</span><span class="kw">min</span>(xerror)) <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(CP) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.numeric</span>()</span>
<span id="cb109-5"><a href="arbres.html#cb109-5"></a>opt.tree &lt;-<span class="st"> </span><span class="kw">prune</span>(tree,<span class="dt">cp=</span>cp_opt)</span></code></pre></div></li>
<li><p>Calculer l’erreur quadratique de ces 3 arbres en utilisant l’échantillon test.</p>
<div class="corR">
<p>
Pour chaque arbre <span class="math inline"><span class="math inline">\(T\)</span></span> on calcule <span class="math display"><span class="math display">\[\frac{1}{n_\text{test}}\sum_{i\in \text{test}}(Y_i-T(X_i))^2.\]</span></span>
</p>
<p>
On définit une table qui regroupe les prédictions des 3 arbres sur l’échantillon test :
</p>
</div>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="arbres.html#cb110-1"></a>data.prev &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">simple=</span><span class="kw">predict</span>(simple.tree,<span class="dt">newdata =</span> test),</span>
<span id="cb110-2"><a href="arbres.html#cb110-2"></a>                    <span class="dt">large=</span><span class="kw">predict</span>(large.tree,<span class="dt">newdata =</span> test),</span>
<span id="cb110-3"><a href="arbres.html#cb110-3"></a>                    <span class="dt">opt=</span><span class="kw">predict</span>(opt.tree,<span class="dt">newdata =</span> test),</span>
<span id="cb110-4"><a href="arbres.html#cb110-4"></a>                    <span class="dt">obs=</span>test<span class="op">$</span>Sale)</span></code></pre></div>
<div class="corR">
<p>
On en déduit les erreurs quadratique
</p>
</div>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="arbres.html#cb111-1"></a>data.prev <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarise_at</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>,<span class="op">~</span><span class="kw">mean</span>((obs<span class="op">-</span>.)<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb111-2"><a href="arbres.html#cb111-2"></a>    simple   large      opt</span>
<span id="cb111-3"><a href="arbres.html#cb111-3"></a><span class="dv">1</span> <span class="fl">5.800361</span> <span class="fl">5.43738</span> <span class="fl">4.469369</span></span></code></pre></div>
<div class="corR">
<p>
L’arbre sélectionné a ici la plus petite erreur.
</p>
</div></li>
<li><p>Refaire la comparaison avec une validation croisée 10 blocs.</p>
<div class="corR">
<p>
On créé tout d’abord les blocs.
</p>
</div>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="arbres.html#cb112-1"></a><span class="kw">library</span>(caret)</span>
<span id="cb112-2"><a href="arbres.html#cb112-2"></a>K &lt;-<span class="st"> </span><span class="dv">10</span></span>
<span id="cb112-3"><a href="arbres.html#cb112-3"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb112-4"><a href="arbres.html#cb112-4"></a>kfolds &lt;-<span class="st"> </span><span class="kw">createFolds</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(Carseats),<span class="dt">k=</span>K)</span></code></pre></div>
<div class="corR">
<p>
On fait la validation croisée.
</p>
</div>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="arbres.html#cb113-1"></a>prev &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,<span class="dt">nrow=</span><span class="kw">nrow</span>(Carseats),<span class="dt">ncol=</span><span class="dv">3</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.data.frame</span>()</span>
<span id="cb113-2"><a href="arbres.html#cb113-2"></a><span class="kw">names</span>(prev) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;simple&quot;</span>,<span class="st">&quot;large&quot;</span>,<span class="st">&quot;opt&quot;</span>)</span>
<span id="cb113-3"><a href="arbres.html#cb113-3"></a><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K){</span>
<span id="cb113-4"><a href="arbres.html#cb113-4"></a>  train &lt;-<span class="st"> </span>Carseats[<span class="op">-</span>kfolds[[j]],]</span>
<span id="cb113-5"><a href="arbres.html#cb113-5"></a>  test &lt;-<span class="st"> </span>Carseats[kfolds[[j]],]</span>
<span id="cb113-6"><a href="arbres.html#cb113-6"></a>  tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(Sales<span class="op">~</span>.,<span class="dt">data=</span>train,<span class="dt">minsplit=</span><span class="dv">2</span>,<span class="dt">cp=</span><span class="fl">1e-9</span>)</span>
<span id="cb113-7"><a href="arbres.html#cb113-7"></a>  simple &lt;-<span class="st"> </span><span class="kw">prune</span>(tree,<span class="dt">cp=</span>tree<span class="op">$</span>cptable[<span class="dv">2</span>,<span class="dv">1</span>])</span>
<span id="cb113-8"><a href="arbres.html#cb113-8"></a>  large &lt;-<span class="st"> </span><span class="kw">prune</span>(tree,<span class="dt">cp=</span><span class="fl">1e-9</span>)</span>
<span id="cb113-9"><a href="arbres.html#cb113-9"></a>  cp_opt &lt;-<span class="st"> </span>tree<span class="op">$</span>cptable <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(xerror<span class="op">==</span><span class="kw">min</span>(xerror)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb113-10"><a href="arbres.html#cb113-10"></a>dplyr<span class="op">::</span><span class="kw">select</span>(CP) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.numeric</span>()</span>
<span id="cb113-11"><a href="arbres.html#cb113-11"></a>  opt &lt;-<span class="st"> </span><span class="kw">prune</span>(tree,<span class="dt">cp=</span>cp_opt) </span>
<span id="cb113-12"><a href="arbres.html#cb113-12"></a>  prev[kfolds[[j]],<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="kw">predict</span>(simple,<span class="dt">newdata=</span>test)</span>
<span id="cb113-13"><a href="arbres.html#cb113-13"></a>  prev[kfolds[[j]],<span class="dv">2</span>] &lt;-<span class="st"> </span><span class="kw">predict</span>(large,<span class="dt">newdata=</span>test)</span>
<span id="cb113-14"><a href="arbres.html#cb113-14"></a>  prev[kfolds[[j]],<span class="dv">3</span>] &lt;-<span class="st"> </span><span class="kw">predict</span>(opt,<span class="dt">newdata=</span>test)</span>
<span id="cb113-15"><a href="arbres.html#cb113-15"></a>}</span>
<span id="cb113-16"><a href="arbres.html#cb113-16"></a>prev <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">obs=</span>Carseats<span class="op">$</span>Sales) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarize_at</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>,<span class="op">~</span><span class="kw">mean</span>((obs<span class="op">-</span>.)<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb113-17"><a href="arbres.html#cb113-17"></a>    simple   large      opt</span>
<span id="cb113-18"><a href="arbres.html#cb113-18"></a><span class="dv">1</span> <span class="fl">6.003064</span> <span class="fl">4.79406</span> <span class="fl">4.556404</span></span></code></pre></div></li>
</ol>
</div>
<div id="élagage-en-classification-binaire-et-matrice-de-coût" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Élagage en classification binaire et matrice de coût</h3>
<p>On considère ici les mêmes données que précédemment mais on cherche à expliquer une version binaire de la variable <strong>Sales</strong>. Cette nouvelle variable, appelée <code>High</code> prend pour valeurs <code>No</code> si <code>Sales</code> est inférieur ou égal à 8, <code>Yes</code> sinon. On travaillera donc avec le jeu <code>data1</code> défini ci-dessous.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="arbres.html#cb114-1"></a>High &lt;-<span class="st"> </span><span class="kw">ifelse</span>(Carseats<span class="op">$</span>Sales<span class="op">&lt;=</span><span class="dv">8</span>,<span class="st">&quot;No&quot;</span>,<span class="st">&quot;Yes&quot;</span>)</span>
<span id="cb114-2"><a href="arbres.html#cb114-2"></a>data1 &lt;-<span class="st"> </span>Carseats <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="op">-</span>Sales) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(High)</span></code></pre></div>
<ol style="list-style-type: decimal">
<li><p>Construire un arbre permettant d’expliquer <code>High</code> par les autres variables (sans <code>Sales</code> évidemment !) et expliquer les principales différences par rapport à la partie précédente précédente.</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="arbres.html#cb115-1"></a><span class="kw">set.seed</span>(<span class="dv">321</span>)</span>
<span id="cb115-2"><a href="arbres.html#cb115-2"></a>tree &lt;-<span class="st"> </span><span class="kw">rpart</span>(High<span class="op">~</span>.,<span class="dt">data=</span>data1)</span>
<span id="cb115-3"><a href="arbres.html#cb115-3"></a><span class="kw">rpart.plot</span>(tree)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-174-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="corR">
<p>
L’arbre construit est un <strong>arbre de classification</strong>. Le procédé de découpe des noeuds est différent : il utilise l’impureté de Gini au lieu de la variance.
</p>
</div></li>
<li><p>Expliquer l’option <code>parms</code> dans la commande :</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="arbres.html#cb116-1"></a>tree1 &lt;-<span class="st"> </span><span class="kw">rpart</span>(High<span class="op">~</span>.,<span class="dt">data=</span>data1,<span class="dt">parms=</span><span class="kw">list</span>(<span class="dt">split=</span><span class="st">&quot;information&quot;</span>))</span>
<span id="cb116-2"><a href="arbres.html#cb116-2"></a>tree1<span class="op">$</span>parms</span>
<span id="cb116-3"><a href="arbres.html#cb116-3"></a><span class="op">$</span>prior</span>
<span id="cb116-4"><a href="arbres.html#cb116-4"></a>   <span class="dv">1</span>    <span class="dv">2</span> </span>
<span id="cb116-5"><a href="arbres.html#cb116-5"></a><span class="fl">0.59</span> <span class="fl">0.41</span> </span>
<span id="cb116-6"><a href="arbres.html#cb116-6"></a></span>
<span id="cb116-7"><a href="arbres.html#cb116-7"></a><span class="op">$</span>loss</span>
<span id="cb116-8"><a href="arbres.html#cb116-8"></a>     [,<span class="dv">1</span>] [,<span class="dv">2</span>]</span>
<span id="cb116-9"><a href="arbres.html#cb116-9"></a>[<span class="dv">1</span>,]    <span class="dv">0</span>    <span class="dv">1</span></span>
<span id="cb116-10"><a href="arbres.html#cb116-10"></a>[<span class="dv">2</span>,]    <span class="dv">1</span>    <span class="dv">0</span></span>
<span id="cb116-11"><a href="arbres.html#cb116-11"></a></span>
<span id="cb116-12"><a href="arbres.html#cb116-12"></a><span class="op">$</span>split</span>
<span id="cb116-13"><a href="arbres.html#cb116-13"></a>[<span class="dv">1</span>] <span class="dv">2</span></span></code></pre></div>
<div class="corR">
<p>
On change de fonction d’impureté (<strong>information</strong> au lieu de <strong>Gini</strong>).
</p>
</div></li>
<li><p>Expliquer les sorties de la fonction <strong>printcp</strong> sur le premier arbre construit et retrouver la valeur du dernier terme de la colonne <strong>rel error</strong>.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="arbres.html#cb117-1"></a><span class="kw">printcp</span>(tree)</span>
<span id="cb117-2"><a href="arbres.html#cb117-2"></a></span>
<span id="cb117-3"><a href="arbres.html#cb117-3"></a>Classification tree<span class="op">:</span></span>
<span id="cb117-4"><a href="arbres.html#cb117-4"></a><span class="kw">rpart</span>(<span class="dt">formula =</span> High <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> data1)</span>
<span id="cb117-5"><a href="arbres.html#cb117-5"></a></span>
<span id="cb117-6"><a href="arbres.html#cb117-6"></a>Variables actually used <span class="cf">in</span> tree construction<span class="op">:</span></span>
<span id="cb117-7"><a href="arbres.html#cb117-7"></a>[<span class="dv">1</span>] Advertising Age         CompPrice   Income     </span>
<span id="cb117-8"><a href="arbres.html#cb117-8"></a>[<span class="dv">5</span>] Price       ShelveLoc  </span>
<span id="cb117-9"><a href="arbres.html#cb117-9"></a></span>
<span id="cb117-10"><a href="arbres.html#cb117-10"></a>Root node error<span class="op">:</span><span class="st"> </span><span class="dv">164</span><span class="op">/</span><span class="dv">400</span> =<span class="st"> </span><span class="fl">0.41</span></span>
<span id="cb117-11"><a href="arbres.html#cb117-11"></a></span>
<span id="cb117-12"><a href="arbres.html#cb117-12"></a>n=<span class="st"> </span><span class="dv">400</span> </span>
<span id="cb117-13"><a href="arbres.html#cb117-13"></a></span>
<span id="cb117-14"><a href="arbres.html#cb117-14"></a>        CP nsplit rel error  xerror     xstd</span>
<span id="cb117-15"><a href="arbres.html#cb117-15"></a><span class="dv">1</span> <span class="fl">0.286585</span>      <span class="dv">0</span>   <span class="fl">1.00000</span> <span class="fl">1.00000</span> <span class="fl">0.059980</span></span>
<span id="cb117-16"><a href="arbres.html#cb117-16"></a><span class="dv">2</span> <span class="fl">0.109756</span>      <span class="dv">1</span>   <span class="fl">0.71341</span> <span class="fl">0.71341</span> <span class="fl">0.055477</span></span>
<span id="cb117-17"><a href="arbres.html#cb117-17"></a><span class="dv">3</span> <span class="fl">0.045732</span>      <span class="dv">2</span>   <span class="fl">0.60366</span> <span class="fl">0.66463</span> <span class="fl">0.054298</span></span>
<span id="cb117-18"><a href="arbres.html#cb117-18"></a><span class="dv">4</span> <span class="fl">0.036585</span>      <span class="dv">4</span>   <span class="fl">0.51220</span> <span class="fl">0.64634</span> <span class="fl">0.053821</span></span>
<span id="cb117-19"><a href="arbres.html#cb117-19"></a><span class="dv">5</span> <span class="fl">0.027439</span>      <span class="dv">5</span>   <span class="fl">0.47561</span> <span class="fl">0.60976</span> <span class="fl">0.052806</span></span>
<span id="cb117-20"><a href="arbres.html#cb117-20"></a><span class="dv">6</span> <span class="fl">0.024390</span>      <span class="dv">7</span>   <span class="fl">0.42073</span> <span class="fl">0.58537</span> <span class="fl">0.052083</span></span>
<span id="cb117-21"><a href="arbres.html#cb117-21"></a><span class="dv">7</span> <span class="fl">0.012195</span>      <span class="dv">8</span>   <span class="fl">0.39634</span> <span class="fl">0.56707</span> <span class="fl">0.051515</span></span>
<span id="cb117-22"><a href="arbres.html#cb117-22"></a><span class="dv">8</span> <span class="fl">0.010000</span>     <span class="dv">10</span>   <span class="fl">0.37195</span> <span class="fl">0.53659</span> <span class="fl">0.050518</span></span></code></pre></div>
<div class="corR">
<p>
<p>On peut lire des informations sur la suite d’arbres emboîtés, cette suite est de longueur 17 ici. Dans le dernier tableau, chaque ligne représente un arbre de la suite et on a dans les colonnes :</p>
</p>
<ul>
<li>
<p><strong>CP</strong> : le paramètre de complexité, plus il est petit plus l’arbre est profond ;</p>
</li>
<li>
<p><strong>nsplit</strong> : nombre de coupures de l’arbre ;</p>
</li>
<li>
<p><strong>rel error</strong> contient l’erreur calculée sur les données d’apprentissage. Cette erreur décroit lorsque la complexité augmente et peut être interprétée comme une erreur d’ajustement ;</p>
</li>
<li>
<p><strong>xerror</strong> : contient l’erreur calculée par validation croisée. Elle peut être interprétée comme une erreur de prévision ;</p>
</li>
<li>
<p><strong>xstd</strong> correspond à l’écart type estimé de l’erreur.</p>
</li>
</ul>
<p>
<p>Les types d’erreurs dépendent du problème considéré. Vu qu’on est ici sur un problème de classification, c’est l’<strong>erreur de classification</strong> qui est considérée. De plus ces erreurs sont normalisées par rapport à l’erreur de l’arbre racine (sans coupure). Ainsi on retrouve l’erreur demandée avec</p>
</p>
</div>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="arbres.html#cb118-1"></a> data1 <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">fitted=</span><span class="kw">predict</span>(tree,<span class="dt">type=</span><span class="st">&quot;class&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb118-2"><a href="arbres.html#cb118-2"></a><span class="st">   </span><span class="kw">summarise</span>(<span class="dt">MC=</span><span class="kw">mean</span>(fitted<span class="op">!=</span>High)<span class="op">/</span><span class="kw">mean</span>(High<span class="op">==</span><span class="st">&quot;Yes&quot;</span>))</span>
<span id="cb118-3"><a href="arbres.html#cb118-3"></a>          MC</span>
<span id="cb118-4"><a href="arbres.html#cb118-4"></a> <span class="dv">1</span> <span class="fl">0.3719512</span></span>
<span id="cb118-5"><a href="arbres.html#cb118-5"></a> <span class="co">#mean(predict(arbre,type=&quot;class&quot;)!=donnees$High)/mean(donnees$High==&quot;Yes&quot;)</span></span></code></pre></div></li>
<li><p>Sélectionner un arbre optimal dans la suite.</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="arbres.html#cb119-1"></a>tree1 &lt;-<span class="st"> </span><span class="kw">rpart</span>(High<span class="op">~</span>.,<span class="dt">data=</span>data1,<span class="dt">cp=</span><span class="fl">0.000001</span>,<span class="dt">minsplit=</span><span class="dv">2</span>)</span>
<span id="cb119-2"><a href="arbres.html#cb119-2"></a><span class="kw">plotcp</span>(tree1)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-181-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="arbres.html#cb120-1"></a>cp_opt &lt;-<span class="st"> </span>tree1<span class="op">$</span>cptable <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(<span class="kw">which.min</span>(xerror)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb120-2"><a href="arbres.html#cb120-2"></a><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(CP) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.numeric</span>()</span>
<span id="cb120-3"><a href="arbres.html#cb120-3"></a>tree_sel &lt;-<span class="st"> </span><span class="kw">prune</span>(tree1,<span class="dt">cp=</span>cp_opt)</span>
<span id="cb120-4"><a href="arbres.html#cb120-4"></a><span class="kw">rpart.plot</span>(tree_sel) </span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-181-2.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>On considère la suite d’arbres</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="arbres.html#cb121-1"></a>tree2 &lt;-<span class="st"> </span><span class="kw">rpart</span>(High<span class="op">~</span>.,<span class="dt">data=</span>data1,<span class="dt">parms=</span><span class="kw">list</span>(<span class="dt">loss=</span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">0</span>),<span class="dt">ncol=</span><span class="dv">2</span>)),</span>
<span id="cb121-2"><a href="arbres.html#cb121-2"></a>           <span class="dt">cp=</span><span class="fl">0.01</span>,<span class="dt">minsplit=</span><span class="dv">2</span>)</span></code></pre></div>
<p>Expliquer les sorties des commandes suivantes. On pourra notamment calculer le dernier terme de la colonne <strong>rel error</strong> de la table <strong>cptable</strong>.</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="arbres.html#cb122-1"></a>tree2<span class="op">$</span>parms</span>
<span id="cb122-2"><a href="arbres.html#cb122-2"></a><span class="op">$</span>prior</span>
<span id="cb122-3"><a href="arbres.html#cb122-3"></a>   <span class="dv">1</span>    <span class="dv">2</span> </span>
<span id="cb122-4"><a href="arbres.html#cb122-4"></a><span class="fl">0.59</span> <span class="fl">0.41</span> </span>
<span id="cb122-5"><a href="arbres.html#cb122-5"></a></span>
<span id="cb122-6"><a href="arbres.html#cb122-6"></a><span class="op">$</span>loss</span>
<span id="cb122-7"><a href="arbres.html#cb122-7"></a>     [,<span class="dv">1</span>] [,<span class="dv">2</span>]</span>
<span id="cb122-8"><a href="arbres.html#cb122-8"></a>[<span class="dv">1</span>,]    <span class="dv">0</span>    <span class="dv">1</span></span>
<span id="cb122-9"><a href="arbres.html#cb122-9"></a>[<span class="dv">2</span>,]    <span class="dv">5</span>    <span class="dv">0</span></span>
<span id="cb122-10"><a href="arbres.html#cb122-10"></a></span>
<span id="cb122-11"><a href="arbres.html#cb122-11"></a><span class="op">$</span>split</span>
<span id="cb122-12"><a href="arbres.html#cb122-12"></a>[<span class="dv">1</span>] <span class="dv">1</span></span>
<span id="cb122-13"><a href="arbres.html#cb122-13"></a><span class="kw">printcp</span>(tree2)</span>
<span id="cb122-14"><a href="arbres.html#cb122-14"></a></span>
<span id="cb122-15"><a href="arbres.html#cb122-15"></a>Classification tree<span class="op">:</span></span>
<span id="cb122-16"><a href="arbres.html#cb122-16"></a><span class="kw">rpart</span>(<span class="dt">formula =</span> High <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> data1, <span class="dt">parms =</span> <span class="kw">list</span>(<span class="dt">loss =</span> <span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">0</span>, </span>
<span id="cb122-17"><a href="arbres.html#cb122-17"></a>    <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">0</span>), <span class="dt">ncol =</span> <span class="dv">2</span>)), <span class="dt">cp =</span> <span class="fl">0.01</span>, <span class="dt">minsplit =</span> <span class="dv">2</span>)</span>
<span id="cb122-18"><a href="arbres.html#cb122-18"></a></span>
<span id="cb122-19"><a href="arbres.html#cb122-19"></a>Variables actually used <span class="cf">in</span> tree construction<span class="op">:</span></span>
<span id="cb122-20"><a href="arbres.html#cb122-20"></a>[<span class="dv">1</span>] Advertising Age         CompPrice   Education  </span>
<span id="cb122-21"><a href="arbres.html#cb122-21"></a>[<span class="dv">5</span>] Income      Population  Price       ShelveLoc  </span>
<span id="cb122-22"><a href="arbres.html#cb122-22"></a></span>
<span id="cb122-23"><a href="arbres.html#cb122-23"></a>Root node error<span class="op">:</span><span class="st"> </span><span class="dv">236</span><span class="op">/</span><span class="dv">400</span> =<span class="st"> </span><span class="fl">0.59</span></span>
<span id="cb122-24"><a href="arbres.html#cb122-24"></a></span>
<span id="cb122-25"><a href="arbres.html#cb122-25"></a>n=<span class="st"> </span><span class="dv">400</span> </span>
<span id="cb122-26"><a href="arbres.html#cb122-26"></a></span>
<span id="cb122-27"><a href="arbres.html#cb122-27"></a>         CP nsplit rel error xerror    xstd</span>
<span id="cb122-28"><a href="arbres.html#cb122-28"></a><span class="dv">1</span>  <span class="fl">0.101695</span>      <span class="dv">0</span>   <span class="fl">1.00000</span> <span class="fl">5.0000</span> <span class="fl">0.20840</span></span>
<span id="cb122-29"><a href="arbres.html#cb122-29"></a><span class="dv">2</span>  <span class="fl">0.050847</span>      <span class="dv">2</span>   <span class="fl">0.79661</span> <span class="fl">3.7119</span> <span class="fl">0.20834</span></span>
<span id="cb122-30"><a href="arbres.html#cb122-30"></a><span class="dv">3</span>  <span class="fl">0.036017</span>      <span class="dv">3</span>   <span class="fl">0.74576</span> <span class="fl">3.1653</span> <span class="fl">0.20197</span></span>
<span id="cb122-31"><a href="arbres.html#cb122-31"></a><span class="dv">4</span>  <span class="fl">0.035311</span>      <span class="dv">5</span>   <span class="fl">0.67373</span> <span class="fl">2.9449</span> <span class="fl">0.19818</span></span>
<span id="cb122-32"><a href="arbres.html#cb122-32"></a><span class="dv">5</span>  <span class="fl">0.025424</span>      <span class="dv">9</span>   <span class="fl">0.50847</span> <span class="fl">2.4915</span> <span class="fl">0.18872</span></span>
<span id="cb122-33"><a href="arbres.html#cb122-33"></a><span class="dv">6</span>  <span class="fl">0.016949</span>     <span class="dv">11</span>   <span class="fl">0.45763</span> <span class="fl">2.3559</span> <span class="fl">0.18485</span></span>
<span id="cb122-34"><a href="arbres.html#cb122-34"></a><span class="dv">7</span>  <span class="fl">0.015537</span>     <span class="dv">16</span>   <span class="fl">0.37288</span> <span class="fl">2.0339</span> <span class="fl">0.17491</span></span>
<span id="cb122-35"><a href="arbres.html#cb122-35"></a><span class="dv">8</span>  <span class="fl">0.014831</span>     <span class="dv">21</span>   <span class="fl">0.28814</span> <span class="fl">2.0339</span> <span class="fl">0.17491</span></span>
<span id="cb122-36"><a href="arbres.html#cb122-36"></a><span class="dv">9</span>  <span class="fl">0.010593</span>     <span class="dv">23</span>   <span class="fl">0.25847</span> <span class="fl">1.8983</span> <span class="fl">0.16982</span></span>
<span id="cb122-37"><a href="arbres.html#cb122-37"></a><span class="dv">10</span> <span class="fl">0.010000</span>     <span class="dv">25</span>   <span class="fl">0.23729</span> <span class="fl">1.8263</span> <span class="fl">0.16664</span></span></code></pre></div>
<div class="corR">
<p>
Le critère est ici modifié, on utilise une <strong>erreur de classification pondérée</strong> pour choisir l’arbre. On rappelle que l’erreur de classification est définie par <span class="math display"><span class="math display">\[L(g)=P(g(X)\neq Y)=E[\alpha_11_{g(X)=0,Y=1}+\alpha_21_{g(X)=1,Y=0}]\]</span></span> avec <span class="math inline"><span class="math inline">\(\alpha_1=\alpha_2=1\)</span></span>. Cette erreur donne donc le même poids aux deux erreurs possible (prédire 1 à tort ou prédire 0 à tort). Utiliser cette erreur revient donc à supposer qu’elles ont la même importance pour le problème considéré. Ce n’est bien entendu pas toujours le cas en pratique. La matrice <strong>loss</strong> contient les valeurs de <span class="math inline"><span class="math inline">\(\alpha_1\)</span></span> et <span class="math inline"><span class="math inline">\(\alpha_2\)</span></span> et modifier ces valeurs permettra de donner des poids différents à ces deux erreurs.
</p>
<p>
Avec cette nouvelle commande, on donne un poids de 5 pour une erreur et de 1 pour l’autre. On obtient le terme demandé avec
</p>
</div>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="arbres.html#cb123-1"></a>prev &lt;-<span class="st"> </span><span class="kw">predict</span>(tree2,<span class="dt">type=</span><span class="st">&quot;class&quot;</span>)</span>
<span id="cb123-2"><a href="arbres.html#cb123-2"></a>conf &lt;-<span class="st"> </span><span class="kw">table</span>(data1<span class="op">$</span>High,prev)</span>
<span id="cb123-3"><a href="arbres.html#cb123-3"></a>conf</span>
<span id="cb123-4"><a href="arbres.html#cb123-4"></a>     prev</span>
<span id="cb123-5"><a href="arbres.html#cb123-5"></a>       No Yes</span>
<span id="cb123-6"><a href="arbres.html#cb123-6"></a>  No  <span class="dv">185</span>  <span class="dv">51</span></span>
<span id="cb123-7"><a href="arbres.html#cb123-7"></a>  Yes   <span class="dv">1</span> <span class="dv">163</span></span>
<span id="cb123-8"><a href="arbres.html#cb123-8"></a>loss &lt;-<span class="st"> </span>tree2<span class="op">$</span>parms<span class="op">$</span>loss</span>
<span id="cb123-9"><a href="arbres.html#cb123-9"></a>(conf[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">*</span>loss[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">+</span>conf[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">*</span>loss[<span class="dv">2</span>,<span class="dv">1</span>])<span class="op">/</span><span class="kw">nrow</span>(data1)<span class="op">/</span><span class="kw">mean</span>(data1<span class="op">$</span>High<span class="op">==</span><span class="st">&quot;No&quot;</span>)</span>
<span id="cb123-10"><a href="arbres.html#cb123-10"></a>[<span class="dv">1</span>] <span class="fl">0.2372881</span></span></code></pre></div></li>
<li><p>Comparer les valeurs ajustées par les deux arbres considérés.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="arbres.html#cb124-1"></a><span class="kw">summary</span>(<span class="kw">predict</span>(tree_sel,<span class="dt">type=</span><span class="st">&quot;class&quot;</span>))</span>
<span id="cb124-2"><a href="arbres.html#cb124-2"></a> No Yes </span>
<span id="cb124-3"><a href="arbres.html#cb124-3"></a><span class="dv">240</span> <span class="dv">160</span> </span>
<span id="cb124-4"><a href="arbres.html#cb124-4"></a><span class="kw">summary</span>(<span class="kw">predict</span>(tree2,<span class="dt">type=</span><span class="st">&quot;class&quot;</span>))</span>
<span id="cb124-5"><a href="arbres.html#cb124-5"></a> No Yes </span>
<span id="cb124-6"><a href="arbres.html#cb124-6"></a><span class="dv">186</span> <span class="dv">214</span> </span></code></pre></div>
<div class="corR">
<p>
Il y a plus de <code>Yes</code> prédits dans le second arbre. Cela vient des changements dans la matrice loss : la perte pour prédire <code>No</code> au lieu de <code>Yes</code> est de 5 pour le second arbre. Cela signifie bien détecter les <code>Yes</code> est plus important pour cet arbre, c’est donc tout à fait normal qu’il prédise plus souvent <code>Yes</code> que le premier.
</p>
<p>
Cette stratégie de changer la matrice de coût peut se révéler intéressante dans le cas de <strong>données déséquilibrées</strong> : une modalité de la cible sous-représentée par rapport à l’autre. En effet, pour de tels problèmes il est souvent très important de bien détecter la modalité sous-représentée. On pourra donc donner un poids plus fort lorsqu’on détecte mal cette modalité.
</p>
</div></li>
</ol>


</div>
</div>
</div>
<h3>Références</h3>
<div id="refs" class="references">
<div id="ref-brfrolst84">
<p>Breiman, L., J. Friedman, R. Olshen, and C. Stone. 1984. <em>Classification and Regression Trees</em>. Wadsworth &amp; Brooks.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="SVM.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="agregation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["TUTO_ML.pdf"],
"toc": {
"collapse": "subsection",
"sharing": {
"facebook": true,
"github": true,
"twitter": true
}
},
"highlight": "tango"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
