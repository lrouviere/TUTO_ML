<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapitre 5 Agrégation : forêts aléatoires et gradient boosting | Machine learning</title>
  <meta name="description" content="Chapitre 5 Agrégation : forêts aléatoires et gradient boosting | Machine learning" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapitre 5 Agrégation : forêts aléatoires et gradient boosting | Machine learning" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapitre 5 Agrégation : forêts aléatoires et gradient boosting | Machine learning" />
  
  
  

<meta name="author" content="Laurent Rouvière" />


<meta name="date" content="2021-01-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="SVM.html"/>
<link rel="next" href="deep.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.2/htmlwidgets.js"></script>
<link href="libs/vis-4.20.1/vis.css" rel="stylesheet" />
<script src="libs/vis-4.20.1/vis.min.js"></script>
<script src="libs/visNetwork-binding-2.0.9/visNetwork.js"></script>
<script src="libs/FileSaver-1.1.20151003/FileSaver.min.js"></script>
<script src="libs/Blob-1.0/Blob.js"></script>
<script src="libs/canvas-toBlob-1.0/canvas-toBlob.js"></script>
<script src="libs/html2canvas-0.5.0/html2canvas.js"></script>
<script src="libs/jspdf-1.3.2/jspdf.debug.js"></script>
<link href="libs/jquery-sparkline-2.1.2/jquery.sparkline.css" rel="stylesheet" />
<script src="libs/jquery-sparkline-2.1.2/jquery.sparkline.js"></script>
<script src="libs/sparkline-binding-2.0/sparkline.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning<br> <br> L. Rouvière</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Présentation</a></li>
<li class="part"><span><b>I Algorithmes de référence</b></span></li>
<li class="chapter" data-level="1" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>1</b> Estimation du risque avec caret</a><ul>
<li class="chapter" data-level="1.1" data-path="caret.html"><a href="caret.html#notion-de-risque-en-apprentissage-supervisé"><i class="fa fa-check"></i><b>1.1</b> Notion de risque en apprentissage supervisé</a></li>
<li class="chapter" data-level="1.2" data-path="caret.html"><a href="caret.html#la-validation-croisée"><i class="fa fa-check"></i><b>1.2</b> La validation croisée</a></li>
<li class="chapter" data-level="1.3" data-path="caret.html"><a href="caret.html#le-package-caret"><i class="fa fa-check"></i><b>1.3</b> Le package caret</a></li>
<li class="chapter" data-level="1.4" data-path="caret.html"><a href="caret.html#la-courbe-roc"><i class="fa fa-check"></i><b>1.4</b> La courbe ROC</a></li>
<li class="chapter" data-level="1.5" data-path="caret.html"><a href="caret.html#compléments"><i class="fa fa-check"></i><b>1.5</b> Compléments</a><ul>
<li class="chapter" data-level="1.5.1" data-path="caret.html"><a href="caret.html#calcul-parallèle"><i class="fa fa-check"></i><b>1.5.1</b> Calcul parallèle</a></li>
<li class="chapter" data-level="1.5.2" data-path="caret.html"><a href="caret.html#répéter-les-méthodes-de-rééchantillonnage"><i class="fa fa-check"></i><b>1.5.2</b> Répéter les méthodes de rééchantillonnage</a></li>
<li class="chapter" data-level="1.5.3" data-path="caret.html"><a href="caret.html#modifier-le-risque"><i class="fa fa-check"></i><b>1.5.3</b> Modifier le risque</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="lda.html"><a href="lda.html"><i class="fa fa-check"></i><b>2</b> Analyse discriminante linéaire</a><ul>
<li class="chapter" data-level="2.1" data-path="lda.html"><a href="lda.html#prise-en-main-lda-et-qda-sur-les-iris-de-fisher"><i class="fa fa-check"></i><b>2.1</b> Prise en main : LDA et QDA sur les iris de Fisher</a></li>
<li class="chapter" data-level="2.2" data-path="lda.html"><a href="lda.html#un-cas-avec-beaucoup-de-classes"><i class="fa fa-check"></i><b>2.2</b> Un cas avec beaucoup de classes</a></li>
<li class="chapter" data-level="2.3" data-path="lda.html"><a href="lda.html#grande-dimension-reconnaissance-de-phonèmes"><i class="fa fa-check"></i><b>2.3</b> Grande dimension : reconnaissance de phonèmes</a></li>
<li class="chapter" data-level="2.4" data-path="lda.html"><a href="lda.html#exercices"><i class="fa fa-check"></i><b>2.4</b> Exercices</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="arbres.html"><a href="arbres.html"><i class="fa fa-check"></i><b>3</b> Arbres</a><ul>
<li class="chapter" data-level="3.1" data-path="arbres.html"><a href="arbres.html#coupures-cart-en-fonction-de-la-nature-des-variables"><i class="fa fa-check"></i><b>3.1</b> Coupures CART en fonction de la nature des variables</a><ul>
<li class="chapter" data-level="3.1.1" data-path="arbres.html"><a href="arbres.html#arbres-de-régression"><i class="fa fa-check"></i><b>3.1.1</b> Arbres de régression</a></li>
<li class="chapter" data-level="3.1.2" data-path="arbres.html"><a href="arbres.html#arbres-de-classification"><i class="fa fa-check"></i><b>3.1.2</b> Arbres de classification</a></li>
<li class="chapter" data-level="3.1.3" data-path="arbres.html"><a href="arbres.html#entrée-qualitative"><i class="fa fa-check"></i><b>3.1.3</b> Entrée qualitative</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="arbres.html"><a href="arbres.html#élagage"><i class="fa fa-check"></i><b>3.2</b> Élagage</a><ul>
<li class="chapter" data-level="3.2.1" data-path="arbres.html"><a href="arbres.html#élagage-pour-un-problème-de-régression"><i class="fa fa-check"></i><b>3.2.1</b> Élagage pour un problème de régression</a></li>
<li class="chapter" data-level="3.2.2" data-path="arbres.html"><a href="arbres.html#élagage-en-classification-binaire-et-matrice-de-coût"><i class="fa fa-check"></i><b>3.2.2</b> Élagage en classification binaire et matrice de coût</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Algorithmes avancés</b></span></li>
<li class="chapter" data-level="4" data-path="SVM.html"><a href="SVM.html"><i class="fa fa-check"></i><b>4</b> Support Vector Machine (SVM)</a><ul>
<li class="chapter" data-level="4.1" data-path="SVM.html"><a href="SVM.html#cas-séparable"><i class="fa fa-check"></i><b>4.1</b> Cas séparable</a></li>
<li class="chapter" data-level="4.2" data-path="SVM.html"><a href="SVM.html#cas-non-séparable"><i class="fa fa-check"></i><b>4.2</b> Cas non séparable</a></li>
<li class="chapter" data-level="4.3" data-path="SVM.html"><a href="SVM.html#lastuce-du-noyau"><i class="fa fa-check"></i><b>4.3</b> L’astuce du noyau</a></li>
<li class="chapter" data-level="4.4" data-path="SVM.html"><a href="SVM.html#support-vector-régression"><i class="fa fa-check"></i><b>4.4</b> Support vector régression</a></li>
<li class="chapter" data-level="4.5" data-path="SVM.html"><a href="SVM.html#svm-sur-les-données-spam"><i class="fa fa-check"></i><b>4.5</b> SVM sur les données spam</a></li>
<li class="chapter" data-level="4.6" data-path="SVM.html"><a href="SVM.html#exercices-1"><i class="fa fa-check"></i><b>4.6</b> Exercices</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="agregation.html"><a href="agregation.html"><i class="fa fa-check"></i><b>5</b> Agrégation : forêts aléatoires et gradient boosting</a><ul>
<li class="chapter" data-level="5.1" data-path="agregation.html"><a href="agregation.html#forets"><i class="fa fa-check"></i><b>5.1</b> Forêts aléatoires</a></li>
<li class="chapter" data-level="5.2" data-path="agregation.html"><a href="agregation.html#boosting"><i class="fa fa-check"></i><b>5.2</b> Gradient boosting</a><ul>
<li class="chapter" data-level="5.2.1" data-path="agregation.html"><a href="agregation.html#un-exemple-simple-en-régression"><i class="fa fa-check"></i><b>5.2.1</b> Un exemple simple en régression</a></li>
<li class="chapter" data-level="5.2.2" data-path="agregation.html"><a href="agregation.html#adaboost-et-logitboost-pour-la-classification-binaire."><i class="fa fa-check"></i><b>5.2.2</b> Adaboost et logitboost pour la classification binaire.</a></li>
<li class="chapter" data-level="5.2.3" data-path="agregation.html"><a href="agregation.html#exo:grad-boost"><i class="fa fa-check"></i><b>5.2.3</b> Exercices</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="deep.html"><a href="deep.html"><i class="fa fa-check"></i><b>6</b> Réseaux de neurones avec Keras</a></li>
<li class="chapter" data-level="7" data-path="dondes.html"><a href="dondes.html"><i class="fa fa-check"></i><b>7</b> Données déséquilibrées</a><ul>
<li class="chapter" data-level="7.1" data-path="dondes.html"><a href="dondes.html#critères-de-performance-pour-données-déséquilibrées"><i class="fa fa-check"></i><b>7.1</b> Critères de performance pour données déséquilibrées</a></li>
<li class="chapter" data-level="7.2" data-path="dondes.html"><a href="dondes.html#ré-équilibrage"><i class="fa fa-check"></i><b>7.2</b> Ré-équilibrage</a></li>
<li class="chapter" data-level="7.3" data-path="dondes.html"><a href="dondes.html#exercices-supplémentaires"><i class="fa fa-check"></i><b>7.3</b> Exercices supplémentaires</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="comp-algo.html"><a href="comp-algo.html"><i class="fa fa-check"></i><b>8</b> Comparaison d’algorithmes</a></li>
<li class="chapter" data-level="" data-path="références.html"><a href="références.html"><i class="fa fa-check"></i>Références</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="agregation" class="section level1">
<h1><span class="header-section-number">Chapitre 5</span> Agrégation : forêts aléatoires et gradient boosting</h1>
<p>Les méthodes par arbres présentées précédemment sont des algorithmes qui possèdent tout un tas de qualités (facile à mettre en œuvre, interprétable…). Ce sont néanmoins rarement les algorithmes qui se révèlent les plus performants. Les méthodes d’agrégation d’arbres présentées dans cette partie sont souvent beaucoup plus pertinentes, notamment en terme de qualité de prédiction. Elles consistent à construire un très grand nombre d’arbres “simples” : <span class="math inline">\(g_1,\dots,g_B\)</span> et à les agréger en faisant la moyenne :
<span class="math display">\[\frac{1}{B}\sum_{k=1}^Bg_k(x).\]</span>
Les forêts aléatoires <span class="citation">(Breiman <a href="#ref-bre01" role="doc-biblioref">2001</a>)</span> et le gradient boosting <span class="citation">(Friedman <a href="#ref-fri01" role="doc-biblioref">2001</a>)</span> utilisent ce procédé d’agrégation.</p>
<div id="forets" class="section level2">
<h2><span class="header-section-number">5.1</span> Forêts aléatoires</h2>
<p>L’algorithme des forêts aléatoires consiste à construire des arbres sur des échantillons bootstrap et à les agréger. Il peut s’écrire de la façon suivante :</p>
<div class="correction">
<p>
<strong>Entrées</strong> :
</p>
<ul>
<li>
<span class="math inline"><span class="math inline">\(x\in\mathbb R^d\)</span></span> l’observation à prévoir, <span class="math inline"><span class="math inline">\(\mathcal D_n\)</span></span> l’échantillon ;
</li>
<li>
<span class="math inline"><span class="math inline">\(B\)</span></span> nombre d’arbres ; <span class="math inline"><span class="math inline">\(n_{max}\)</span></span> nombre max d’observations par nœud
</li>
<li>
<span class="math inline"><span class="math inline">\(m\in\{1,\dots,d\}\)</span></span> le nombre de variables candidates pour découper un nœud.
</li>
</ul>
<p>
<strong>Algorithme</strong> : pour <span class="math inline"><span class="math inline">\(k=1,\dots,B\)</span></span> :
</p>
<ol style="list-style-type: decimal">
<li>
Tirer un échantillon <em>bootstrap</em> dans <span class="math inline"><span class="math inline">\(\mathcal D_n\)</span></span>
</li>
<li>
Construire un <em>arbre CART sur cet échantillon bootstrap</em>, chaque coupure est sélectionnée en minimisant la fonction de coût de CART sur un ensemble de <em><span class="math inline"><span class="math inline">\(m\)</span></span> variables choisies au hasard</em> parmi les <span class="math inline"><span class="math inline">\(d\)</span></span>. On note <span class="math inline"><span class="math inline">\(T(.,\theta_k,\mathcal D_n)\)</span></span> l’arbre construit.
</li>
</ol>
<p>
<strong>Sortie</strong> : l’estimateur <span class="math inline"><span class="math inline">\(T_B(x)=\frac{1}{B}\sum_{k=1}^BT(x,\theta_k,\mathcal D_n)\)</span></span>.
</p>
</div>
<p>Cet algorithme peut être utilisé sur <strong>R</strong> avec la fonction <strong>randomForest</strong> du package <strong>randomForest</strong>. Nous la présentons à travers l’exemple du jeu de données <strong>spam</strong> du package <strong>kernlab</strong>.</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="agregation.html#cb169-1"></a><span class="kw">library</span>(kernlab)</span>
<span id="cb169-2"><a href="agregation.html#cb169-2"></a><span class="kw">data</span>(spam)</span>
<span id="cb169-3"><a href="agregation.html#cb169-3"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb169-4"><a href="agregation.html#cb169-4"></a>spam &lt;-<span class="st"> </span>spam[<span class="kw">sample</span>(<span class="kw">nrow</span>(spam)),]</span></code></pre></div>
<p>Le problème est d’expliquer la variable binaire <strong>type</strong> par les autres.</p>
<ol style="list-style-type: decimal">
<li><p>A l’aide de la fonction <strong>randomForest</strong> du package <strong>randomForest</strong>, ajuster une forêt aléatoire pour répondre au problème posé.</p>
<div class="corR">
<p>
On commence par charger le package
</p>
</div>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="agregation.html#cb170-1"></a><span class="kw">library</span>(randomForest)</span></code></pre></div>
<div class="corR">
<p>
Et on construit la forêt avec <strong>randomForest</strong> :
</p>
</div>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="agregation.html#cb171-1"></a>rf1 &lt;-<span class="st"> </span><span class="kw">randomForest</span>(type<span class="op">~</span>.,<span class="dt">data=</span>spam)</span></code></pre></div></li>
<li><p>Appliquer la fonction <strong>plot</strong> à l’objet construit avec <strong>randomForest</strong> et expliquer le graphe obtenu. A quoi peut servir ce graphe en pratique ?</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="agregation.html#cb172-1"></a><span class="kw">plot</span>(rf1)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-291-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="corR">
<p>
Ce graphe permet de visualiser l’erreur de classication ainsi que les taux de faux positifs et faux négatifs calculés par Out Of Bag en fonction du nombre d’arbres de la forêt. Ce graphe peut être utilisé pour voir si l’algorithme a bien “convergé”. Si ce n’est pas le cas, il faut construire une forêt avec plus d’abres.
</p>
</div></li>
<li><p>Construire la forêt avec <strong>mtry=1</strong> et comparer ses performances avec celle construite précédemment.</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="agregation.html#cb173-1"></a>rf2 &lt;-<span class="st"> </span><span class="kw">randomForest</span>(type<span class="op">~</span>.,<span class="dt">data=</span>spam,<span class="dt">mtry=</span><span class="dv">1</span>)</span>
<span id="cb173-2"><a href="agregation.html#cb173-2"></a>rf1</span>
<span id="cb173-3"><a href="agregation.html#cb173-3"></a></span>
<span id="cb173-4"><a href="agregation.html#cb173-4"></a>Call<span class="op">:</span></span>
<span id="cb173-5"><a href="agregation.html#cb173-5"></a><span class="st"> </span><span class="kw">randomForest</span>(<span class="dt">formula =</span> type <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> spam) </span>
<span id="cb173-6"><a href="agregation.html#cb173-6"></a>               Type of random forest<span class="op">:</span><span class="st"> </span>classification</span>
<span id="cb173-7"><a href="agregation.html#cb173-7"></a>                     Number of trees<span class="op">:</span><span class="st"> </span><span class="dv">500</span></span>
<span id="cb173-8"><a href="agregation.html#cb173-8"></a>No. of variables tried at each split<span class="op">:</span><span class="st"> </span><span class="dv">7</span></span>
<span id="cb173-9"><a href="agregation.html#cb173-9"></a></span>
<span id="cb173-10"><a href="agregation.html#cb173-10"></a>        OOB estimate of  error rate<span class="op">:</span><span class="st"> </span><span class="fl">4.56</span>%</span>
<span id="cb173-11"><a href="agregation.html#cb173-11"></a>Confusion matrix<span class="op">:</span></span>
<span id="cb173-12"><a href="agregation.html#cb173-12"></a><span class="st">        </span>nonspam spam class.error</span>
<span id="cb173-13"><a href="agregation.html#cb173-13"></a>nonspam    <span class="dv">2711</span>   <span class="dv">77</span>  <span class="fl">0.02761836</span></span>
<span id="cb173-14"><a href="agregation.html#cb173-14"></a>spam        <span class="dv">133</span> <span class="dv">1680</span>  <span class="fl">0.07335907</span></span>
<span id="cb173-15"><a href="agregation.html#cb173-15"></a>rf2</span>
<span id="cb173-16"><a href="agregation.html#cb173-16"></a></span>
<span id="cb173-17"><a href="agregation.html#cb173-17"></a>Call<span class="op">:</span></span>
<span id="cb173-18"><a href="agregation.html#cb173-18"></a><span class="st"> </span><span class="kw">randomForest</span>(<span class="dt">formula =</span> type <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> spam, <span class="dt">mtry =</span> <span class="dv">1</span>) </span>
<span id="cb173-19"><a href="agregation.html#cb173-19"></a>               Type of random forest<span class="op">:</span><span class="st"> </span>classification</span>
<span id="cb173-20"><a href="agregation.html#cb173-20"></a>                     Number of trees<span class="op">:</span><span class="st"> </span><span class="dv">500</span></span>
<span id="cb173-21"><a href="agregation.html#cb173-21"></a>No. of variables tried at each split<span class="op">:</span><span class="st"> </span><span class="dv">1</span></span>
<span id="cb173-22"><a href="agregation.html#cb173-22"></a></span>
<span id="cb173-23"><a href="agregation.html#cb173-23"></a>        OOB estimate of  error rate<span class="op">:</span><span class="st"> </span><span class="fl">7.89</span>%</span>
<span id="cb173-24"><a href="agregation.html#cb173-24"></a>Confusion matrix<span class="op">:</span></span>
<span id="cb173-25"><a href="agregation.html#cb173-25"></a><span class="st">        </span>nonspam spam class.error</span>
<span id="cb173-26"><a href="agregation.html#cb173-26"></a>nonspam    <span class="dv">2729</span>   <span class="dv">59</span>  <span class="fl">0.02116212</span></span>
<span id="cb173-27"><a href="agregation.html#cb173-27"></a>spam        <span class="dv">304</span> <span class="dv">1509</span>  <span class="fl">0.16767788</span></span></code></pre></div>
<div class="corR">
<p>
La forêt <code>rf1</code> est plus performante en terme d’erreur de classification OOB.
</p>
</div></li>
<li><p>Utiliser la fonction <strong>train</strong> du package <strong>caret</strong> pour choisir le paramètre <strong>mtry</strong> dans la grille <strong>seq(1,30,by=5)</strong>.</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="agregation.html#cb174-1"></a><span class="kw">library</span>(caret)</span>
<span id="cb174-2"><a href="agregation.html#cb174-2"></a>grille.mtry &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">mtry=</span><span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">30</span>,<span class="dt">by=</span><span class="dv">5</span>))</span>
<span id="cb174-3"><a href="agregation.html#cb174-3"></a>ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;oob&quot;</span>)</span>
<span id="cb174-4"><a href="agregation.html#cb174-4"></a><span class="kw">library</span>(doParallel) <span class="co">## pour paralléliser</span></span>
<span id="cb174-5"><a href="agregation.html#cb174-5"></a>cl &lt;-<span class="st"> </span><span class="kw">makePSOCKcluster</span>(<span class="dv">4</span>)</span>
<span id="cb174-6"><a href="agregation.html#cb174-6"></a><span class="kw">registerDoParallel</span>(cl)</span>
<span id="cb174-7"><a href="agregation.html#cb174-7"></a><span class="kw">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb174-8"><a href="agregation.html#cb174-8"></a>sel.mtry &lt;-<span class="st"> </span><span class="kw">train</span>(type<span class="op">~</span>.,<span class="dt">data=</span>spam,<span class="dt">method=</span><span class="st">&quot;rf&quot;</span>,<span class="dt">trControl=</span>ctrl,<span class="dt">tuneGrid=</span>grille.mtry)</span>
<span id="cb174-9"><a href="agregation.html#cb174-9"></a><span class="kw">on.exit</span>(<span class="kw">stopCluster</span>(cl))</span></code></pre></div>
<div class="corR">
<p>
On choisit
</p>
</div>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="agregation.html#cb175-1"></a>sel.mtry<span class="op">$</span>bestTune</span>
<span id="cb175-2"><a href="agregation.html#cb175-2"></a>  mtry</span>
<span id="cb175-3"><a href="agregation.html#cb175-3"></a><span class="dv">2</span>    <span class="dv">6</span></span></code></pre></div></li>
<li><p>Construire la forêt avec le paramètre <strong>mtry</strong> sélectionné. Calculer l’importance des variables et représenter ces importance à l’aide d’un diagramme en barres.</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="agregation.html#cb176-1"></a>rf3 &lt;-<span class="st"> </span><span class="kw">randomForest</span>(type<span class="op">~</span>.,<span class="dt">data=</span>spam,<span class="dt">mtry=</span><span class="kw">unlist</span>(sel.mtry<span class="op">$</span>bestTune),<span class="dt">importance=</span><span class="ot">TRUE</span>)</span>
<span id="cb176-2"><a href="agregation.html#cb176-2"></a>Imp &lt;-<span class="st"> </span>randomForest<span class="op">::</span><span class="kw">importance</span>(rf3,<span class="dt">type=</span><span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb176-3"><a href="agregation.html#cb176-3"></a><span class="kw">mutate</span>(<span class="dt">variable=</span><span class="kw">names</span>(spam)[<span class="op">-</span><span class="dv">58</span>]) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(<span class="kw">desc</span>(MeanDecreaseAccuracy))</span>
<span id="cb176-4"><a href="agregation.html#cb176-4"></a><span class="kw">head</span>(Imp)</span>
<span id="cb176-5"><a href="agregation.html#cb176-5"></a>  MeanDecreaseAccuracy        variable</span>
<span id="cb176-6"><a href="agregation.html#cb176-6"></a><span class="dv">1</span>             <span class="fl">47.58430</span> charExclamation</span>
<span id="cb176-7"><a href="agregation.html#cb176-7"></a><span class="dv">2</span>             <span class="fl">40.83001</span>          remove</span>
<span id="cb176-8"><a href="agregation.html#cb176-8"></a><span class="dv">3</span>             <span class="fl">40.79968</span>      charDollar</span>
<span id="cb176-9"><a href="agregation.html#cb176-9"></a><span class="dv">4</span>             <span class="fl">40.39225</span>      capitalAve</span>
<span id="cb176-10"><a href="agregation.html#cb176-10"></a><span class="dv">5</span>             <span class="fl">37.18721</span>            free</span>
<span id="cb176-11"><a href="agregation.html#cb176-11"></a><span class="dv">6</span>             <span class="fl">36.17332</span>             edu</span></code></pre></div>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="agregation.html#cb177-1"></a><span class="kw">ggplot</span>(Imp) <span class="op">+</span><span class="st"> </span><span class="kw">aes</span>(<span class="dt">x=</span><span class="kw">reorder</span>(variable,MeanDecreaseAccuracy),<span class="dt">y=</span>MeanDecreaseAccuracy)<span class="op">+</span><span class="kw">geom_bar</span>(<span class="dt">stat=</span><span class="st">&quot;identity&quot;</span>)<span class="op">+</span><span class="kw">coord_flip</span>()<span class="op">+</span><span class="kw">xlab</span>(<span class="st">&quot;&quot;</span>)<span class="op">+</span><span class="kw">theme_classic</span>()</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-296-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="corR">
<p>
La fonction <strong>vip</strong> du package <strong>vip</strong> permet de faire le diagramme en barres plus facilement
</p>
</div>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="agregation.html#cb178-1"></a><span class="kw">library</span>(vip)</span>
<span id="cb178-2"><a href="agregation.html#cb178-2"></a><span class="kw">vip</span>(rf3)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-298-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>La fonction <strong>ranger</strong> du package <strong>ranger</strong> permet également de calculer des forêts aléatoires. Comparer les temps de calcul de cette fonction avec <strong>randomForest</strong></p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="agregation.html#cb179-1"></a><span class="kw">library</span>(ranger)</span>
<span id="cb179-2"><a href="agregation.html#cb179-2"></a><span class="kw">system.time</span>(rf4 &lt;-<span class="st"> </span><span class="kw">ranger</span>(type<span class="op">~</span>.,<span class="dt">data=</span>spam))</span>
<span id="cb179-3"><a href="agregation.html#cb179-3"></a>   user  system elapsed </span>
<span id="cb179-4"><a href="agregation.html#cb179-4"></a>  <span class="fl">3.278</span>   <span class="fl">0.048</span>   <span class="fl">0.642</span> </span>
<span id="cb179-5"><a href="agregation.html#cb179-5"></a><span class="kw">system.time</span>(rf5 &lt;-<span class="st"> </span><span class="kw">randomForest</span>(type<span class="op">~</span>.,<span class="dt">data=</span>spam))</span>
<span id="cb179-6"><a href="agregation.html#cb179-6"></a>   user  system elapsed </span>
<span id="cb179-7"><a href="agregation.html#cb179-7"></a>  <span class="fl">7.955</span>   <span class="fl">0.103</span>   <span class="fl">8.092</span> </span></code></pre></div>
<div class="corR">
<p>
Le temps de calcul est plus rapide avec <strong>ranger</strong>. Ce package permet une implémentation efficace des forêts aléatoires pour des données de grande dimension. on peut touver plus d’information <a href="https://arxiv.org/pdf/1508.04409.pdf">ici</a>.
</p>
</div></li>
</ol>
</div>
<div id="boosting" class="section level2">
<h2><span class="header-section-number">5.2</span> Gradient boosting</h2>
<p>Les algorithmes de gradient boosting permettent de minimiser des pertes empiriques de la forme
<span class="math display">\[\frac{1}{n}\sum_{i=1}^n\ell(y_i,f(x_i)).\]</span>
où <span class="math inline">\(\ell:\mathbb R\times\mathbb R\to\mathbb R\)</span> est une fonction de coût convexe en son second argument. Il existe plusieurs type d’algorithmes boosting. Un des plus connus et utilisés a été proposé par <span class="citation">Friedman (<a href="#ref-fri01" role="doc-biblioref">2001</a>)</span>, c’est la version que nous étudions dans cette partie.</p>
<p>Cette approche propose de chercher la meilleure combinaison linéaire d’arbres binaires, c’est-à-dire que l’on recherche <span class="math inline">\(g(x)=\sum_{m=1}^M\alpha_mh_m(x)\)</span> qui minimise
<span class="math display">\[\mathcal R_n(g)=\frac{1}{n}\sum_{i=1}^n\ell(y_i,g(x_i)).\]</span>
Optimiser sur toutes les combinaisons d’arbres binaires se révélant souvent trop compliqué, <span class="citation">Friedman (<a href="#ref-fri01" role="doc-biblioref">2001</a>)</span> utilise une descente de gradient pour construire la combinaison d’abres de façon récursive. L’algorithme est le suivant :</p>

<div class="correction">
<p><strong>Entrées</strong> :</p>
<ul>
<li><span class="math inline">\(d_n=(x_1,y_1),\dots,(x_n,y_n)\)</span> l’échantillon, <span class="math inline">\(\lambda\)</span> un paramètre de régularisation tel que <span class="math inline">\(0&lt;\lambda\leq 1\)</span>.</li>
<li><span class="math inline">\(M\in\mathbb N\)</span> le nombre d’itérations.</li>
<li>paramètres de l’arbre (nombre de coupures…)</li>
</ul>
<p><strong>Itérations</strong> :</p>
<ol style="list-style-type: decimal">
<li>Initialisation : <span class="math inline">\(g_0(.)=\mathop{\mathrm{argmin}}_c \frac{1}{n}\sum_{i=1}^n \ell(y_i,c)\)</span></li>
<li>Pour <span class="math inline">\(m=1\)</span> à <span class="math inline">\(M\)</span> :
<ol style="list-style-type: lower-alpha">
<li>Calculer l’opposé du gradient <span class="math inline">\(-\frac{\partial}{\partial g(x_i)}\ell(y_i,g(x_i))\)</span> et l’évaluer aux points <span class="math inline">\(g_{m-1}(x_i)\)</span> :
<span class="math display">\[U_i=-\frac{\partial}{\partial g(x_i)}\ell(y_i,g(x_i)) _{\Big |g(x_i)=g_{m-1}(x_i)},\quad i=1,\dots,n.\]</span></li>
<li>Ajuster un arbre sur l’échantillon <span class="math inline">\((x_1,U_1),\dots,(x_n,U_n)\)</span>, on le note <span class="math inline">\(h_m\)</span>.</li>
<li>Mise à jour : <span class="math inline">\(g_m(x)=g_{m-1}(x)+\lambda h_m(x)\)</span>.</li>
</ol></li>
</ol>
<strong>Sortie</strong> : la suite <span class="math inline">\((g_m(x))_m\)</span>.
</div>

<p>Sur <strong>R</strong> On peut utiliser différents packages pour faire du gradient boosting. Nous utilisons ici le package <strong>gbm</strong> <span class="citation">(Ridgeway <a href="#ref-rid06" role="doc-biblioref">2006</a>)</span>.</p>
<div id="un-exemple-simple-en-régression" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Un exemple simple en régression</h3>
<p>On considère un jeu de données <span class="math inline">\((x_i,y_i),i=1,\dots,200\)</span> issu d’un modèle de régression
<span class="math display">\[y_i=m(x_i)+\varepsilon_i\]</span>
où la vraie fonction de régression est la fonction <strong>sinus</strong> (mais on va faire comme si on ne le savait pas).</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="agregation.html#cb180-1"></a>x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">2</span><span class="op">*</span>pi,<span class="dv">2</span><span class="op">*</span>pi,<span class="dt">by=</span><span class="fl">0.01</span>)</span>
<span id="cb180-2"><a href="agregation.html#cb180-2"></a>y &lt;-<span class="st"> </span><span class="kw">sin</span>(x)</span>
<span id="cb180-3"><a href="agregation.html#cb180-3"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb180-4"><a href="agregation.html#cb180-4"></a>X &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">200</span>,<span class="op">-</span><span class="dv">2</span><span class="op">*</span>pi,<span class="dv">2</span><span class="op">*</span>pi)</span>
<span id="cb180-5"><a href="agregation.html#cb180-5"></a>Y &lt;-<span class="st"> </span><span class="kw">sin</span>(X)<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">200</span>,<span class="dt">sd=</span><span class="fl">0.2</span>)</span>
<span id="cb180-6"><a href="agregation.html#cb180-6"></a>df1 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(X,Y)</span>
<span id="cb180-7"><a href="agregation.html#cb180-7"></a>df2 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">X=</span>x,<span class="dt">Y=</span>y)</span>
<span id="cb180-8"><a href="agregation.html#cb180-8"></a>p1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(df1)<span class="op">+</span><span class="kw">aes</span>(<span class="dt">x=</span>X,<span class="dt">y=</span>Y)<span class="op">+</span><span class="kw">geom_point</span>()<span class="op">+</span><span class="kw">geom_line</span>(<span class="dt">data=</span>df2,<span class="dt">size=</span><span class="dv">1</span>)<span class="op">+</span><span class="kw">xlab</span>(<span class="st">&quot;&quot;</span>)<span class="op">+</span><span class="kw">ylab</span>(<span class="st">&quot;&quot;</span>)</span>
<span id="cb180-9"><a href="agregation.html#cb180-9"></a>p1</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-301-1.png" width="672" style="display: block; margin: auto;" /></p>
<ol style="list-style-type: decimal">
<li><p>Rappeler ce que siginifie le <span class="math inline">\(L_2\)</span>-boosting.</p>
<div class="corR">
<p>
Il s’agit de l’algorithme de gradient boosting présenté ci-dessus appliqué à la fonction de perte <span class="math display"><span class="math display">\[\ell(y,f(x))=\frac{1}{2}(y-f(x))^2.\]</span></span>
</p>
</div></li>
<li><p>A l’aide de la fonction <strong>gbm</strong> du package <strong>gbm</strong> construire un algorithme de <span class="math inline">\(L_2\)</span>-boosting. On utilisera 500000 itérations et gardera les autres valeurs par défaut de paramètres.</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="agregation.html#cb181-1"></a><span class="kw">library</span>(gbm)</span>
<span id="cb181-2"><a href="agregation.html#cb181-2"></a>L2boost &lt;-<span class="st"> </span><span class="kw">gbm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df1,<span class="dt">n.trees =</span> <span class="dv">500000</span>,<span class="dt">distribution=</span><span class="st">&quot;gaussian&quot;</span>,<span class="dt">bag.fraction =</span> <span class="dv">1</span>)</span></code></pre></div></li>
<li><p>Visualiser l’estimateur à la première itération. On pourra faire un <strong>predict</strong> avec l’option <code>n.trees</code>.</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="agregation.html#cb182-1"></a>prev1 &lt;-<span class="st"> </span><span class="kw">predict</span>(L2boost,<span class="dt">newdata=</span>df2,<span class="dt">n.trees=</span><span class="dv">1</span>)</span>
<span id="cb182-2"><a href="agregation.html#cb182-2"></a>df3 &lt;-<span class="st"> </span>df2 <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">rename</span>(<span class="dt">vraie=</span>Y) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="st">`</span><span class="dt">M=1</span><span class="st">`</span>=prev1)</span>
<span id="cb182-3"><a href="agregation.html#cb182-3"></a>df4 &lt;-<span class="st"> </span>df3 <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pivot_longer</span>(<span class="op">-</span>X,<span class="dt">names_to=</span><span class="st">&quot;courbes&quot;</span>,<span class="dt">values_to=</span><span class="st">&quot;prev&quot;</span>)</span>
<span id="cb182-4"><a href="agregation.html#cb182-4"></a><span class="kw">ggplot</span>(df4)<span class="op">+</span><span class="kw">aes</span>(<span class="dt">x=</span>X,<span class="dt">y=</span>prev,<span class="dt">color=</span>courbes)<span class="op">+</span><span class="kw">geom_line</span>(<span class="dt">size=</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-304-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="corR">
<p>
On remarque que l’estimateur est un arbre avec une seule coupure. On aurait aussi pu utiliser :
</p>
</div>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="agregation.html#cb183-1"></a><span class="kw">plot</span>(L2boost,<span class="dt">n.trees=</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-306-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>Faire de même pour les itérations 1000 et 500000.</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="agregation.html#cb184-1"></a>prev1000 &lt;-<span class="st"> </span><span class="kw">predict</span>(L2boost,<span class="dt">newdata=</span>df2,<span class="dt">n.trees=</span><span class="dv">1000</span>)</span>
<span id="cb184-2"><a href="agregation.html#cb184-2"></a>prev500000 &lt;-<span class="st"> </span><span class="kw">predict</span>(L2boost,<span class="dt">newdata=</span>df2,<span class="dt">n.trees=</span><span class="dv">500000</span>)</span>
<span id="cb184-3"><a href="agregation.html#cb184-3"></a>df31 &lt;-<span class="st"> </span>df3 <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="st">`</span><span class="dt">M=1000</span><span class="st">`</span>=prev1000,<span class="st">`</span><span class="dt">M=500000</span><span class="st">`</span>=prev500000)</span>
<span id="cb184-4"><a href="agregation.html#cb184-4"></a>df41 &lt;-<span class="st"> </span>df31 <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pivot_longer</span>(<span class="op">-</span>X,<span class="dt">names_to=</span><span class="st">&quot;courbes&quot;</span>,<span class="dt">values_to=</span><span class="st">&quot;prev&quot;</span>)</span>
<span id="cb184-5"><a href="agregation.html#cb184-5"></a><span class="kw">ggplot</span>(df41)<span class="op">+</span><span class="kw">aes</span>(<span class="dt">x=</span>X,<span class="dt">y=</span>prev,<span class="dt">color=</span>courbes)<span class="op">+</span><span class="kw">geom_line</span>(<span class="dt">size=</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-307-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="corR">
<p>
On surajuste lorsque le nombre d’itérations est trop important.
</p>
</div></li>
<li><p>Sélectionner le nombre d’itérations par la procédure de votre choix.</p>
<div class="corR">
<p>
On propose de faire une validation hold out. C’est assez facile avec <strong>gbm</strong> il suffit de renseigner l’option <code>train.fraction</code> de <strong>gbm</strong>.
</p>
</div>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="agregation.html#cb185-1"></a><span class="co">#parallel:::setDefaultClusterOptions(setup_strategy = &quot;sequential&quot;)</span></span>
<span id="cb185-2"><a href="agregation.html#cb185-2"></a>L2boost.sel &lt;-<span class="st"> </span><span class="kw">gbm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df1,<span class="dt">n.trees =</span> <span class="dv">10000</span>,<span class="dt">distribution=</span><span class="st">&quot;gaussian&quot;</span>,</span>
<span id="cb185-3"><a href="agregation.html#cb185-3"></a>           <span class="dt">bag.fraction =</span> <span class="dv">1</span>,<span class="dt">train.fraction=</span><span class="fl">0.75</span>)</span>
<span id="cb185-4"><a href="agregation.html#cb185-4"></a><span class="kw">gbm.perf</span>(L2boost.sel)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-310-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>[1] 4787</code></pre></li>
</ol>
</div>
<div id="adaboost-et-logitboost-pour-la-classification-binaire." class="section level3">
<h3><span class="header-section-number">5.2.2</span> Adaboost et logitboost pour la classification binaire.</h3>
<p>On considère le jeu de données <strong>spam</strong> du package <strong>kernlab</strong>.</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="agregation.html#cb187-1"></a><span class="kw">library</span>(kernlab)</span>
<span id="cb187-2"><a href="agregation.html#cb187-2"></a><span class="kw">data</span>(spam)</span>
<span id="cb187-3"><a href="agregation.html#cb187-3"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb187-4"><a href="agregation.html#cb187-4"></a>spam &lt;-<span class="st"> </span>spam[<span class="kw">sample</span>(<span class="kw">nrow</span>(spam)),]</span></code></pre></div>
<ol style="list-style-type: decimal">
<li><p>Exécuter la commande</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="agregation.html#cb188-1"></a>model_ada1 &lt;-<span class="st"> </span><span class="kw">gbm</span>(type<span class="op">~</span>.,<span class="dt">data=</span>spam,<span class="dt">distribution=</span><span class="st">&quot;adaboost&quot;</span>,<span class="dt">interaction.depth=</span><span class="dv">2</span>,</span>
<span id="cb188-2"><a href="agregation.html#cb188-2"></a>              <span class="dt">shrinkage=</span><span class="fl">0.05</span>,<span class="dt">n.trees=</span><span class="dv">500</span>)</span></code></pre></div>
<div class="corR">
<p>
On obtient le message d’erreur suivant :
</p>
</div>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="agregation.html#cb189-1"></a>model_ada1 &lt;-<span class="st"> </span><span class="kw">gbm</span>(type<span class="op">~</span>.,<span class="dt">data=</span>spam,<span class="dt">distribution=</span><span class="st">&quot;adaboost&quot;</span>,<span class="dt">interaction.depth=</span><span class="dv">2</span>,</span>
<span id="cb189-2"><a href="agregation.html#cb189-2"></a>              <span class="dt">shrinkage=</span><span class="fl">0.05</span>,<span class="dt">n.trees=</span><span class="dv">500</span>)</span>
<span id="cb189-3"><a href="agregation.html#cb189-3"></a>Error <span class="cf">in</span> <span class="kw">gbm.fit</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y, <span class="dt">offset =</span> offset, <span class="dt">distribution =</span> distribution, <span class="op">:</span><span class="st"> </span>This version of AdaBoost requires the response to be <span class="cf">in</span> {<span class="dv">0</span>,<span class="dv">1</span>}</span></code></pre></div></li>
<li><p>Proposer une correction permettant de faire fonctionner l’algorithme.</p>
<div class="corR">
<p>
Il est nécessaire que la variable qualitative à expliquer soit codée 0-1 pour adaboost.
</p>
</div>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="agregation.html#cb190-1"></a>spam1 &lt;-<span class="st"> </span>spam</span>
<span id="cb190-2"><a href="agregation.html#cb190-2"></a>spam1<span class="op">$</span>type &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(spam1<span class="op">$</span>type)<span class="op">-</span><span class="dv">1</span></span>
<span id="cb190-3"><a href="agregation.html#cb190-3"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb190-4"><a href="agregation.html#cb190-4"></a>model_ada1 &lt;-<span class="st"> </span><span class="kw">gbm</span>(type<span class="op">~</span>.,<span class="dt">data=</span>spam1,<span class="dt">distribution=</span><span class="st">&quot;adaboost&quot;</span>,<span class="dt">interaction.depth=</span><span class="dv">2</span>,</span>
<span id="cb190-5"><a href="agregation.html#cb190-5"></a>              <span class="dt">shrinkage=</span><span class="fl">0.05</span>,<span class="dt">n.trees=</span><span class="dv">500</span>)</span></code></pre></div></li>
<li><p>Expliciter le modèle ajusté par la commande précédente.</p>
<div class="corR">
<p>
L’algorithme <strong>gbm</strong> est une descente de gradient qui minimise la fonction de perte <span class="math display"><span class="math display">\[\frac{1}{n}\sum_{i=1}^n \ell(y_i,g(x_i)).\]</span></span> Dans le cas de <strong>adaboost</strong> on utilise la perte exponentielle : <span class="math inline"><span class="math inline">\(\ell(y,g(x))=\exp(-yg(x))\)</span></span>.
</p>
</div></li>
<li><p>Effectuer un <strong>summary</strong> du modèle ajusté. Expliquer la sortie.</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="agregation.html#cb191-1"></a><span class="kw">summary</span>(model_ada1)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-318-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre style="max-height: 500px;"><code>                                var     rel.inf
charExclamation     charExclamation 20.04035224
charDollar               charDollar 17.51535261
remove                       remove 11.51692621
free                           free  7.49397637
hp                               hp  6.25654932
capitalLong             capitalLong  5.42905223
capitalAve               capitalAve  4.69521299
your                           your  4.23371585
george                       george  2.50300727
edu                             edu  2.19692796
our                             our  1.99655393
money                         money  1.79063219
email                         email  1.51773292
capitalTotal           capitalTotal  1.43872496
internet                   internet  1.12579132
receive                     receive  0.97001932
will                           will  0.94015881
you                             you  0.89915372
business                   business  0.84418397
re                               re  0.82959153
num1999                     num1999  0.80016393
num650                       num650  0.79468746
meeting                     meeting  0.69494729
num000                       num000  0.56448978
charRoundbracket   charRoundbracket  0.39921437
report                       report  0.38621968
charSemicolon         charSemicolon  0.29835251
credit                       credit  0.27841575
over                           over  0.27064075
order                         order  0.26017226
mail                           mail  0.22398163
technology               technology  0.10340435
hpl                             hpl  0.10151723
original                   original  0.09615196
font                           font  0.09539134
make                           make  0.08995855
project                     project  0.07970985
all                             all  0.05392468
people                       people  0.05359692
address                     address  0.04690996
parts                         parts  0.04260362
conference               conference  0.02037549
num85                         num85  0.01155488
num3d                         num3d  0.00000000
addresses                 addresses  0.00000000
lab                             lab  0.00000000
labs                           labs  0.00000000
telnet                       telnet  0.00000000
num857                       num857  0.00000000
data                           data  0.00000000
num415                       num415  0.00000000
pm                               pm  0.00000000
direct                       direct  0.00000000
cs                               cs  0.00000000
table                         table  0.00000000
charSquarebracket charSquarebracket  0.00000000
charHash                   charHash  0.00000000</code></pre>
<div class="corR">
<p>
On obtient un indicateur qui permet de mesurer l’importance des variable dans la construction de la méthode.
</p>
</div></li>
<li><p>Utiliser la fonction <strong>vip</strong> du package <strong>vip</strong> pour retrouver ce sorties.</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="agregation.html#cb193-1"></a><span class="kw">library</span>(vip)</span>
<span id="cb193-2"><a href="agregation.html#cb193-2"></a><span class="kw">vip</span>(model_ada1,<span class="dt">num_features =</span> 20L)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-320-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>Sélectionner le nombre d’itérations pour l’algorithme adaboost en faisant de la validation croisée 5 blocs.</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="agregation.html#cb194-1"></a>model_ada2 &lt;-<span class="st"> </span><span class="kw">gbm</span>(type<span class="op">~</span>.,<span class="dt">data=</span>spam1,<span class="dt">distribution=</span><span class="st">&quot;adaboost&quot;</span>,<span class="dt">interaction.depth=</span><span class="dv">2</span>,<span class="dt">bag.fraction=</span><span class="dv">1</span>,<span class="dt">cv.folds =</span> <span class="dv">5</span>,<span class="dt">n.trees=</span><span class="dv">500</span>)</span>
<span id="cb194-2"><a href="agregation.html#cb194-2"></a><span class="kw">gbm.perf</span>(model_ada2)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/cv-ada1-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>[1] 233</code></pre></li>
<li><p>Faire la même procédure en changeant la valeur du paramètre <strong>shrinkage</strong>. Interpréter.</p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="agregation.html#cb196-1"></a>model_ada3 &lt;-<span class="st"> </span><span class="kw">gbm</span>(type<span class="op">~</span>.,<span class="dt">data=</span>spam1,<span class="dt">distribution=</span><span class="st">&quot;adaboost&quot;</span>,<span class="dt">interaction.depth=</span><span class="dv">2</span>,<span class="dt">bag.fraction=</span><span class="dv">1</span>,<span class="dt">cv.folds =</span> <span class="dv">5</span>,<span class="dt">n.trees=</span><span class="dv">500</span>,<span class="dt">shrinkage=</span><span class="fl">0.05</span>)</span>
<span id="cb196-2"><a href="agregation.html#cb196-2"></a><span class="kw">gbm.perf</span>(model_ada3)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/cv-ada2-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>[1] 370</code></pre>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="agregation.html#cb198-1"></a>model_ada4 &lt;-<span class="st"> </span><span class="kw">gbm</span>(type<span class="op">~</span>.,<span class="dt">data=</span>spam1,<span class="dt">distribution=</span><span class="st">&quot;adaboost&quot;</span>,<span class="dt">interaction.depth=</span><span class="dv">2</span>,<span class="dt">bag.fraction=</span><span class="dv">1</span>,<span class="dt">cv.folds =</span> <span class="dv">5</span>,<span class="dt">n.trees=</span><span class="dv">500</span>,<span class="dt">shrinkage=</span><span class="fl">0.5</span>)</span>
<span id="cb198-2"><a href="agregation.html#cb198-2"></a><span class="kw">gbm.perf</span>(model_ada4)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/cv-ada3-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>[1] 36</code></pre>
<div class="corR">
<p>
Le nombre d’itérations optimal augmente lorsque <strong>shrinkage</strong> diminue. C’est logique car ce dernier paramètre contrôle la vitesse de descente de gradient : plus il est grand, plus on minimise vite et moins on itère. Il faut néanmoins veiller à ne pas le prendre trop petit pour avoir un estimateur stable. Ici, 0.05 semble être une bonne valeur.
</p>
</div></li>
<li><p>Expliquer la différence entre <strong>adaboost</strong> et <strong>logitboost</strong> et précisez comment on peut mettre en œuvre ce dernier algorithme.</p>
<div class="corR">
<p>
La seule différence se situe au niveau de la <strong>fonction de perte</strong>, adaboost utilise <span class="math display"><span class="math display">\[\exp(-yg(x))\]</span></span> tandis que logitboost utilise <span class="math display"><span class="math display">\[\log(1+\exp(-2yg(x)))\]</span></span> Avec <strong>gbm</strong> il faudra utiliser l’option <code>distribution=“bernoulli”</code> pour faire du logitboost, par exemple :
</p>
</div>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="agregation.html#cb200-1"></a><span class="kw">set.seed</span>(<span class="dv">4321</span>)</span>
<span id="cb200-2"><a href="agregation.html#cb200-2"></a>logitboost &lt;-<span class="st"> </span><span class="kw">gbm</span>(type<span class="op">~</span>.,<span class="dt">data=</span>spam1,<span class="dt">distribution=</span><span class="st">&quot;bernoulli&quot;</span>,<span class="dt">interaction.depth=</span><span class="dv">2</span>,<span class="dt">bag.fraction=</span><span class="dv">1</span>,<span class="dt">cv.folds =</span> <span class="dv">5</span>,<span class="dt">n.trees=</span><span class="dv">500</span>,<span class="dt">shrinkage=</span><span class="fl">0.4</span>)</span>
<span id="cb200-3"><a href="agregation.html#cb200-3"></a><span class="kw">gbm.perf</span>(logitboost)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/cv-logitboost-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>[1] 288</code></pre></li>
</ol>
</div>
<div id="exo:grad-boost" class="section level3">
<h3><span class="header-section-number">5.2.3</span> Exercices</h3>
<ol style="list-style-type: decimal">
<li><p>Rappeler la fonction de risque adaboost.</p>
<div class="correction">
<p>
La fonction de perte adaboost est <span class="math inline"><span class="math inline">\(\ell(y,f(x))=\exp(-yf(x))\)</span></span>. Le risque est donc <span class="math display"><span class="math display">\[\mathcal R(f)=\mathbf E[\exp(-Yf(X))].\]</span></span>
</p>
</div></li>
<li><p>Montrer que le risque est minimum en
<span class="math display">\[f^\star(x)=\frac{1}{2}\log\frac{\mathbf P(Y=1|X=x)}{\mathbf P(Y=-1|X=x)}.\]</span></p>
<div class="correction">
<p>
On cherche <span class="math inline"><span class="math inline">\(f(x)\)</span></span> qui minimise <span class="math inline"><span class="math inline">\(\mathbf E[\exp(-Yf(X))|X=x]\)</span></span>. On a <span class="math display"><span class="math display">\[\mathbf E[\exp(-Yf(X))|X=x]=\mathbf P(Y=1|X=x)\exp(-f(x))+\mathbf P(Y=-1|X=x)\exp(f(x)).\]</span></span> D’où <span class="math display"><span class="math display">\[\frac{\partial \mathbf E[\exp(-Yf(X))|X=x]}{\partial f(x)}=-\mathbf P(Y=1|X=x)\exp(-f(x))+\mathbf P(Y=-1|X=x)\exp(f(x)).\]</span></span> La quantité ci-dessus est égale à 0 en <span class="math display"><span class="math display">\[f^\star(x)=\frac{1}{2}\log\frac{\mathbf P(Y=1|X=x)}{\mathbf P(Y=-1|X=x)}.\]</span></span>
</p>
</div></li>
<li><p>Mêmes questions pour le risque logitboost.</p>
<div class="correction">
<p>
On cherche ici à minimiser l’opposé de la log-vraisemblance <span class="math display"><span class="math display">\[-(y\log p(x)+(1-y)\log(1-p(x)))\]</span></span> avec <span class="math display"><span class="math display">\[p(x)=\frac{1}{1+\exp(-2f(x))}\quad\text{et}\quad 1-p(x)=\frac{1}{1+\exp(2f(x))}.\]</span></span> On déduit <span class="math display"><span class="math display">\[-(y\log p(x)+(1-y)\log(1-p(x)))=\log(1+\exp(-2\tilde yf(x)))\]</span></span> avec <span class="math inline"><span class="math inline">\(\tilde y=2y-1\in\{-1,1\}\)</span></span>. Le risque peut donc s’écrire <span class="math display"><span class="math display">\[\mathcal R(f)=\log(1+\exp(-2\tilde Yf(X))).\]</span></span> De plus <span class="math display"><span class="math display">\[\begin{align*}
 \mathbf E[\log(1+\exp(-2\tilde Yf(X)))|X=x]= &amp;p(x)\log(1+\exp(-2f(x))) \\
 &amp;+(1-p(x))\log(1+\exp(2f(x))).
 \end{align*}\]</span></span>
</p>
<p>
Donc <span class="math display"><span class="math display">\[\frac{\partial \mathbf E[\log(1+\exp(-2\tilde Yf(X)))|X=x]}{\partial f(x)}=p(x)\frac{-2\exp(-2f(x))}{1+\exp(-2f(x))}+(1-p(x)) \frac{2\exp(2f(x))}{1+\exp(2f(x))}.\]</span></span> En annulant la dérivée on obtient <span class="math display"><span class="math display">\[-p(x)\frac{1}{1+\exp(2f(x))}+(1-p(x))\frac{1}{1+\exp(-2f(x))}=0,\]</span></span> d’où <span class="math display"><span class="math display">\[\frac{p(x)}{1-p(x)}=\frac{1+\exp(2f(x))}{1+\exp(-2f(x))}=\frac{\exp(2f(x))(1+\exp(2f(x)))}{\exp(2f(x))(1+\exp(-2f(x)))}=\exp(2f(x)).\]</span></span> Par conséquent <span class="math display"><span class="math display">\[f^\star(x)=\frac{1}{2}\log\frac{\mathbf P(Y=1|X=x)}{\mathbf P(Y=-1|X=x)}.\]</span></span>
</p>
</div></li>
</ol>

</div>
</div>
</div>
<h3>Références</h3>
<div id="refs" class="references">
<div id="ref-bre01">
<p>Breiman, L. 2001. “Random Forests.” <em>Machine Learning</em> 45: 5–32.</p>
</div>
<div id="ref-fri01">
<p>Friedman, J. H. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” <em>Annals of Statistics</em> 29: 1189–1232.</p>
</div>
<div id="ref-rid06">
<p>Ridgeway, G. 2006. “Generalized Boosted Models: A Guide to the Gbm Package.”</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="SVM.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="deep.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["TUTO_ML.pdf"],
"toc": {
"collapse": "subsection",
"sharing": {
"facebook": true,
"github": true,
"twitter": true
}
},
"highlight": "tango"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
