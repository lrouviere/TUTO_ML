<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapitre 2 Analyse discriminante linéaire | Machine learning</title>
  <meta name="description" content="Chapitre 2 Analyse discriminante linéaire | Machine learning" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapitre 2 Analyse discriminante linéaire | Machine learning" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapitre 2 Analyse discriminante linéaire | Machine learning" />
  
  
  

<meta name="author" content="Laurent Rouvière" />


<meta name="date" content="2021-01-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="caret.html"/>
<link rel="next" href="arbres.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning<br> <br> L. Rouvière</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Présentation</a></li>
<li class="part"><span><b>I Algorithmes de référence</b></span></li>
<li class="chapter" data-level="1" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>1</b> Estimation du risque avec caret</a><ul>
<li class="chapter" data-level="1.1" data-path="caret.html"><a href="caret.html#notion-de-risque-en-apprentissage-supervisé"><i class="fa fa-check"></i><b>1.1</b> Notion de risque en apprentissage supervisé</a></li>
<li class="chapter" data-level="1.2" data-path="caret.html"><a href="caret.html#la-validation-croisée"><i class="fa fa-check"></i><b>1.2</b> La validation croisée</a></li>
<li class="chapter" data-level="1.3" data-path="caret.html"><a href="caret.html#le-package-caret"><i class="fa fa-check"></i><b>1.3</b> Le package caret</a></li>
<li class="chapter" data-level="1.4" data-path="caret.html"><a href="caret.html#la-courbe-roc"><i class="fa fa-check"></i><b>1.4</b> La courbe ROC</a></li>
<li class="chapter" data-level="1.5" data-path="caret.html"><a href="caret.html#compléments"><i class="fa fa-check"></i><b>1.5</b> Compléments</a><ul>
<li class="chapter" data-level="1.5.1" data-path="caret.html"><a href="caret.html#calcul-parallèle"><i class="fa fa-check"></i><b>1.5.1</b> Calcul parallèle</a></li>
<li class="chapter" data-level="1.5.2" data-path="caret.html"><a href="caret.html#répéter-les-méthodes-de-rééchantillonnage"><i class="fa fa-check"></i><b>1.5.2</b> Répéter les méthodes de rééchantillonnage</a></li>
<li class="chapter" data-level="1.5.3" data-path="caret.html"><a href="caret.html#modifier-le-risque"><i class="fa fa-check"></i><b>1.5.3</b> Modifier le risque</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="lda.html"><a href="lda.html"><i class="fa fa-check"></i><b>2</b> Analyse discriminante linéaire</a><ul>
<li class="chapter" data-level="2.1" data-path="lda.html"><a href="lda.html#prise-en-main-lda-et-qda-sur-les-iris-de-fisher"><i class="fa fa-check"></i><b>2.1</b> Prise en main : LDA et QDA sur les iris de Fisher</a></li>
<li class="chapter" data-level="2.2" data-path="lda.html"><a href="lda.html#un-cas-avec-beaucoup-de-classes"><i class="fa fa-check"></i><b>2.2</b> Un cas avec beaucoup de classes</a></li>
<li class="chapter" data-level="2.3" data-path="lda.html"><a href="lda.html#grande-dimension-reconnaissance-de-phonèmes"><i class="fa fa-check"></i><b>2.3</b> Grande dimension : reconnaissance de phonèmes</a></li>
<li class="chapter" data-level="2.4" data-path="lda.html"><a href="lda.html#exercices"><i class="fa fa-check"></i><b>2.4</b> Exercices</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="arbres.html"><a href="arbres.html"><i class="fa fa-check"></i><b>3</b> Arbres</a><ul>
<li class="chapter" data-level="3.1" data-path="arbres.html"><a href="arbres.html#coupures-cart-en-fonction-de-la-nature-des-variables"><i class="fa fa-check"></i><b>3.1</b> Coupures CART en fonction de la nature des variables</a><ul>
<li class="chapter" data-level="3.1.1" data-path="arbres.html"><a href="arbres.html#arbres-de-régression"><i class="fa fa-check"></i><b>3.1.1</b> Arbres de régression</a></li>
<li class="chapter" data-level="3.1.2" data-path="arbres.html"><a href="arbres.html#arbres-de-classification"><i class="fa fa-check"></i><b>3.1.2</b> Arbres de classification</a></li>
<li class="chapter" data-level="3.1.3" data-path="arbres.html"><a href="arbres.html#entrée-qualitative"><i class="fa fa-check"></i><b>3.1.3</b> Entrée qualitative</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="arbres.html"><a href="arbres.html#élagage"><i class="fa fa-check"></i><b>3.2</b> Élagage</a><ul>
<li class="chapter" data-level="3.2.1" data-path="arbres.html"><a href="arbres.html#élagage-pour-un-problème-de-régression"><i class="fa fa-check"></i><b>3.2.1</b> Élagage pour un problème de régression</a></li>
<li class="chapter" data-level="3.2.2" data-path="arbres.html"><a href="arbres.html#élagage-en-classification-binaire-et-matrice-de-coût"><i class="fa fa-check"></i><b>3.2.2</b> Élagage en classification binaire et matrice de coût</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Algorithmes avancés</b></span></li>
<li class="chapter" data-level="4" data-path="SVM.html"><a href="SVM.html"><i class="fa fa-check"></i><b>4</b> Support Vector Machine (SVM)</a><ul>
<li class="chapter" data-level="4.1" data-path="SVM.html"><a href="SVM.html#cas-séparable"><i class="fa fa-check"></i><b>4.1</b> Cas séparable</a></li>
<li class="chapter" data-level="4.2" data-path="SVM.html"><a href="SVM.html#cas-non-séparable"><i class="fa fa-check"></i><b>4.2</b> Cas non séparable</a></li>
<li class="chapter" data-level="4.3" data-path="SVM.html"><a href="SVM.html#lastuce-du-noyau"><i class="fa fa-check"></i><b>4.3</b> L’astuce du noyau</a></li>
<li class="chapter" data-level="4.4" data-path="SVM.html"><a href="SVM.html#support-vector-régression"><i class="fa fa-check"></i><b>4.4</b> Support vector régression</a></li>
<li class="chapter" data-level="4.5" data-path="SVM.html"><a href="SVM.html#svm-sur-les-données-spam"><i class="fa fa-check"></i><b>4.5</b> SVM sur les données spam</a></li>
<li class="chapter" data-level="4.6" data-path="SVM.html"><a href="SVM.html#exercices-1"><i class="fa fa-check"></i><b>4.6</b> Exercices</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="agregation.html"><a href="agregation.html"><i class="fa fa-check"></i><b>5</b> Agrégation : forêts aléatoires et gradient boosting</a><ul>
<li class="chapter" data-level="5.1" data-path="agregation.html"><a href="agregation.html#forets"><i class="fa fa-check"></i><b>5.1</b> Forêts aléatoires</a></li>
<li class="chapter" data-level="5.2" data-path="agregation.html"><a href="agregation.html#boosting"><i class="fa fa-check"></i><b>5.2</b> Gradient boosting</a><ul>
<li class="chapter" data-level="5.2.1" data-path="agregation.html"><a href="agregation.html#un-exemple-simple-en-régression"><i class="fa fa-check"></i><b>5.2.1</b> Un exemple simple en régression</a></li>
<li class="chapter" data-level="5.2.2" data-path="agregation.html"><a href="agregation.html#adaboost-et-logitboost-pour-la-classification-binaire."><i class="fa fa-check"></i><b>5.2.2</b> Adaboost et logitboost pour la classification binaire.</a></li>
<li class="chapter" data-level="5.2.3" data-path="agregation.html"><a href="agregation.html#exo:grad-boost"><i class="fa fa-check"></i><b>5.2.3</b> Exercices</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="deep.html"><a href="deep.html"><i class="fa fa-check"></i><b>6</b> Réseaux de neurones avec Keras</a></li>
<li class="chapter" data-level="7" data-path="dondes.html"><a href="dondes.html"><i class="fa fa-check"></i><b>7</b> Données déséquilibrées</a><ul>
<li class="chapter" data-level="7.1" data-path="dondes.html"><a href="dondes.html#critères-de-performance-pour-données-déséquilibrées"><i class="fa fa-check"></i><b>7.1</b> Critères de performance pour données déséquilibrées</a></li>
<li class="chapter" data-level="7.2" data-path="dondes.html"><a href="dondes.html#ré-équilibrage"><i class="fa fa-check"></i><b>7.2</b> Ré-équilibrage</a></li>
<li class="chapter" data-level="7.3" data-path="dondes.html"><a href="dondes.html#exercices-supplémentaires"><i class="fa fa-check"></i><b>7.3</b> Exercices supplémentaires</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="comp-algo.html"><a href="comp-algo.html"><i class="fa fa-check"></i><b>8</b> Comparaison d’algorithmes</a></li>
<li class="chapter" data-level="" data-path="références.html"><a href="références.html"><i class="fa fa-check"></i>Références</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lda" class="section level1">
<h1><span class="header-section-number">Chapitre 2</span> Analyse discriminante linéaire</h1>
<p>L’analyse discriminante linéaire est un algorithme de référence en classification supervisée. Il peut être appréhendé de deux façons complémentaires :</p>
<ul>
<li>une approche <strong>géométrique</strong> qui revient à chercher des hyperplans qui séparent au mieux les groupes ;</li>
<li>une approche <strong>modèle</strong> qui fait l’hypothèse que les lois des covariables sont des vecteurs gaussiens avec des valeurs de paramètres différentes pour chaque groupe.</li>
</ul>
<p>On considère <span class="math inline">\((x_1,y_1),\dots,(x_n,y_n)\)</span> un échantillon où <span class="math inline">\(x_i\)</span> est à valeurs dans <span class="math inline">\(\mathbb R^d\)</span> et <span class="math inline">\(y_i\)</span> dans <span class="math inline">\(\{0,1\}\)</span>. L’approche <strong>géométrique</strong> revient à chercher une droite de <span class="math inline">\(\mathbb R^d\)</span> d’équation <span class="math inline">\(a_1x_1+\dots+a_dx_d=0\)</span> telle que :</p>
<ul>
<li>les centres de gravité de chaque groupe projeté sur cette droite soit au mieux séparé <span class="math inline">\(\Longrightarrow\)</span> maximiser la distance inter-classe.</li>
<li>les observations projetés soient proches de leur centre de gravité projeté <span class="math inline">\(\Longrightarrow\)</span> minimiser la distance intra-classe.</li>
</ul>
<p>Le compromis entre ces deux distances s’obtient en maximisant le <strong>coefficient de Rayleigh</strong> qui est le quotient entre ces deux distance :
<span class="math display">\[J(a)=\frac{B(a)}{W(a)}=\frac{a^tBa}{a^tWa}\]</span>
où <span class="math inline">\(B\)</span> et <span class="math inline">\(W\)</span> sont les matrices inter et intra classes définies pas
<span class="math display">\[B=\frac{1}{n}\sum_{k=1}^Kn_k(g_k-g)(g_k-g)^t\quad\text{et}\quad W=\frac{1}{n}\sum_{k=1}^Kn_kV_k\quad\text{avec}\quad V_k=\frac{1}{n_k}\sum_{i:Y_i=k}(X_i-g_k)(X_i-g_k)^t.\]</span></p>
<p>Ici <span class="math inline">\(g\)</span> désigne le centre de gravité du nuage <span class="math inline">\(x_i,i=1,\dots,n\)</span> et <span class="math inline">\(g_k,k=0,1\)</span> les centres de gravité des deux groupes. La solution est donnée par un vecteur propre associé à la plus grande valeur propre de <span class="math inline">\(W^{-1}B\)</span>.</p>
<p>L’approche <strong>modèle</strong> fait l’hypothèse que les vecteurs <span class="math inline">\(X|Y=k,k=0,1\)</span> sont des vecteurs gaussiens d’espérance <span class="math inline">\(\mu_k\in\mathbb R^d\)</span> et de matrice de variance covariance <span class="math inline">\(\Sigma\)</span>. Ces paramètres sont estimés par maximum de vraisemblance et on déduit les probablités a posteriori par la <strong>formule de Bayes</strong> :
<span class="math display">\[\mathbf P(Y=k|X=x)=\frac{\pi_kf_{X|Y=k}(x)}{f(x)}\]</span>
Le lien entre ces deux approches est établi dans l’exercice <a href="lda.html#exr:exo-calcul-axes-lda">2.4</a>. Nous proposons dans cette partie quelques exercices pour mettre en œuvre et analyser des analyses discriminantes avec <strong>R</strong>.</p>
<div id="prise-en-main-lda-et-qda-sur-les-iris-de-fisher" class="section level2">
<h2><span class="header-section-number">2.1</span> Prise en main : LDA et QDA sur les iris de Fisher</h2>
<p>On considère les données sur les iris de Fisher.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="lda.html#cb18-1"></a><span class="kw">data</span>(iris)</span></code></pre></div>
<ol style="list-style-type: decimal">
<li><p>A l’aide de la fonction <strong>PCA</strong> du package <strong>FactoMineR</strong>, réaliser une ACP en utilisant comme variables actives les 4 variables quantitatives du jeu de données. On mettra la variable <code>Species</code> comme variable qualitative supplémentaire (option <code>quali.sup</code>).</p></li>
<li><p>Représenter le nuage des individus sur les 2 premiers axes de l’ACP en utilisant une couleur différente pour chaque espèce d’iris (option <code>habillage</code>).</p></li>
<li><p>A l’aide de la fonction <strong>lda</strong> du package <strong>MASS</strong>, effectuer une analyse discriminante linéaire permettant d’expliquer l’espèce par les 4 autres variables explicatives.</p></li>
<li><p>Représenter le nuage des individus sur les deux premiers axes de l’analyse discriminante linéaire (en utilisant une couleur différente pour chaque espèce d’iris).</p></li>
<li><p>Rappeler comment sont obtenues les coordonnées des individus sur chaque axe. En déduire une interprétation de la position des individus.</p></li>
<li><p>Comparer les représentations des questions 2 et 4.</p></li>
<li><p>Expliquer les sorties des commandes suivantes (<code>mod.lda</code> est l’objet construit avec la fonction <strong>lda</strong>).</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="lda.html#cb19-1"></a>score &lt;-<span class="st"> </span><span class="kw">predict</span>(mod.lda)<span class="op">$</span>x</span>
<span id="cb19-2"><a href="lda.html#cb19-2"></a><span class="kw">ldahist</span>(score[,<span class="dv">1</span>],iris[,<span class="dv">5</span>])    </span>
<span id="cb19-3"><a href="lda.html#cb19-3"></a><span class="kw">ldahist</span>(score[,<span class="dv">2</span>],iris[,<span class="dv">5</span>])    </span></code></pre></div>
<p>````</p></li>
<li><p>Exécuter et analyser les sorties de la commande</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="lda.html#cb20-1"></a>mod.lda2 &lt;-<span class="st"> </span><span class="kw">lda</span>(Species<span class="op">~</span>.,<span class="dt">data=</span>iris,<span class="dt">CV=</span><span class="ot">TRUE</span>)</span></code></pre></div></li>
<li><p>Comparer, en terme d’erreur de prévision, les performances de LDA et QDA.</p></li>
</ol>
</div>
<div id="un-cas-avec-beaucoup-de-classes" class="section level2">
<h2><span class="header-section-number">2.2</span> Un cas avec beaucoup de classes</h2>
<p>On considère les jeux de données <strong>Vowel</strong> (training et test) qui se trouvent à cet <a href="https://statweb.stanford.edu/~hastie/ElemStatLearn/">url</a>. On peut les importer avec</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="lda.html#cb21-1"></a>dapp &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.train&quot;</span>)[,<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb21-2"><a href="lda.html#cb21-2"></a>dtest &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;https://web.stanford.edu/~hastie/ElemStatLearn/datasets/vowel.test&quot;</span>)[,<span class="op">-</span><span class="dv">1</span>]</span></code></pre></div>
<ol style="list-style-type: decimal">
<li><p>Expliquer le problème.</p></li>
<li><p>Effectuer une analyse discriminante linéaire (uniquement avec les données d’apprentissage) et visualiser les individus sur les 2 premiers axes de l’analyse discriminante. On pourra utiliser <strong>predict</strong>.</p></li>
<li><p>La fonction suivante permet de choisir les axes à visualiser, ainsi que les centres de gravité projetés des groupes.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="lda.html#cb22-1"></a>repres_axes &lt;-<span class="st"> </span><span class="cf">function</span>(prev,cdg,<span class="dt">axe1=</span><span class="dv">1</span>,<span class="dt">axe2=</span><span class="dv">2</span>){</span>
<span id="cb22-2"><a href="lda.html#cb22-2"></a>  cdg &lt;-<span class="st"> </span>prev <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(y) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">summarise_all</span>(mean)</span>
<span id="cb22-3"><a href="lda.html#cb22-3"></a>  nom1 &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;LD&quot;</span>,<span class="kw">as.character</span>(axe1),<span class="dt">sep=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb22-4"><a href="lda.html#cb22-4"></a>  nom2 &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;LD&quot;</span>,<span class="kw">as.character</span>(axe2),<span class="dt">sep=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb22-5"><a href="lda.html#cb22-5"></a>  <span class="kw">ggplot</span>(prev)<span class="op">+</span><span class="kw">aes_string</span>(<span class="dt">x=</span><span class="kw">as.name</span>(nom1),<span class="dt">y=</span><span class="kw">as.name</span>(nom2))<span class="op">+</span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color=</span>y))<span class="op">+</span><span class="kw">geom_point</span>(<span class="dt">data=</span>cdg,<span class="kw">aes</span>(<span class="dt">color=</span>y),<span class="dt">shape=</span><span class="dv">17</span>,<span class="dt">size=</span><span class="dv">4</span>)<span class="op">+</span><span class="kw">theme_classic</span>()</span>
<span id="cb22-6"><a href="lda.html#cb22-6"></a>}</span></code></pre></div>
<p>Étudier la pertinence des axes.</p></li>
<li><p>Représenter les individus sur le premier plan factoriel de l’ACP, on utilisera une couleur différente pour chaque groupe. On pourra utiliser le package <strong>FactoMineR</strong>.</p></li>
<li><p>Comparer cette projection avec celle obtenue par l’analyse discriminante linéaire.</p></li>
<li><p>Évaluer la performance de la <strong>lda</strong> sur les données test. Comparer avec l’analyse discriminante quadratique.</p></li>
<li><p>Expliquer comment on peut faire de la prévision en réduisant la dimension de l’espace des <span class="math inline">\(X\)</span>.</p></li>
<li><p>Proposer une méthode permettant de choisir le meilleur nombre d’axes. On pourra notamment utiliser l’option <code>dimen</code> de la fonction <strong>predict.lda</strong>.</p></li>
</ol>
</div>
<div id="grande-dimension-reconnaissance-de-phonèmes" class="section level2">
<h2><span class="header-section-number">2.3</span> Grande dimension : reconnaissance de phonèmes</h2>
<p>On considère le jeu de données <strong>phoneme</strong> téléchargeable à l’url <a href="https://github.com/cran/ElemStatLearn/blob/master/data/phoneme.RData">https://github.com/cran/ElemStatLearn/blob/master/data/phoneme.RData</a>.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="lda.html#cb23-1"></a><span class="kw">load</span>(<span class="st">&quot;data/phoneme.RData&quot;</span>)</span>
<span id="cb23-2"><a href="lda.html#cb23-2"></a><span class="kw">data</span>(phoneme)</span>
<span id="cb23-3"><a href="lda.html#cb23-3"></a>donnees &lt;-<span class="st"> </span>phoneme[,<span class="op">-</span><span class="dv">258</span>]</span></code></pre></div>
<ol style="list-style-type: decimal">
<li><p>Expliquer le problème et représenter pour chaque groupe la courbe moyenne.</p></li>
<li><p>Séparer les données en un échantillon d’apprentissage de taille 3000 et un échantillon test de taille 1509.</p></li>
<li><p>Effectuer une analyse discriminante linéaire et une analyse discriminante quadratique sur les données d’apprentissage uniquement. Évaluer les performances de ces deux approches sur les données test.</p></li>
<li><p>Quels peuvent être les intérêts d’effectuer une analyse discriminante régularisée dans ce contexte ? Effectuer une telle analyse à l’aide de la fonction <strong>rda</strong> du package <strong>klaR</strong>.</p></li>
<li><p>Sélectionner les paramètres de régularisation à l’aide du package <strong>caret</strong>. Comparer le nouveau modèle aux précédents.</p></li>
</ol>
</div>
<div id="exercices" class="section level2">
<h2><span class="header-section-number">2.4</span> Exercices</h2>

<div class="exercise">
<span id="exr:exo-preuve-bayse" class="exercise"><strong>Exercice 2.1  (Optimalité de la règle de Bayes)  </strong></span>
</div>

<p>On dispose de <span class="math inline">\(n\)</span> observations <span class="math inline">\((x_1,y_1),\dots,(x_n,y_n)\)</span> telles que <span class="math inline">\(x_i\in\mathbb R^p\)</span> et <span class="math inline">\(y_i\in\{0,1\}\)</span> pour <span class="math inline">\(i=1,\dots,n\)</span>. On souhaite expliquer les sorties <span class="math inline">\(y_i\)</span> par les entrées <span class="math inline">\(x_i\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Rappeler la définition d’une règle de prévision.</p></li>
<li><p>Rappeler la définition de la règle de Bayes <span class="math inline">\(g^\star\)</span> et de l’erreur de Bayes <span class="math inline">\(L^\star\)</span>.</p></li>
<li><p>Soit <span class="math inline">\(g\)</span> une règle de décision. Montrer que
<span class="math display">\[\mathbf P(g(X)\neq Y|X=x)=1-(\mathbf 1_{g(x)=1}\eta(x)+\mathbf 1_{g(x)=0}(1-\eta(x)))\]</span>
où <span class="math inline">\(\eta(x)=\mathbf P(Y=1|X=x)\)</span>.</p></li>
<li><p>En déduire que pour tout <span class="math inline">\(x\in\mathcal X\)</span> et pour toute règle <span class="math inline">\(g\)</span>
<span class="math display">\[\mathbf P(g(X)\neq Y|X=x)-\mathbf P(g^\star(X)\neq Y|X=x)\geq 0.\]</span>
Conclure.</p></li>
<li><p>On considère <span class="math inline">\((X,Y)\)</span> un couple aléatoire à valeurs dans <span class="math inline">\(\mathbb R\times\{0,1\}\)</span> tel que
<span class="math display">\[\begin{equation*}
X\sim\mathcal U[-2,2]\quad\text{et}\quad (Y|X=x)\sim\left\{
  \begin{array}{ll}
\mathcal B(1/5) &amp; \textrm{si } x\leq 0 \\
\mathcal B(9/10) &amp; \textrm{si } x&gt;0
  \end{array}\right.
\end{equation*}\]</span>
où <span class="math inline">\(\mathcal U[a,b]\)</span> désigne la loi uniforme sur <span class="math inline">\([a,b]\)</span> et <span class="math inline">\(\mathcal B(p)\)</span> la loi de Bernoulli de paramètre <span class="math inline">\(p\)</span>. Calculer la règle de Bayes et l’erreur de Bayes.</p></li>
</ol>

<div class="exercise">
<span id="exr:exo-evm-lda" class="exercise"><strong>Exercice 2.2  (MV pour LDA)  </strong></span>
</div>

<p>On cherche à expliquer une variable aléatoire <span class="math inline">\(Y\)</span> à valeurs dans <span class="math inline">\(\{0,1\}\)</span> par une variable aléatoire <span class="math inline">\(X\)</span> à valeurs dans <span class="math inline">\(\mathbb R\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Quels sont les paramètres à estimer dans le modèle d’analyse discriminante linéaire.</p></li>
<li><p>Calculer la vraisemblance conditionnelle à <span class="math inline">\(Y\)</span> et en déduire les estimateurs des paramètres des lois gaussiennes.</p></li>
<li><p>Comparer les estimateurs obtenus avec ceux du cours.</p></li>
</ol>

<div class="exercise">
<span id="exr:exo-fondiscri-lda" class="exercise"><strong>Exercice 2.3  (Fonctions linéaires discriminantes)  </strong></span>
</div>

<p>On cherche à expliquer une variable aléatoire <span class="math inline">\(Y\)</span> à valeurs dans <span class="math inline">\(\{0,1\}\)</span> par une variable aléatoire <span class="math inline">\(X\)</span> à valeurs dans <span class="math inline">\(\mathbb R^p\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Rappeler le modèle d’analyse discriminante linéaire.</p></li>
<li><p>Soit <span class="math inline">\(x\in\mathbb R^p\)</span> un nouvel individu. Montrer que la règle qui consiste à affecter <span class="math inline">\(x\)</span> dans le groupe qui maximise <span class="math inline">\(\mathbf P(Y=k|X=x)\)</span> est équivalente à la règle qui consiste à affecter <span class="math inline">\(x\)</span> dans le groupe qui maximise les fonctions linéaires discriminantes (on prendra soin de rappeler la définition des fonctions linéaires discriminantes).</p></li>
</ol>

<div class="exercise">
<span id="exr:exo-calcul-axes-lda" class="exercise"><strong>Exercice 2.4  (Approche géométrique de la LDA)  </strong></span>
</div>

<p>On considère un <span class="math inline">\(n\)</span>-échantillon i.i.d. <span class="math inline">\((x_1,y_1),\dots,(x_n,y_n)\)</span> où <span class="math inline">\(x_i\)</span> est à valeurs dans <span class="math inline">\(\mathbb R^2\)</span> et <span class="math inline">\(y_i\)</span> dans <span class="math inline">\(\{0,1\}\)</span>. On cherche une droite vectorielle <span class="math inline">\(a\)</span> telle que les projections de chaque groupe sur <span class="math inline">\(a\)</span> soient séparées “au mieux”. Dit autrement, on cherche <span class="math inline">\(a\)</span> telle que</p>
<ul>
<li>la distance entre les centres de gravité
<span class="math display">\[g_0=\frac{1}{\mbox{card}\{i:y_i=0\}}\sum_{i:y_i=0}x_i\quad\textrm{et}\quad g_1=\frac{1}{\mbox{card}\{i:y_i=1\}}\sum_{i:y_i=1}x_i\]</span>
projetés sur <span class="math inline">\(a\)</span> soit maximale (cette distance est appelée distance interclasse) ;</li>
<li>la distance entre les projections des individus et leur centre de gravité soit minimale (distance interclasse).</li>
</ul>
<p>Pour un vecteur <span class="math inline">\(u\)</span> de <span class="math inline">\(\mathbb R^2\)</span>, on désigne par <span class="math inline">\(\pi_a(u)\)</span> son projeté sur la droite engendrée par <span class="math inline">\(a\)</span>. Sans perte de généralité on supposera dans un premier temps que <span class="math inline">\(a\)</span> est de norme 1.</p>
<ol style="list-style-type: decimal">
<li><p>Rappeler les définitions des variances totale <span class="math inline">\(V\)</span>, intra <span class="math inline">\(W\)</span> et inter <span class="math inline">\(B\)</span> des observations <span class="math inline">\((x_1,y_1),\dots,(x_n,y_n)\)</span>.</p></li>
<li><p>Pour <span class="math inline">\(u\)</span> fixé dans <span class="math inline">\(\mathbb R^2\)</span>, exprimer <span class="math inline">\(\pi_a(u)\)</span> en fonction de <span class="math inline">\(u\)</span> et <span class="math inline">\(a\)</span> et en déduire que <span class="math inline">\(\|\pi_a(u)\|^2=a^tuu^ta\)</span>.</p></li>
<li><p>Exprimer les variances totale <span class="math inline">\(V(a)\)</span>, intra <span class="math inline">\(W(a)\)</span> et inter <span class="math inline">\(B(a)\)</span> projetées sur <span class="math inline">\(a\)</span> en fonction des variances calculées à la question 1.</p></li>
<li><p>On cherche maintenant à maximiser
<span class="math display">\[J(a)=\frac{B(a)}{W(a)}\]</span>
ou encore à
<span class="math display" id="eq:pbmaxrayleigh">\[\begin{equation}
\textrm{maximiser }B(a)\quad\textrm{sous la contrainte}\quad W(a)=1.
\tag{2.1}
\end{equation}\]</span>
La méthode des multiplicateurs de Lagrange permet de résoudre un tel problème. La solution du problème de maximisation d’une fonction <span class="math inline">\(f(x)\)</span> sujette à <span class="math inline">\(h(x)=0\)</span> s’obtient en résolvant l’équation
<span class="math display">\[\frac{\partial L(x,\lambda)}{\partial x}=0,\quad\textrm{où}\quad L(x,\lambda)=f(x)+\lambda h(x).\]</span></p>
<ol style="list-style-type: lower-alpha">
<li><p>Montrer que la solution du problème <a href="lda.html#eq:pbmaxrayleigh">(2.1)</a> est un vecteur propre de <span class="math inline">\(W^{-1}B\)</span> associé à la plus grande valeur propre de <span class="math inline">\(W^{-1}B\)</span>. On note <span class="math inline">\(a^\star\)</span> cette solution.</p></li>
<li><p>Montrer que <span class="math inline">\(a^\star\)</span> est colinéaire à <span class="math inline">\(W^{-1}(g_1-g_0)\)</span>. On pourra admettre que, dans le cas de 2 groupes, on a
<span class="math display">\[B=\frac{n_0n_1}{n^2}(g_1-g_0)(g_1-g_0)^t.\]</span></p></li>
<li><p>On considère la règle géométrique d’affectation qui consiste à classer un nouvel individu <span class="math inline">\(x\in\mathbb R^p\)</span> au groupe 1 si son projeté sur <span class="math inline">\(a^\star\)</span> est plus proche de <span class="math inline">\(\pi_{a^\star}(g_1)\)</span> que de <span class="math inline">\(\pi_{a^\star}(g_0)\)</span>. Montrer que <span class="math inline">\(x\)</span> sera affecté au groupe 1 si
<span class="math display">\[S(x)=x^tW^{-1}(g_1-g_0)&gt;s\]</span>
où on exprimera <span class="math inline">\(s\)</span> en fonction de <span class="math inline">\(g_0\)</span>, <span class="math inline">\(g_1\)</span> et <span class="math inline">\(W\)</span>.</p></li>
<li><p>Montrer que cette règle est équivalente à choisir le groupe qui minimise la distance de Mahalanobis
<span class="math display">\[d(x,g_k)=(x-g_k)^tW^{-1}(x-g_k),\quad k=0,1.\]</span></p></li>
<li><p>On revient maintenant à l’approche probabiliste de l’analyse discriminante linéaire vue en cours et on considère la règle d’affectation qui consiste à décider "groupe 1’’ si <span class="math inline">\(\mathbf P(Y=1|X=x)\geq 0.5\)</span>. Montrer que dans ce cas, un nouvel individu <span class="math inline">\(x\)</span> est affecter au groupe 1 si :
<span class="math display">\[S(x)=x^t\Sigma^{-1}(\mu_1-\mu_0)&gt;\frac{1}{2}(\mu_1+\mu_0)^t\Sigma^{-1}(\mu_1-\mu_0)-\log\left(\frac{\pi_1}{\pi_0}\right).\]</span>
Conclure.</p></li>
</ol></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="caret.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="arbres.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["TUTO_ML.pdf"],
"toc": {
"collapse": "subsection",
"sharing": {
"facebook": true,
"github": true,
"twitter": true
}
},
"highlight": "tango"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
