[
["index.html", "Machine learning Présentation", " Machine learning Laurent Rouvière 2020-06-30 Présentation Ce tutoriel présente une introduction au machine learning avec R. Les thèmes suivants sont abordés : Estimation du risque, présentation du package caret SVM Arbres (essentiellement avec dplyr) Agrégation (représentations standards et avec ggplot2) On pourra trouver des supports de cours ainsi que les données utilisées à l’adresse suivante https://lrouviere.github.io/ml_lecture/. Des compléments sur les outils du tidyverse pourront être consultés dans le très complet document de (???) ainsi que les ouvrages de (???) et de (???). "],
["caret.html", "Chapitre 1 Estimation du risque avec caret 1.1 Notion de risque en apprentissage supervisé 1.2 La validation croisée 1.3 Le package caret 1.4 Compléments", " Chapitre 1 Estimation du risque avec caret 1.1 Notion de risque en apprentissage supervisé L’apprentissage supervisée consiste à expliquer ou prédire une sortie \\(y\\in\\mathcal Y\\) par des entrées \\(x\\in\\mathcal X\\) (le plus souvent \\(\\mathcal X=\\mathbb R^p\\)). Cela revient à trouver un algorithme ou machine représenté par une fonction \\[f:\\mathcal X\\to\\mathcal Y\\] qui à une nouvelle observation \\(x\\) associe la prévision \\(f(x)\\). Bien entendu pour un problème donné, on va chercher le meilleur algorithme. Cette notion nécessite la définition de critères que l’on va chercher à optimiser. Les critère sont le plus souvent définie à partir du fonction de perte \\[\\begin{align*} \\ell:\\mathcal Y \\times\\mathcal Y &amp; \\mapsto \\mathbb R^+ \\\\ (y,y^\\prime) &amp; \\to\\ell(y,y^\\prime) \\end{align*}\\] où \\(\\ell(y,y^\\prime)\\) représentera l’erreur (ou la perte) pour la prévision \\(y^\\prime\\) par rapport à l’observation \\(y\\). Si on représente le phénomène d’intérêt par un couple aléatoire \\((X,Y)\\) à valeurs dans \\(\\mathcal X\\times\\mathcal Y\\), on mesurera la performance d’un algorithme \\(f\\) par son risque \\[\\mathcal R(f)=\\mathbf E[\\ell(Y,f(X))].\\] Trouver le meilleur algorithme revient alors à trouver \\(f\\) qui minimise \\(\\mathcal R(f)\\). Bien entendu, ce cadre possède une utilité limitée en pratique puisqu’on ne connaît jamais la loi de \\((X,Y)\\), on ne pourra donc jamais calculé le vrai risque d’un algorithme \\(f\\). Tout le problème va donc être de trouver l’algorithme qui a le plus petit risque à partir de \\(n\\) observations \\((x_1,y_1),\\dots,(x_n,y_n)\\). Nous verrons dans les chapitres suivants plusieurs façons de construire des algorithmes mais dans tous les cas, un algorithme est représenté par une fonction \\[f_n:\\mathcal X\\times(\\mathcal X\\times\\mathcal Y)^n\\to\\mathcal Y\\] qui pour une nouvelle donnée \\(x\\) renverra la prévision \\(f_n(x)\\) calculée à partir de l’échantillon qui vit dans \\((\\mathcal X\\times\\mathcal Y)^n\\). Dès lors la question qui se pose est de calculer (ou plutôt d’estimer) le risque (inconnu) \\(\\mathcal R(f_n)\\) d’un algorithme \\(f_n\\). Les techniques classiques reposent sur des algorithmes de type validation croisée. Nous les mettons en œuvre dans cette partie pour un algorithme simple : les \\(k\\) plus proches voisins. On commencera par faire les algorithmes “à la main” puis nous utiliserons le package caret qui permet de calculer des risques pour quasiment tous les algorithmes que l’on retrouver en apprentissage supervisé. 1.2 La validation croisée On cherche à expliquer une variable binaire \\(Y\\) par deux variables quantitatives \\(X_1\\) et \\(X_2\\) à l’aide du jeu de données suivant &gt; n &lt;- 2000 &gt; set.seed(12345) &gt; X1 &lt;- runif(n) &gt; X2 &lt;- runif(n) &gt; set.seed(9012) &gt; R1 &lt;- X1&lt;=0.25 &gt; R2 &lt;- (X1&gt;0.25 &amp; X2&gt;=0.75) &gt; R3 &lt;- (X1&gt;0.25 &amp; X2&lt;0.75) &gt; Y &lt;- rep(0,n) &gt; Y[R1] &lt;- rbinom(sum(R1),1,0.25) &gt; Y[R2] &lt;- rbinom(sum(R2),1,0.25) &gt; Y[R3] &lt;- rbinom(sum(R3),1,0.75) &gt; donnees &lt;- data.frame(X1,X2,Y) &gt; donnees$Y &lt;- as.factor(donnees$Y) &gt; ggplot(donnees)+aes(x=X1,y=X2,color=Y)+geom_point() On considère la perte indicatrice : \\(\\ell(y,y^\\prime)=\\mathbf 1_{y\\neq y^\\prime}\\), le risque d’un algorithme \\(f\\) est donc \\[\\mathcal R(f)=\\mathbf E[\\mathbf 1_{Y\\neq f(X)}]=\\mathbf P(Y\\neq f(X)),\\] il est appelé probabilité d’erreur ou erreur de classification. Séparer le jeu de données en un échantillon d’apprentissage dapp de taille 1500 et un échantillon test dtest de taille 500. &gt; set.seed(234) &gt; indapp &lt;- sample(nrow(donnees),1500) &gt; dapp &lt;- donnees[indapp,] &gt; dtest &lt;- donnees[-indapp,] On considère la règle de classification des \\(k\\) plus proches voisins. Pour un entier \\(k\\) plus petit que \\(n\\) et un nouvel individu \\(x\\), cette règle affecte à \\(x\\) le label majoritaire des \\(k\\) plus proches voisins de \\(x\\). Sur R on utilise la fonction knn du package class. On peut par exemple obtenir les prévisions des individus de l’échantillon test de la règle des 3 plus proches voisins avec &gt; library(class) &gt; knn3 &lt;- knn(dapp[,1:2],dtest[,1:2],cl=dapp$Y,k=3) Calculer l’erreur de classification de la règle des 3 plus proches voisins sur les données test (procédure validation hold out). &gt; mean(knn3!=dtest$Y) [1] 0.338 Expliquer la fonction knn.cv. Cette fonction permet, pour la règle des plus proches voisins, de prédire le groupe de chaque individu par validation croisée leave-one-out : \\[\\widehat y_i=g_{k,i}(x_i),\\quad i=1,\\dots,n\\] où \\(g_{k,i}\\) désigne la règle de \\(k\\) plus proche voisins construites à partir de l’échantillon amputé de la \\(i\\)ème observation. Calculer l’erreur de classification de la règle des 3 plus proches voisins par validation croisée leave-one-out. &gt; prev_cv &lt;- knn.cv(donnees[,-3],cl=donnees$Y,k=3) On peut alors estimer l’erreur de la règle des 10 ppv par \\[\\frac{1}{n}\\sum_{i=1}^n1_{g_{k,i}(x_i)\\neq y_i}.\\] &gt; mean(prev_cv!=donnees$Y) [1] 0.334 On considère le vecteur de plus proches voisins suivant : &gt; K_cand &lt;- seq(1,500,by=20) Sélectionner une valeur de \\(k\\) dans ce vecteur à l’aide d’une validation hold out et d’une leave-one-out : On calcule l’erreur de classification par validation hold out pour chaque valeur de \\(k\\) : &gt; err.ho &lt;- rep(0,length(K_cand)) &gt; for (i in 1:length(K_cand)){ + knni &lt;- knn(dapp[,1:2],dtest[,1:2],cl=dapp$Y,k=K_cand[i]) + err.ho[i] &lt;- mean(knni!=dtest$Y) + } Puis on choisit la valeur de \\(k\\) pour laquelle l’erreur est minimale. &gt; K_cand[which.min(err.ho)] [1] 41 On fait la même chose avec la validation croisée leave-one-out : &gt; err.loo &lt;- rep(0,length(K_cand)) &gt; for (i in 1:length(K_cand)){ + knni &lt;- knn.cv(donnees[,-3],cl=donnees$Y,k=K_cand[i]) + err.loo[i] &lt;- mean(knni!=donnees$Y) + } &gt; K_cand[which.min(err.loo)] [1] 121 Faire la même chose à l’aide d’une validation croisée 10 blocs. On pourra construire les blocs avec &gt; set.seed(2345) &gt; blocs &lt;- caret::createFolds(1:nrow(donnees),10,returnTrain = TRUE) &gt; err.cv &lt;- rep(0,length(K_cand)) &gt; prev &lt;- donnees$Y &gt; for (i in 1:length(K_cand)){ + for (j in 1:length(blocs)){ + train &lt;- donnees[blocs[[j]],] + test &lt;- donnees[-blocs[[j]],] + prev[-blocs[[j]]] &lt;- knn(train[,1:2],test[,1:2],cl=train$Y,k=K_cand[i]) + } + err.cv[i] &lt;- mean(prev!=donnees$Y) + } &gt; K_cand[which.min(err.cv)] [1] 101 1.3 Le package caret Dans la partie précédente, nous avons utiliser des méthodes de validation croisée pour sélectionner le nombre de voisins dans l’algorithme des plus proches voisins. L’approche revenait à * estimer un risque pour une grille de valeurs candidates de \\(k\\) * choisir la valeur de \\(k\\) qui minimise le risque estimé. Cette pratique est courante en machine learning : on la retrouve fréquemment pour calibrer les algorithmes. Le protocole est toujours le même, pour un méthode donnée il faut spécifier : une grille de valeurs pour les paramètres un risque un algorithme pour estimer le risque. Le package caret permet d’appliquer ce protocole pour plus de 200 algorithmes machine learning. On pourra trouver une documentation complète à cette url http://topepo.github.io/caret/index.html. Deux fonctions sont à utiliser : traincontrol qui permettra notamment de spécifier l’algorithme pour estimer le risque ainsi que les paramètres de cet algorithme ; train dans laquelle on renseignera les données, la grille de candidats… On reprend les données de la partie précédente. Expliquer les sorties des commandes &gt; library(caret) &gt; set.seed(321) &gt; ctrl1 &lt;- trainControl(method=&quot;LGOCV&quot;,number=1) &gt; KK &lt;- data.frame(k=K_cand) &gt; caret.ho &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl1,tuneGrid=KK) &gt; caret.ho k-Nearest Neighbors 2000 samples 2 predictor 2 classes: &#39;0&#39;, &#39;1&#39; No pre-processing Resampling: Repeated Train/Test Splits Estimated (1 reps, 75%) Summary of sample sizes: 1500 Resampling results across tuning parameters: k Accuracy Kappa 1 0.602 0.1956346 21 0.690 0.3649415 41 0.694 0.3736696 61 0.706 0.3992546 81 0.700 0.3867338 101 0.712 0.4122641 121 0.700 0.3882944 141 0.706 0.4017971 161 0.700 0.3903629 181 0.702 0.3941710 201 0.700 0.3898471 221 0.696 0.3806637 241 0.692 0.3714491 261 0.698 0.3829078 281 0.692 0.3693074 301 0.696 0.3764358 321 0.682 0.3474407 341 0.682 0.3468831 361 0.678 0.3352601 381 0.672 0.3214167 401 0.668 0.3113633 421 0.666 0.3057172 441 0.658 0.2853800 461 0.658 0.2841354 481 0.654 0.2732314 Accuracy was used to select the optimal model using the largest value. The final value used for the model was k = 101. &gt; plot(caret.ho) On obtient ici l’accuracy (1 moins l’erreur de classification) pour chaque valeur de \\(k\\) calculé par validation hold out. Cette technique a été précisée dans la fonction trainControl via l’option method=\"LGOCV\". Un autre indicateur est calculé : le kappa de Cohen. Cet indicateur peut se révéler pertinent en présence de données déséquilibrées, on pourra trouver de l’information sur cet indicateur dans ce document https://lrouviere.github.io/INP-HB/cours_don_des.pdf En modifiant les paramètres du code précédent, retrouver les résultats de la validation hold out de la partie précédente. On pourra utiliser l’option index dans la fonction trainControl. &gt; ctrl2 &lt;- trainControl(method=&quot;LGOCV&quot;,number=1,index=list(indapp)) &gt; caret.ho2 &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl2,tuneGrid=KK) &gt; caret.ho2$bestTune k 3 41 On retrouve bien la même valeur de \\(k\\). Utiliser caret pour sélectionner \\(k\\) par validation croisée leave-one-out. &gt; ctrl3 &lt;- trainControl(method=&quot;LOOCV&quot;,number=1) &gt; caret.loo &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl3,tuneGrid=KK) &gt; caret.loo$bestTune k 7 121 Faire de même pour la validation croisée 10 blocs en gardant les mêmes blocs que dans la partie précédente. &gt; ctrl4 &lt;- trainControl(method=&quot;cv&quot;,index=blocs) &gt; caret.cv &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl4,tuneGrid=KK) &gt; caret.cv$bestTune k 6 101 1.4 Compléments 1.4.1 Calcul parallèle Les validations croisés peuvent se révéler coûteuses en temps de calcul. On utilise souvent des techniques de parallélisation pour améliorer les performances computationnelles. Ces techniques sont relativement facile à mettre en œuvre avec caret, on peut par exemple utiliser la librairie doParallel : &gt; library(doParallel) &gt; cl &lt;- makePSOCKcluster(1) &gt; registerDoParallel(cl) &gt; system.time(ee3 &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl4,tuneGrid=KK)) user system elapsed 12.708 0.042 12.803 &gt; stopCluster(cl) &gt; cl &lt;- makePSOCKcluster(4) &gt; registerDoParallel(cl) &gt; system.time(ee3 &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl4,tuneGrid=KK)) user system elapsed 0.504 0.013 5.703 &gt; stopCluster(cl) 1.4.2 Répéter les méthodes de rééchantillonnage Les méthodes d’estimation du risque présentées dans cette partie (hold out, validation croisée) sont basées sur du rééchantillonnage. Elles peuvent se révéler sensible à la manière de couper l’échantillon. C’est pourquoi il est recommandé de les répéter plusieurs fois et de moyenner les erreurs sur les répétitions. Ces répétitions sont très faciles à mettre en œuvre avec caret, par exemple pour la validation hold out on utilise l’option number: &gt; ctrl &lt;- trainControl(method=&quot;LGOCV&quot;,number=5) &gt; caret.ho.rep &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl,tuneGrid=KK) la validation croisée on utilise les options repeatedcv et repeats : &gt; ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;,repeats=5) &gt; caret.ho.rep &lt;- train(Y~.,data=donnees,method=&quot;knn&quot;,trControl=ctrl,tuneGrid=KK) 1.4.3 Modifier le risque Enfin nous avons uniquement considéré l’erreur de classification. Il est bien entendu possible d’utiliser d’autres risques pour évaluer les performances. C’est l’option metric de la fonction train qui permet généralement de spécifier le risque, si on est par exemple intéressé par l’aire sur la courbe ROC (AUC) on fera : &gt; donnees1 &lt;- donnees &gt; names(donnees1)[3] &lt;- &quot;Class&quot; &gt; levels(donnees1$Class) &lt;- c(&quot;G0&quot;,&quot;G1&quot;) &gt; ctrl &lt;- trainControl(method=&quot;LGOCV&quot;,number=1,classProbs=TRUE,summary=twoClassSummary) &gt; caret.auc &lt;- train(Class~.,data=donnees1,method=&quot;knn&quot;,trControl=ctrl,tuneGrid=KK,metric=&quot;ROC&quot;) &gt; caret.auc k-Nearest Neighbors 2000 samples 2 predictor 2 classes: &#39;G0&#39;, &#39;G1&#39; No pre-processing Resampling: Repeated Train/Test Splits Estimated (1 reps, 75%) Summary of sample sizes: 1500 Resampling results across tuning parameters: k ROC Sens Spec 1 0.5904827 0.5758929 0.6050725 21 0.6945765 0.6294643 0.7789855 41 0.7206506 0.6250000 0.7789855 61 0.7291424 0.6250000 0.7753623 81 0.7247752 0.6205357 0.7934783 101 0.7241282 0.6250000 0.7934783 121 0.7204322 0.6250000 0.8007246 141 0.7198580 0.6294643 0.7862319 161 0.7221791 0.6250000 0.7826087 181 0.7225188 0.6205357 0.7826087 201 0.7170597 0.6205357 0.7934783 221 0.7143100 0.6160714 0.7862319 241 0.7196801 0.6205357 0.7898551 261 0.7150055 0.6205357 0.7898551 281 0.7184669 0.6116071 0.7898551 301 0.7187096 0.5892857 0.7971014 321 0.7187904 0.5758929 0.8007246 341 0.7178927 0.5491071 0.8079710 361 0.7158789 0.5178571 0.8224638 381 0.7177957 0.5089286 0.8224638 401 0.7168818 0.4776786 0.8297101 421 0.7174964 0.4598214 0.8478261 441 0.7134770 0.4330357 0.8623188 461 0.7141401 0.4241071 0.8695652 481 0.7131535 0.4107143 0.8659420 ROC was used to select the optimal model using the largest value. The final value used for the model was k = 61. "],
["SVM.html", "Chapitre 2 Support Vector Machine (SVM) 2.1 Cas séparable 2.2 Cas non séparable 2.3 L’astuce du noyau 2.4 Exercices", " Chapitre 2 Support Vector Machine (SVM) Etant donnée un échantillon \\((x_1,y_1),\\dots,(x_n,y_n)\\) où les \\(x_i\\) sont à valeurs dans \\(\\mathbb R^p\\) et les \\(y_i\\) sont binaires à valeurs dans \\(\\{-1,1\\}\\), l’approche SVM cherche le meilleur hyperplan en terme de séparation des données. Globalement on veut que les 1 se trouvent d’un coté de l’hyperplan et les -1 de l’autre. Dans cette partie on propose d’étudier la mise en œuvre de cet algorithme tout d’abord dans le cas idéal où les données sont séparables puis dans le cas plus réel où elles ne le sont pas. Nous verrons ensuite comment introduire de la non linéarité ne utilisant l’astuce du noyau. 2.1 Cas séparable Le cas séparable est le cas facile : il correspond à la situation où il existe effectivement un (même plusieurs) hyperplan(s) qui sépare(nt) parfaitement les 1 des -1. Il n’arrive jamais en pratique mais il convient de l’étudier pour comprendre comment est construit l’algorithme. Dans ce cas on cherche l’hyperplan d’équation \\(\\langle w,x\\rangle+b=w^tx+b=0\\) tel que la marge (qui peut être vue comme la distance entre les observations les plus priche de l’hyperplan et l’hyperplan) soit maximale. Mathématiquement le problème se réécrit comme un problème d’optimisation sous contraintes : \\[\\begin{equation} \\min_{w,b}\\frac{1}{2}\\|w\\|^2 \\tag{2.1} \\end{equation}\\] \\[\\text{sous les contraintes } y_i(w^tx_i+b)\\geq 1,\\ i=1,\\dots,n.\\] La solution s’obtient de façon classique en résolvant le problème dual et elle s’écrit comme une combinaison linéaire des \\(x_i\\) \\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\] De plus, les conditions KKT impliquent que pour tout \\(i=1,\\dots,n\\): \\(\\alpha_i^\\star=0\\) ou \\(y_i(x_i^tw+b)-1=0.\\) Ces conditions entraînent que \\(w^\\star\\) s’écrit comme uen combinaison linéaire de quelques points qui se trouvent sur la marge. Nous proposons maintenant de retrouver ces points et de tracer la marge sur un exemple simple. On considère le nuage de points suivant : &gt; n &lt;- 20 &gt; set.seed(123) &gt; X1 &lt;- scale(runif(n)) &gt; set.seed(567) &gt; X2 &lt;- scale(runif(n)) &gt; Y &lt;- rep(-1,n) &gt; Y[X1&gt;X2] &lt;- 1 &gt; Y &lt;- as.factor(Y) &gt; donnees &lt;- data.frame(X1=X1,X2=X2,Y=Y) &gt; p &lt;- ggplot(donnees)+aes(x=X2,y=X1,color=Y)+geom_point() &gt; p La fonction svm du package e1071 permet d’ajuster une SVM : &gt; library(e1071) &gt; mod.svm &lt;- svm(Y~.,data=donnees,kernel=&quot;linear&quot;,cost=10000000000) Récupérer les vecteurs supports et visualiser les sur le graphe. On les affectera à un data.frame dont les 2 premières colonnes représenteront les valeurs de \\(X_1\\) et \\(X_2\\) des vecteurs supports. Les vecteurs supports se trouvent dans la sortie index de la fonction svm : &gt; ind.svm &lt;- mod.svm$index &gt; sv &lt;- donnees %&gt;% slice(ind.svm) &gt; sv X1 X2 Y 1 -1.61179777 -0.6599042 -1 2 0.06962369 0.7140262 -1 3 -0.31095135 -0.5332139 1 &gt; p1 &lt;- p+geom_point(data=sv,aes(x=X2,y=X1),color=&quot;blue&quot;,size=2) On peut ainsi représenter la marge en traçant les droites qui passent par ces points. &gt; sv1 &lt;- sv[,2:1] &gt; b &lt;- (sv1[1,2]-sv1[2,2])/(sv1[1,1]-sv1[2,1]) &gt; a &lt;- sv1[1,2]-b*sv1[1,1] &gt; a1 &lt;- sv1[3,2]-b*sv1[3,1] &gt; p1+geom_abline(intercept = c(a,a1),slope=b,col=&quot;blue&quot;,size=1) Retrouver ce graphe à l’aide de la fonction plot. &gt; plot(mod.svm,data=donnees,grid=250) Rappeler la règle de décision associée la méthode SVM. Donner les estimations des paramètres de la règle de décision sur cet exemple. On pourra notamment regarder la sortie coef de la fonction svm. Une fois \\(w^\\star\\) et \\(b^\\star\\) obtenus, la règle s’écrit \\[g(x)=1_{\\langle w^\\star,x\\rangle+b^\\star\\leq 0}-1_{\\langle w^\\star,x\\rangle+b^\\star\\geq 0}.\\] L’objet mod.svm$coefs contient les coefficients \\(\\alpha_i^\\star y_i\\) pour chaque vecteur support. On peut ainsi récupérer l’équation de l’hyperplan et faire la prévision avec &gt; w &lt;- apply(mod.svm$coefs*donnees[mod.svm$index,1:2],2,sum) &gt; w X1 X2 -1.745100 2.136029 &gt; b &lt;- -mod.svm$rho &gt; b [1] -0.4035113 L’hyperplan séparateur a donc pour équation : \\[-1.74x_1+2.12x_2-0.40=0.\\] On dispose d’un nouvel individu \\(x=(-0.5,0.5)\\). Expliquer comment on peut prédire son groupe. Il suffit de calculer \\(\\langle w^\\star,x\\rangle+b\\) et de prédire en fonction du signe de cette valeur : &gt; newX &lt;- data.frame(X1=-0.5,X2=0.5) &gt; sum(w*newX)+b [1] 1.537053 On prédira le groupe -1 pour ce nouvel individu. Retrouver les résultats de la question précédente à l’aide de la fonction predict. On pourra utiliser l’option decision.values = TRUE. &gt; predict(mod.svm,newX,decision.values = TRUE) 1 -1 attr(,&quot;decision.values&quot;) -1/1 1 1.537053 Levels: -1 1 Plus cette valeur est élevée, plus on est loin de l’hyperplan. On peut donc l’interpréter comme un score. Obtenir les probabilités prédites à l’aide de la fonction predict. On pourra utiliser probability=TRUE dans la fonction svm. &gt; mod.svm1 &lt;- svm(Y~.,data=donnees,kernel=&quot;linear&quot;,cost=10000000000,probability=TRUE) &gt; predict(mod.svm1,newX,decision.values=TRUE,probability=TRUE) 1 -1 attr(,&quot;decision.values&quot;) -1/1 1 1.537053 attr(,&quot;probabilities&quot;) -1 1 1 0.8294474 0.1705526 Levels: -1 1 Comme souvent, il est possible d’obtenir une estimation des probabilités d’être dans les groupes -1 et 1 à partir du score, il “suffit” de ramener ce score sur l’échelle \\([0,1]\\) avec des transformations de type logit par exemple. Pour la svm, ces probabilités sont obtenues en ajustant un modèle logistique sur les scores \\(S(x)\\) : \\[P(Y=1|X=x)=\\frac{1}{1+\\exp(aS(x)+b)}.\\] On peut retrouver ces probabilités avec : &gt; score.newX &lt;- sum(w*newX)+b &gt; 1/(1+exp(-(mod.svm1$probB+mod.svm1$probA*score.newX))) [1] 0.1705526 2.2 Cas non séparable Dans la vraie vie, les groupes ne sont généralement pas séparables et il n’existe donc pas de solution au problème (2.1). On va donc autoriser certains points à être : mal classés et/ou bien classés mais à l’intérieur de la marge. Mathématiquement, cela revient à introduire des variables ressorts (slacks variables) \\(\\xi_1,\\dots,\\xi_n\\) positives telles que : \\(\\xi_i\\in [0,1]\\Longrightarrow\\) \\(i\\) bien classé mais dans la région définie par la marge ; \\(\\xi_i&gt;1 \\Longrightarrow\\) \\(i\\) mal classé. Le problème d’optimisation devient alors minimiser en \\((w,b,\\xi)\\) \\[\\frac{1}{2}\\|w\\|^2 +C\\sum_{i=1}^n\\xi_i\\] \\[\\textrm{sous les contraintes } \\left\\{ \\begin{array}{l} y_i(w^tx_i+b)\\geq 1 -\\xi_i \\\\ \\xi_i\\geq 0, i=1,\\dots,n. \\end{array}\\right.\\] Le paramètre \\(C&gt;0\\) est à calibrer et on remarque que le cas séparable correspond à \\(C\\to +\\infty\\). Les solutions de ce nouveau problème d’ptimisation s’obtiennent de la même façon que dans le cas séparable, en particulier \\(w^\\star\\) s’écrite toujours comme une combinaison linéaire \\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\] de vecteurs supports sauf qu’on distingue deux cas de vecteurs supports (\\(\\alpha_i^\\star&gt;0\\)): ceux sur la frontière définie par la marge : \\(\\xi_i^\\star=0\\) ; ceux en dehors : \\(\\xi_i^\\star&gt;0\\) et \\(\\alpha_i^\\star=C\\). Le choix de \\(C\\) est crucial : ce paramètre régule le compromis biais/variance de la svm : \\(C\\searrow\\): la marge est privilégiée et les \\(\\xi_i\\nearrow\\) \\(\\Longrightarrow\\) beaucoup d’observations dans la marge ou mal classées (et donc beaucoup de vecteurs supports). \\(C\\nearrow\\Longrightarrow\\) \\(\\xi_i\\searrow\\) donc moins d’observations mal classées \\(\\Longrightarrow\\) meilleur ajustement mais petite marge \\(\\Longrightarrow\\) risque de surajustement. On choisit généralement ce paramètre à l’aide des techiques présentées dans le chapitre 1 : choix d’une grille de valeurs de \\(C\\) et d’un critère ; choix d’une méthode de ré-échantillonnage pour estimer le critère ; choix de la valeur de \\(C\\) qui minimise le critère estimé. On considère le jeu de données df3 définie ci-dessous. &gt; n &lt;- 1000 &gt; set.seed(1234) &gt; df &lt;- as.data.frame(matrix(runif(2*n),ncol=2)) &gt; df1 &lt;- df %&gt;% filter(V1&lt;=V2)%&gt;% mutate(Y=rbinom(nrow(.),1,0.95)) &gt; df2 &lt;- df %&gt;% filter(V1&gt;V2)%&gt;% mutate(Y=rbinom(nrow(.),1,0.05)) &gt; df3 &lt;- bind_rows(df1,df2) %&gt;% mutate(Y=as.factor(Y)) &gt; ggplot(df3)+aes(x=V2,y=V1,color=Y)+geom_point()+ + scale_color_manual(values=c(&quot;#FFFFC8&quot;, &quot;#7D0025&quot;))+ + theme(panel.background = element_rect(fill = &quot;#BFD5E3&quot;, colour = &quot;#6D9EC1&quot;,size = 2, linetype = &quot;solid&quot;), + panel.grid.major = element_blank(), + panel.grid.minor = element_blank()) Ajuster 3 svm en considérant comme valeur de \\(C\\) : 0.000001, 0.1 et 5. On pourra utiliser l’option cost. &gt; mod.svm1 &lt;- svm(Y~.,data=df3,kernel=&quot;linear&quot;,cost=0.000001) &gt; mod.svm2 &lt;- svm(Y~.,data=df3,kernel=&quot;linear&quot;,cost=0.1) &gt; mod.svm3 &lt;- svm(Y~.,data=df3,kernel=&quot;linear&quot;,cost=5) Calculer les nombres de vecteurs supports pour chaque valeur de \\(C\\). &gt; mod.svm1$nSV [1] 469 469 &gt; mod.svm2$nSV [1] 178 178 &gt; mod.svm3$nSV [1] 150 150 Visualiser les 3 svm obtenues. Interpréter. &gt; plot(mod.svm1,data=df3,grid=250) &gt; plot(mod.svm2,data=df3,grid=250) &gt; plot(mod.svm3,data=df3,grid=250) Pour \\(C\\) petit, toutes les observations sont classées 0, la marge est grande et le nombre de vecteurs supports importants. On remarque que ces deux quantités diminuent lorsque \\(C\\) augmente. 2.3 L’astuce du noyau Les SVM présentées précédemment font l’hypothèse que les groupes sont linéairement séparables, ce qui n’est bien entendu pas toujours le cas en pratique. L’astuce du noyau permet de mettre de la non linéarité, elle consiste à : plonger les données dans un nouvel espace appelé espace de représentation ou feature space appliquer une svm linéaire dans ce nouvel espace. Le terme astuce vient du fait que ce procédé ne nécessite pas de connaître explicitement ce nouvel espace : pour résoudre le problème d’optimisation dans le faeture space on a juste besoin de connaître le noyau associé au feature space. D’un point de vu formel un noyau est une fonction \\[K:\\mathcal X\\times\\mathcal X\\to\\mathbb R\\] dont les propriétés sont proches d’un produit scalaire. Il existe donc tout un tas de noyau avec lesquels on peut faire des SVM, par exemple Linéaire (sur \\(\\mathbb R^d\\)) : \\(K(x,x&#39;)=x^tx&#39;\\). Polynomial (sur \\(\\mathbb R^d\\)) : \\(K(x,x&#39;)=(x^tx&#39;+1)^d\\). Gaussien (Gaussian radial basis function ou RBF) (sur \\(\\mathbb R^d\\)) \\[K(x,x&#39;)=\\exp\\left(-\\frac{\\|x-x&#39;\\|}{2\\sigma^2}\\right).\\] Laplace (sur \\(\\mathbb R\\)) : \\(K(x,x&#39;)=\\exp(-\\gamma|x-x&#39;|)\\). Noyau min (sur \\(\\mathbb R^+\\)) : \\(K(x,x&#39;)=\\min(x,x&#39;)\\). … Bien entendu, en pratique tout le problème va consister à trouver le bon noyau ! On considère le jeu de données suivant où le problème est d’expliquer \\(Y\\) par \\(V1\\) et \\(V2\\). &gt; n &lt;- 500 &gt; set.seed(13) &gt; X &lt;- matrix(runif(n*2,-2,2),ncol=2) %&gt;% as.data.frame() &gt; Y &lt;- rep(0,n) &gt; cond &lt;- (X$V1^2+X$V2^2)&lt;=2.8 &gt; Y[cond] &lt;- rbinom(sum(cond),1,0.9) &gt; Y[!cond] &lt;- rbinom(sum(!cond),1,0.1) &gt; df &lt;- X %&gt;% mutate(Y=as.factor(Y)) &gt; ggplot(df)+aes(x=V2,y=V1,color=Y)+geom_point()+theme_classic() Ajuster une svm linéaire et visualiser l’hyperplan séparateur. Que remarquez-vous ? &gt; mod.svm0 &lt;- svm(Y~.,data=df,kernel=&quot;linear&quot;,cost=1) &gt; plot(mod.svm0,df,grid=250) La svm linéaire ne permet pas de séparer les groupes (on pouvait s’y attendre). Exécuter la commande suivante et commenter la sortie. &gt; mod.svm1 &lt;- svm(Y~.,data=df,kernel=&quot;radial&quot;,gamma=1,cost=1) &gt; plot(mod.svm1,df,grid=250) Le noyau radial permet de mettre en évidence une séparation non linéaire. Faire varier les paramètres gamma et cost. Interpréter (on pourra notamment étudier l’évolution du nombre de vecteurs supports en fonction du paramètre cost). &gt; mod.svm2 &lt;- svm(Y~.,data=df,kernel=&quot;radial&quot;,gamma=1,cost=0.0001) &gt; mod.svm3 &lt;- svm(Y~.,data=df,kernel=&quot;radial&quot;,gamma=1,cost=1) &gt; mod.svm4 &lt;- svm(Y~.,data=df,kernel=&quot;radial&quot;,gamma=1,cost=100000) &gt; &gt; plot(mod.svm2,df,grid=250) &gt; plot(mod.svm3,df,grid=250) &gt; plot(mod.svm4,df,grid=250) &gt; &gt; mod.svm2$nSV [1] 244 244 &gt; mod.svm3$nSV [1] 114 114 &gt; mod.svm4$nSV [1] 78 77 Le nombre de vecteurs supports diminue lorsque \\(C\\) augmente. Une forte valeur de \\(C\\) autorise moins d’observations à être dans la marge, elle a donc tendance à diminuer (risque de surapprentissage). Sélectionner automatiquement ces paramètres. On pourra utiliser la fonction tune en faisant varier C dans c(0.1,1,10,100,1000) et gamma dans c(0.5,1,2,3,4). &gt; set.seed(1234) &gt; tune.out &lt;- tune(svm,Y~.,data=df,kernel=&quot;radial&quot;, + ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4))) &gt; summary(tune.out) Parameter tuning of &#39;svm&#39;: - sampling method: 10-fold cross validation - best parameters: cost gamma 10 0.5 - best performance: 0.108 - Detailed performance results: cost gamma error dispersion 1 1e-01 0.5 0.182 0.04565572 2 1e+00 0.5 0.148 0.03155243 3 1e+01 0.5 0.108 0.03425395 4 1e+02 0.5 0.116 0.03373096 5 1e+03 0.5 0.112 0.03425395 6 1e-01 1.0 0.184 0.04402020 7 1e+00 1.0 0.120 0.03651484 8 1e+01 1.0 0.120 0.03126944 9 1e+02 1.0 0.112 0.03155243 10 1e+03 1.0 0.120 0.03887301 11 1e-01 2.0 0.170 0.04136558 12 1e+00 2.0 0.124 0.02458545 13 1e+01 2.0 0.122 0.03457681 14 1e+02 2.0 0.124 0.03502380 15 1e+03 2.0 0.142 0.03705851 16 1e-01 3.0 0.160 0.03651484 17 1e+00 3.0 0.124 0.02458545 18 1e+01 3.0 0.126 0.03134042 19 1e+02 3.0 0.132 0.04022161 20 1e+03 3.0 0.166 0.03272783 21 1e-01 4.0 0.154 0.03777124 22 1e+00 4.0 0.124 0.02458545 23 1e+01 4.0 0.126 0.03134042 24 1e+02 4.0 0.138 0.04467164 25 1e+03 4.0 0.190 0.05754226 La sélection est faite en minimisant l’erreur de classification par validation croisée 10 blocs. Faire de même avec caret, on utilisera method=“svmRadial” et prob.model=TRUE. &gt; C &lt;- c(0.001,0.01,1,10,100,1000) &gt; sigma &lt;- c(0.5,1,2,3,4) &gt; gr &lt;- expand.grid(C=C,sigma=sigma) &gt; ctrl &lt;- trainControl(method=&quot;cv&quot;) &gt; res.caret1 &lt;- train(Y~.,data=df,method=&quot;svmRadial&quot;,trControl=ctrl,tuneGrid=gr,prob.model=TRUE) &gt; res.caret1 Support Vector Machines with Radial Basis Function Kernel 500 samples 2 predictor 2 classes: &#39;0&#39;, &#39;1&#39; No pre-processing Resampling: Cross-Validated (10 fold) Summary of sample sizes: 449, 450, 451, 450, 449, 450, ... Resampling results across tuning parameters: C sigma Accuracy Kappa 1e-03 0.5 0.8359976 0.6734429 1e-03 1.0 0.8439584 0.6890006 1e-03 2.0 0.8398359 0.6806352 1e-03 3.0 0.8578816 0.7164180 1e-03 4.0 0.8577623 0.7162786 1e-02 0.5 0.8400384 0.6813511 1e-02 1.0 0.8419584 0.6851382 1e-02 2.0 0.8458784 0.6926478 1e-02 3.0 0.8518407 0.7044624 1e-02 4.0 0.8577623 0.7162786 1e+00 0.5 0.8676871 0.7347434 1e+00 1.0 0.8857719 0.7713024 1e+00 2.0 0.8838127 0.7672737 1e+00 3.0 0.8798519 0.7594972 1e+00 4.0 0.8838928 0.7675507 1e+01 0.5 0.8798095 0.7596483 1e+01 1.0 0.8818111 0.7634823 1e+01 2.0 0.8838928 0.7676862 1e+01 3.0 0.8778503 0.7554248 1e+01 4.0 0.8720480 0.7438434 1e+02 0.5 0.8818111 0.7635668 1e+02 1.0 0.8839320 0.7677447 1e+02 2.0 0.8680480 0.7359331 1e+02 3.0 0.8517231 0.7031601 1e+02 4.0 0.8377591 0.6749363 1e+03 0.5 0.8760088 0.7521217 1e+03 1.0 0.8760496 0.7520939 1e+03 2.0 0.8498816 0.6998254 1e+03 3.0 0.8297967 0.6590033 1e+03 4.0 0.8100352 0.6192088 Accuracy was used to select the optimal model using the largest value. The final values used for the model were sigma = 1 and C = 1. On peut également répéter plusieurs fois la validation croisée pour stabiliser les résultats (on parallélise avec doParallel) : &gt; library(doParallel) ## pour paralléliser &gt; cl &lt;- makePSOCKcluster(4) &gt; registerDoParallel(cl) &gt; set.seed(12345) &gt; ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;,number=10,repeats=5) &gt; res.caret2 &lt;- train(Y~.,data=df,method=&quot;svmRadial&quot;,trControl=ctrl,tuneGrid=gr,prob.model=TRUE) &gt; on.exit(stopCluster(cl)) &gt; res.caret2 Support Vector Machines with Radial Basis Function Kernel 500 samples 2 predictor 2 classes: &#39;0&#39;, &#39;1&#39; No pre-processing Resampling: Cross-Validated (10 fold, repeated 5 times) Summary of sample sizes: 450, 449, 450, 451, 450, 449, ... Resampling results across tuning parameters: C sigma Accuracy Kappa 1e-03 0.5 0.8222641 0.6462662 1e-03 1.0 0.8458598 0.6928018 1e-03 2.0 0.8507000 0.7022920 1e-03 3.0 0.8555163 0.7118454 1e-03 4.0 0.8607898 0.7222353 1e-02 0.5 0.8242566 0.6502516 1e-02 1.0 0.8462519 0.6935931 1e-02 2.0 0.8499238 0.7007974 1e-02 3.0 0.8543078 0.7094622 1e-02 4.0 0.8639097 0.7284572 1e+00 0.5 0.8640626 0.7275460 1e+00 1.0 0.8839933 0.7678344 1e+00 2.0 0.8843858 0.7685479 1e+00 3.0 0.8823531 0.7644392 1e+00 4.0 0.8799766 0.7596759 1e+01 0.5 0.8848178 0.7696785 1e+01 1.0 0.8803851 0.7606211 1e+01 2.0 0.8775757 0.7549666 1e+01 3.0 0.8751989 0.7501291 1e+01 4.0 0.8727989 0.7453460 1e+02 0.5 0.8815531 0.7631204 1e+02 1.0 0.8751107 0.7501217 1e+02 2.0 0.8743443 0.7484731 1e+02 3.0 0.8615653 0.7229593 1e+02 4.0 0.8507228 0.7011391 1e+03 0.5 0.8803600 0.7607328 1e+03 1.0 0.8731277 0.7462531 1e+03 2.0 0.8499715 0.7000011 1e+03 3.0 0.8319834 0.6640949 1e+03 4.0 0.8089513 0.6174340 Accuracy was used to select the optimal model using the largest value. The final values used for the model were sigma = 0.5 and C = 10. Visualiser la règle sélectionnée. caret utilise la fonction ksvm du package kernlab. Ce package propose un choix plus large pour les noyaux. Par conséquent, si on souhaite visualiser la svm sélectionnée par caret, il est préférable d’utiliser cette fonction. &gt; library(kernlab) &gt; C.opt &lt;- res.caret2$bestTune$C &gt; sigma.opt &lt;- res.caret2$bestTune$sigma &gt; svm.sel &lt;- ksvm(Y~.,data=df,kernel=&quot;rbfdot&quot;,kpar=list(sigma=sigma.opt),C=C.opt) &gt; plot(svm.sel,data=df) 2.4 Exercices Exercice 2.1 (Résolution du problème d’optimisation dans le cas séparable) On considère \\(n\\) observations \\((x_1,y_1),\\dots,(x_n,y_n)\\) telles que \\((x_i,y_i)\\in\\mathbb R^p\\times\\{-1,1\\}\\). On cherche à expliquer la variable \\(Y\\) par \\(X\\). On considère l’algorithme SVM et on se place dans le cas où les données sont séparables. Soit \\(\\mathcal H\\) un hyperplan séparateur d’équation \\(\\langle w,x\\rangle+b=0\\) où \\(w\\in\\mathbb R^p,b\\in\\mathbb R\\). Exprimer la distance entre \\(x_i,i=1,\\dots,n\\) et \\(\\mathcal H\\) en fonction de \\(w\\) et \\(b\\). Soit \\(x_0\\in\\mathcal H\\). La solution correspond à la norme du projeté orthogonal de \\(x-x_0\\) sur \\(\\mathcal H\\), elle est donc colinéaire à \\(w\\) (car \\(w\\) est normal à \\(\\mathcal H\\)) et s’écrit \\[\\frac{\\langle x-x_0,w\\rangle}{\\|w\\|}w=\\frac{\\langle x,w\\rangle}{\\|w\\|}w-\\frac{\\langle x_0,w\\rangle}{\\|w\\|}w,\\] Comme \\(\\langle x_0,w\\rangle=-b\\), on déduit \\(d_{\\mathcal H}(x)=\\frac{|\\langle w,x\\rangle+b|}{\\|w\\|}=x^tw+b\\) si \\(\\|w\\|=1\\). Expliquer la logique du problème d’optimisation \\[\\max_{w,b,\\|w\\|=1}M\\] \\[\\textrm{sous les contraintes } y_i(\\langle w,x_i\\rangle+b)\\geq M,\\ i=1,\\dots,n.\\] L’approche consiste à choisir l’hyperplan : qui sépare les groupes ; tel que la distance entre les observations et lui soit maximale. Montrer que ce problème peut se réécrire \\[\\min_{w,b}\\frac{1}{2}\\|w\\|^2\\] \\[\\textrm{sous les contraintes } y_i(\\langle w,x_i\\rangle+b)\\geq 1,\\ i=1,\\dots,n.\\] Il suffit de poser comme contrainte \\(M=1/\\|w\\|\\). On rappelle que pour la minimisation d’une fonction \\(h:\\mathbb R^p\\to\\mathbb R\\) sous contraintes affines \\(g_i(u)\\geq 0,i=1,\\dots,n\\), le Lagrangien s’écrit \\[L(u,\\alpha)=h(u)-\\sum_{i=1}^n\\alpha_ig_i(u).\\] Si on désigne par \\(u_\\alpha=\\mathop{\\mathrm{argmin}}_uL(u,\\alpha)\\), la fonction duale est alors donnée par \\[\\theta(\\alpha)=L(u_\\alpha,\\alpha)=\\min_{u\\in\\mathbb R^p}L(u,\\alpha),\\] et le problème dual consiste à maximiser \\(\\theta(\\alpha)\\) sous les contraintes \\(\\alpha_i\\geq 0\\). En désignant par \\(\\alpha^\\star\\) la solution de ce problème, on déduit la solution du problème primal \\(u^\\star=u_{\\alpha^\\star}\\). Les conditions de Karush-Kuhn-Tucker sont données par \\(\\alpha_i^\\star\\geq 0\\). \\(g_i(u_{\\alpha^\\star})\\geq 0\\). \\(\\alpha_i^\\star g_i(u_{\\alpha^\\star})=0\\). Écrire le Lagrangien du problème considéré et en déduire une expression de \\(w\\) en fonction des \\(\\alpha_i\\) et des observations. Le lagrangien s’écrit \\[L(w,b;\\alpha)=\\frac{1}{2}\\|w\\|^2-\\sum_{i=1}^n\\alpha_i[y_i(x_i^tw+b)-1].\\] On a alors \\[\\frac{\\partial L(w,b;\\alpha)}{\\partial w}=w-\\sum_{i=1}^n\\alpha_iy_ix_i=0\\] et \\[\\frac{\\partial L(w,b;\\alpha)}{\\partial b}=-\\sum_{i=1}^n\\alpha_iy_i=0.\\] D’où \\(w_\\alpha=\\sum_{i=1}^n\\alpha_iy_ix_i\\). Écrire la fonction duale. La fonction duale s’écrit \\[\\begin{align*} \\theta(\\alpha)=L(w_\\alpha,b_\\alpha;\\alpha)= &amp; \\ \\frac{1}{2}\\langle \\sum_i\\alpha_iy_ix_i,\\sum_j\\alpha_jy_jx_j\\rangle-\\sum_i\\alpha_iy_i\\langle \\sum_j\\alpha_jy_jx_j,x_i\\rangle-\\sum_i\\alpha_iy_ib+\\sum_i\\alpha_i \\\\ = &amp;\\sum_{i=1}^n\\alpha_i-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n\\alpha_i\\alpha_jy_iy_jx_i^tx_j \\end{align*}\\] Écrire les conditions KKT et en déduire les solutions \\(w^\\star\\) et \\(b^\\star\\). Les conditions KKT sont \\(\\alpha_i^\\star\\geq 0,i=1\\dots,n\\) ; \\(\\alpha_i^\\star[y_i(x_i^tw+b)-1]=0, i=1,\\dots,n\\). On obtient ainsi \\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\] et \\(b^\\star\\) en résolvant \\[\\alpha_i^\\star[y_i(x_i^tw+b)-1]=0\\] pour un \\(\\alpha_i^\\star\\) non nul. Interpréter les conditions KKT. Les \\(x_i\\) tels que \\(\\alpha_i^\\star&gt;0\\) vérifient \\[y_i(x_i^tw^\\star+b^\\star)=1.\\] Ils se situent donc sur la frontière définissant la marge maximale. Ce sont les vecteurs supports. Exercice 2.2 (Règle svm à partir de sorties R) On considère \\(n\\) observations \\((x_1,y_1),\\dots,(x_n,y_n)\\) telles que \\((x_i,y_i)\\in\\mathbb R^3\\times\\{-1,1\\}\\). On cherche à expliquer la variable \\(Y\\) par \\(X=(X_1,X_2,X_3)\\). On considère l’algorithme SVM et on se place dans le cas où les données sont séparables. On rappelle que cet algorithme consiste à chercher une droite d’équation \\(w^tx+b=0\\) où \\((w,b)\\in\\mathbb R^3\\times\\mathbb R\\) sont solutions du problème d’optimisation (problème primal) \\[\\min_{w,b}\\frac{1}{2}\\|w\\|^2\\] \\[\\textrm{sous les contraintes } y_i(w^tx_i+b)\\geq 1,\\ i=1,\\dots,n.\\] On désigne par \\(\\alpha_i^\\star,i=1,\\dots,n\\), les solutions du problème dual et par \\((w^\\star,b^\\star)\\) les solutions du problème ci-dessus. Donner la formule permettant de calculer \\(w^\\star\\) en fonction des \\(\\alpha_i^\\star\\). \\(w^\\star\\) se calcule selon \\[w^\\star=\\sum_{i=1}^n\\alpha_i^\\star y_ix_i.\\] Les \\(\\alpha_i^\\star\\) étant nuls pour les vecteurs non supports, il suffit de sommer sur les vecteur supports. Expliquer comment on classe un nouveau point \\(x\\in\\mathbb R^3\\) par la méthode svm. Une fois \\(w^\\star\\) et \\(b^\\star\\) obtenus, la règle s’écrit \\[g(x)=1_{\\langle w^\\star,x\\rangle+b^\\star\\leq 0}-1_{\\langle w^\\star,x\\rangle+b^\\star\\geq 0}.\\] Les données se trouvent dans un dataframe df. On exécute &gt; mod.svm &lt;- svm(Y~.,data=df,kernel=&quot;linear&quot;,cost=10000000000) et on obtient &gt; df[mod.svm$index,] X1 X2 X3 Y 51 -1.1 -1.0 -1.0 1 92 0.7 0.8 1.1 1 31 0.7 0.5 -1.0 -1 37 -0.5 -0.6 0.3 -1 &gt; mod.svm$coefs [,1] [1,] 59 [2,] 49 [3,] -30 [4,] -79 &gt; mod.svm$rho [1] -0.5 Calculer les valeurs de \\(w^\\star\\) et \\(b^\\star\\). En déduire la règle de classification. \\(b^\\star\\) est l’opposé de mod.svm$rho. Pour \\(w^\\star\\) il suffit d’appliquer la formule et on trouve X1 X2 X3 -12.1 12.6 1.2 On dispose d’une nouvelle observation \\(x=(1,-0.5,-1)\\). Dans quel groupe (-1 ou 1) l’algorithme affecte cette nouvelle donnée ? On calcule la combinaison linéaire \\(\\langle w^\\star,x\\rangle+b^\\star\\) : [1] -19.1 On affectera donc la nouvelle donnée au groupe -1. "],
["références.html", "Références", " Références "]
]
