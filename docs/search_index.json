[
["index.html", "Machine learning Présentation", " Machine learning Laurent Rouvière 2020-06-25 Présentation Ce tutoriel présente une introduction au machine learning avec R. Les thèmes suivants sont abordés : Estimation du risque, présentation du package caret SVM Arbres (essentiellement avec dplyr) Agrégation (représentations standards et avec ggplot2) On pourra trouver des supports de cours ainsi que les données utilisées à l’adresse suivante https://lrouviere.github.io/ml_lecture/. Des compléments sur les outils du tidyverse pourront être consultés dans le très complet document de (???) ainsi que les ouvrages de (???) et de (???). "],
["caret.html", "Chapitre 1 Estimation du risque avec caret", " Chapitre 1 Estimation du risque avec caret On cherche à expliquer une variable binaire \\(Y\\) par deux variables quantitatives \\(X_1\\) et \\(X_2\\) à l’aide du jeu de données suivant &gt; n &lt;- 2000 &gt; set.seed(12345) &gt; X1 &lt;- runif(n) &gt; set.seed(5678) &gt; X2 &lt;- runif(n) &gt; set.seed(9012) &gt; R1 &lt;- X1&lt;=0.25 &gt; R2 &lt;- (X1&gt;0.25 &amp; X2&gt;=0.75) &gt; R3 &lt;- (X1&gt;0.25 &amp; X2&lt;0.75) &gt; Y &lt;- rep(0,n) &gt; Y[R1] &lt;- rbinom(sum(R1),1,0.25) &gt; Y[R2] &lt;- rbinom(sum(R2),1,0.25) &gt; Y[R3] &lt;- rbinom(sum(R3),1,0.75) &gt; donnees &lt;- data.frame(X1,X2,Y) &gt; donnees$Y &lt;- as.factor(donnees$Y) Séparer le jeu de données en un échantillon d’apprentissage de taille 1500 et un échantillon test de taille 500. On considère la règle de classification des \\(k\\) plus proches voisins. Pour un entier \\(k\\) plus petit que \\(n\\) et un nouvel individu \\(x\\), cette règle affecte à \\(x\\) le label majoritaire des \\(k\\) plus proches voisins de \\(x\\). Sur R on utilise la fonction knn du package class. On peut par exemple obtenir les prévisions des individus de l’échantillon test de la règle des 3 plus proches voisins avec &gt; library(class) &gt; knn3 &lt;- knn(dapp[,1:2],dtest[,1:2],cl=dapp$Y,k=3) Calculer l’erreur de classification de la règle des 3 plus proches voisins sur les données test. Expliquer la fonction knn.cv Calculer l’erreur de classification de la règle des 3 plus proches voisins par validation croisée leave-one-out. On considère le vecteur de plus proches voisins suivant : &gt; K_cand &lt;- seq(1,500,by=20) Sélectionner une valeur de \\(k\\) dans ce vecteur à l’aide d’une validation hold out et d’une leave-one-out : On calcule l’erreur de classification par validation hold out pour chaque valeur de \\(k\\) : &gt; err.ho &lt;- rep(0,length(K_cand)) &gt; for (i in 1:length(K_cand)){ + ... + ... + } Puis on choisit la valeur de \\(k\\) pour laquelle l’erreur est minimale. On fait la même chose avec la validation croisée leave-one-out : &gt; err.cv &lt;- rep(0,length(K_cand)) &gt; for (i in 1:length(K_cand)){ + ... + ... + } "],
["references.html", "References", " References "]
]
