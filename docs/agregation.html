<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapitre 5 Agrégation : forêts aléatoires et gradient boosting | Machine learning</title>
  <meta name="description" content="Chapitre 5 Agrégation : forêts aléatoires et gradient boosting | Machine learning" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapitre 5 Agrégation : forêts aléatoires et gradient boosting | Machine learning" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapitre 5 Agrégation : forêts aléatoires et gradient boosting | Machine learning" />
  
  
  

<meta name="author" content="Laurent Rouvière" />


<meta name="date" content="2021-02-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="SVM.html"/>
<link rel="next" href="deep.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning<br> <br> L. Rouvière</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Présentation</a></li>
<li class="part"><span><b>I Algorithmes de référence</b></span></li>
<li class="chapter" data-level="1" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>1</b> Estimation du risque avec caret</a><ul>
<li class="chapter" data-level="1.1" data-path="caret.html"><a href="caret.html#notion-de-risque-en-apprentissage-supervisé"><i class="fa fa-check"></i><b>1.1</b> Notion de risque en apprentissage supervisé</a></li>
<li class="chapter" data-level="1.2" data-path="caret.html"><a href="caret.html#la-validation-croisée"><i class="fa fa-check"></i><b>1.2</b> La validation croisée</a></li>
<li class="chapter" data-level="1.3" data-path="caret.html"><a href="caret.html#le-package-caret"><i class="fa fa-check"></i><b>1.3</b> Le package caret</a></li>
<li class="chapter" data-level="1.4" data-path="caret.html"><a href="caret.html#la-courbe-roc"><i class="fa fa-check"></i><b>1.4</b> La courbe ROC</a></li>
<li class="chapter" data-level="1.5" data-path="caret.html"><a href="caret.html#compléments"><i class="fa fa-check"></i><b>1.5</b> Compléments</a><ul>
<li class="chapter" data-level="1.5.1" data-path="caret.html"><a href="caret.html#calcul-parallèle"><i class="fa fa-check"></i><b>1.5.1</b> Calcul parallèle</a></li>
<li class="chapter" data-level="1.5.2" data-path="caret.html"><a href="caret.html#répéter-les-méthodes-de-rééchantillonnage"><i class="fa fa-check"></i><b>1.5.2</b> Répéter les méthodes de rééchantillonnage</a></li>
<li class="chapter" data-level="1.5.3" data-path="caret.html"><a href="caret.html#modifier-le-risque"><i class="fa fa-check"></i><b>1.5.3</b> Modifier le risque</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="lda.html"><a href="lda.html"><i class="fa fa-check"></i><b>2</b> Analyse discriminante linéaire</a><ul>
<li class="chapter" data-level="2.1" data-path="lda.html"><a href="lda.html#prise-en-main-lda-et-qda-sur-les-iris-de-fisher"><i class="fa fa-check"></i><b>2.1</b> Prise en main : LDA et QDA sur les iris de Fisher</a></li>
<li class="chapter" data-level="2.2" data-path="lda.html"><a href="lda.html#un-cas-avec-beaucoup-de-classes"><i class="fa fa-check"></i><b>2.2</b> Un cas avec beaucoup de classes</a></li>
<li class="chapter" data-level="2.3" data-path="lda.html"><a href="lda.html#grande-dimension-reconnaissance-de-phonèmes"><i class="fa fa-check"></i><b>2.3</b> Grande dimension : reconnaissance de phonèmes</a></li>
<li class="chapter" data-level="2.4" data-path="lda.html"><a href="lda.html#exercices"><i class="fa fa-check"></i><b>2.4</b> Exercices</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="arbres.html"><a href="arbres.html"><i class="fa fa-check"></i><b>3</b> Arbres</a><ul>
<li class="chapter" data-level="3.1" data-path="arbres.html"><a href="arbres.html#coupures-cart-en-fonction-de-la-nature-des-variables"><i class="fa fa-check"></i><b>3.1</b> Coupures CART en fonction de la nature des variables</a><ul>
<li class="chapter" data-level="3.1.1" data-path="arbres.html"><a href="arbres.html#arbres-de-régression"><i class="fa fa-check"></i><b>3.1.1</b> Arbres de régression</a></li>
<li class="chapter" data-level="3.1.2" data-path="arbres.html"><a href="arbres.html#arbres-de-classification"><i class="fa fa-check"></i><b>3.1.2</b> Arbres de classification</a></li>
<li class="chapter" data-level="3.1.3" data-path="arbres.html"><a href="arbres.html#entrée-qualitative"><i class="fa fa-check"></i><b>3.1.3</b> Entrée qualitative</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="arbres.html"><a href="arbres.html#élagage"><i class="fa fa-check"></i><b>3.2</b> Élagage</a><ul>
<li class="chapter" data-level="3.2.1" data-path="arbres.html"><a href="arbres.html#élagage-pour-un-problème-de-régression"><i class="fa fa-check"></i><b>3.2.1</b> Élagage pour un problème de régression</a></li>
<li class="chapter" data-level="3.2.2" data-path="arbres.html"><a href="arbres.html#élagage-en-classification-binaire-et-matrice-de-coût"><i class="fa fa-check"></i><b>3.2.2</b> Élagage en classification binaire et matrice de coût</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Algorithmes avancés</b></span></li>
<li class="chapter" data-level="4" data-path="SVM.html"><a href="SVM.html"><i class="fa fa-check"></i><b>4</b> Support Vector Machine (SVM)</a><ul>
<li class="chapter" data-level="4.1" data-path="SVM.html"><a href="SVM.html#cas-séparable"><i class="fa fa-check"></i><b>4.1</b> Cas séparable</a></li>
<li class="chapter" data-level="4.2" data-path="SVM.html"><a href="SVM.html#cas-non-séparable"><i class="fa fa-check"></i><b>4.2</b> Cas non séparable</a></li>
<li class="chapter" data-level="4.3" data-path="SVM.html"><a href="SVM.html#lastuce-du-noyau"><i class="fa fa-check"></i><b>4.3</b> L’astuce du noyau</a></li>
<li class="chapter" data-level="4.4" data-path="SVM.html"><a href="SVM.html#support-vector-régression"><i class="fa fa-check"></i><b>4.4</b> Support vector régression</a></li>
<li class="chapter" data-level="4.5" data-path="SVM.html"><a href="SVM.html#svm-sur-les-données-spam"><i class="fa fa-check"></i><b>4.5</b> SVM sur les données spam</a></li>
<li class="chapter" data-level="4.6" data-path="SVM.html"><a href="SVM.html#exercices-1"><i class="fa fa-check"></i><b>4.6</b> Exercices</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="agregation.html"><a href="agregation.html"><i class="fa fa-check"></i><b>5</b> Agrégation : forêts aléatoires et gradient boosting</a><ul>
<li class="chapter" data-level="5.1" data-path="agregation.html"><a href="agregation.html#forets"><i class="fa fa-check"></i><b>5.1</b> Forêts aléatoires</a></li>
<li class="chapter" data-level="5.2" data-path="agregation.html"><a href="agregation.html#boosting"><i class="fa fa-check"></i><b>5.2</b> Gradient boosting</a><ul>
<li class="chapter" data-level="5.2.1" data-path="agregation.html"><a href="agregation.html#un-exemple-simple-en-régression"><i class="fa fa-check"></i><b>5.2.1</b> Un exemple simple en régression</a></li>
<li class="chapter" data-level="5.2.2" data-path="agregation.html"><a href="agregation.html#adaboost-et-logitboost-pour-la-classification-binaire."><i class="fa fa-check"></i><b>5.2.2</b> Adaboost et logitboost pour la classification binaire.</a></li>
<li class="chapter" data-level="5.2.3" data-path="agregation.html"><a href="agregation.html#exo:grad-boost"><i class="fa fa-check"></i><b>5.2.3</b> Exercices</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="deep.html"><a href="deep.html"><i class="fa fa-check"></i><b>6</b> Réseaux de neurones avec Keras</a></li>
<li class="chapter" data-level="7" data-path="dondes.html"><a href="dondes.html"><i class="fa fa-check"></i><b>7</b> Données déséquilibrées</a><ul>
<li class="chapter" data-level="7.1" data-path="dondes.html"><a href="dondes.html#critères-de-performance-pour-données-déséquilibrées"><i class="fa fa-check"></i><b>7.1</b> Critères de performance pour données déséquilibrées</a></li>
<li class="chapter" data-level="7.2" data-path="dondes.html"><a href="dondes.html#ré-équilibrage"><i class="fa fa-check"></i><b>7.2</b> Ré-équilibrage</a></li>
<li class="chapter" data-level="7.3" data-path="dondes.html"><a href="dondes.html#exercices-supplémentaires"><i class="fa fa-check"></i><b>7.3</b> Exercices supplémentaires</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="comp-algo.html"><a href="comp-algo.html"><i class="fa fa-check"></i><b>8</b> Comparaison d’algorithmes</a></li>
<li class="chapter" data-level="" data-path="références.html"><a href="références.html"><i class="fa fa-check"></i>Références</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="agregation" class="section level1">
<h1><span class="header-section-number">Chapitre 5</span> Agrégation : forêts aléatoires et gradient boosting</h1>
<p>Les méthodes par arbres présentées précédemment sont des algorithmes qui possèdent tout un tas de qualités (facile à mettre en œuvre, interprétable…). Ce sont néanmoins rarement les algorithmes qui se révèlent les plus performants. Les méthodes d’agrégation d’arbres présentées dans cette partie sont souvent beaucoup plus pertinentes, notamment en terme de qualité de prédiction. Elles consistent à construire un très grand nombre d’arbres “simples” : <span class="math inline">\(g_1,\dots,g_B\)</span> et à les agréger en faisant la moyenne :
<span class="math display">\[\frac{1}{B}\sum_{k=1}^Bg_k(x).\]</span>
Les forêts aléatoires <span class="citation">(Breiman <a href="#ref-bre01" role="doc-biblioref">2001</a>)</span> et le gradient boosting <span class="citation">(Friedman <a href="#ref-fri01" role="doc-biblioref">2001</a>)</span> utilisent ce procédé d’agrégation.</p>
<div id="forets" class="section level2">
<h2><span class="header-section-number">5.1</span> Forêts aléatoires</h2>
<p>L’algorithme des forêts aléatoires consiste à construire des arbres sur des échantillons bootstrap et à les agréger. Il peut s’écrire de la façon suivante :</p>
<div class="correction">
<p>
<strong>Entrées</strong> :
</p>
<ul>
<li>
<span class="math inline"><span class="math inline">\(x\in\mathbb R^d\)</span></span> l’observation à prévoir, <span class="math inline"><span class="math inline">\(\mathcal D_n\)</span></span> l’échantillon ;
</li>
<li>
<span class="math inline"><span class="math inline">\(B\)</span></span> nombre d’arbres ; <span class="math inline"><span class="math inline">\(n_{max}\)</span></span> nombre max d’observations par nœud
</li>
<li>
<span class="math inline"><span class="math inline">\(m\in\{1,\dots,d\}\)</span></span> le nombre de variables candidates pour découper un nœud.
</li>
</ul>
<p>
<strong>Algorithme</strong> : pour <span class="math inline"><span class="math inline">\(k=1,\dots,B\)</span></span> :
</p>
<ol style="list-style-type: decimal">
<li>
Tirer un échantillon <em>bootstrap</em> dans <span class="math inline"><span class="math inline">\(\mathcal D_n\)</span></span>
</li>
<li>
Construire un <em>arbre CART sur cet échantillon bootstrap</em>, chaque coupure est sélectionnée en minimisant la fonction de coût de CART sur un ensemble de <em><span class="math inline"><span class="math inline">\(m\)</span></span> variables choisies au hasard</em> parmi les <span class="math inline"><span class="math inline">\(d\)</span></span>. On note <span class="math inline"><span class="math inline">\(T(.,\theta_k,\mathcal D_n)\)</span></span> l’arbre construit.
</li>
</ol>
<p>
<strong>Sortie</strong> : l’estimateur <span class="math inline"><span class="math inline">\(T_B(x)=\frac{1}{B}\sum_{k=1}^BT(x,\theta_k,\mathcal D_n)\)</span></span>.
</p>
</div>
<p>Cet algorithme peut être utilisé sur <strong>R</strong> avec la fonction <strong>randomForest</strong> du package <strong>randomForest</strong>. Nous la présentons à travers l’exemple du jeu de données <strong>spam</strong> du package <strong>kernlab</strong>.</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="agregation.html#cb52-1"></a><span class="kw">library</span>(kernlab)</span>
<span id="cb52-2"><a href="agregation.html#cb52-2"></a><span class="kw">data</span>(spam)</span>
<span id="cb52-3"><a href="agregation.html#cb52-3"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb52-4"><a href="agregation.html#cb52-4"></a>spam &lt;-<span class="st"> </span>spam[<span class="kw">sample</span>(<span class="kw">nrow</span>(spam)),]</span></code></pre></div>
<p>Le problème est d’expliquer la variable binaire <strong>type</strong> par les autres.</p>
<ol style="list-style-type: decimal">
<li><p>A l’aide de la fonction <strong>randomForest</strong> du package <strong>randomForest</strong>, ajuster une forêt aléatoire pour répondre au problème posé.</p></li>
<li><p>Appliquer la fonction <strong>plot</strong> à l’objet construit avec <strong>randomForest</strong> et expliquer le graphe obtenu. A quoi peut servir ce graphe en pratique ?</p></li>
<li><p>Construire la forêt avec <strong>mtry=1</strong> et comparer ses performances avec celle construite précédemment.</p></li>
<li><p>Utiliser la fonction <strong>train</strong> du package <strong>caret</strong> pour choisir le paramètre <strong>mtry</strong> dans la grille <strong>seq(1,30,by=5)</strong>.</p></li>
<li><p>Construire la forêt avec le paramètre <strong>mtry</strong> sélectionné. Calculer l’importance des variables et représenter ces importance à l’aide d’un diagramme en barres.</p></li>
<li><p>La fonction <strong>ranger</strong> du package <strong>ranger</strong> permet également de calculer des forêts aléatoires. Comparer les temps de calcul de cette fonction avec <strong>randomForest</strong></p></li>
</ol>
</div>
<div id="boosting" class="section level2">
<h2><span class="header-section-number">5.2</span> Gradient boosting</h2>
<p>Les algorithmes de gradient boosting permettent de minimiser des pertes empiriques de la forme
<span class="math display">\[\frac{1}{n}\sum_{i=1}^n\ell(y_i,f(x_i)).\]</span>
où <span class="math inline">\(\ell:\mathbb R\times\mathbb R\to\mathbb R\)</span> est une fonction de coût convexe en son second argument. Il existe plusieurs type d’algorithmes boosting. Un des plus connus et utilisés a été proposé par <span class="citation">Friedman (<a href="#ref-fri01" role="doc-biblioref">2001</a>)</span>, c’est la version que nous étudions dans cette partie.</p>
<p>Cette approche propose de chercher la meilleure combinaison linéaire d’arbres binaires, c’est-à-dire que l’on recherche <span class="math inline">\(g(x)=\sum_{m=1}^M\alpha_mh_m(x)\)</span> qui minimise
<span class="math display">\[\mathcal R_n(g)=\frac{1}{n}\sum_{i=1}^n\ell(y_i,g(x_i)).\]</span>
Optimiser sur toutes les combinaisons d’arbres binaires se révélant souvent trop compliqué, <span class="citation">Friedman (<a href="#ref-fri01" role="doc-biblioref">2001</a>)</span> utilise une descente de gradient pour construire la combinaison d’abres de façon récursive. L’algorithme est le suivant :</p>

<div class="correction">
<p><strong>Entrées</strong> :</p>
<ul>
<li><span class="math inline">\(d_n=(x_1,y_1),\dots,(x_n,y_n)\)</span> l’échantillon, <span class="math inline">\(\lambda\)</span> un paramètre de régularisation tel que <span class="math inline">\(0&lt;\lambda\leq 1\)</span>.</li>
<li><span class="math inline">\(M\in\mathbb N\)</span> le nombre d’itérations.</li>
<li>paramètres de l’arbre (nombre de coupures…)</li>
</ul>
<p><strong>Itérations</strong> :</p>
<ol style="list-style-type: decimal">
<li>Initialisation : <span class="math inline">\(g_0(.)=\mathop{\mathrm{argmin}}_c \frac{1}{n}\sum_{i=1}^n \ell(y_i,c)\)</span></li>
<li>Pour <span class="math inline">\(m=1\)</span> à <span class="math inline">\(M\)</span> :
<ol style="list-style-type: lower-alpha">
<li>Calculer l’opposé du gradient <span class="math inline">\(-\frac{\partial}{\partial g(x_i)}\ell(y_i,g(x_i))\)</span> et l’évaluer aux points <span class="math inline">\(g_{m-1}(x_i)\)</span> :
<span class="math display">\[U_i=-\frac{\partial}{\partial g(x_i)}\ell(y_i,g(x_i)) _{\Big |g(x_i)=g_{m-1}(x_i)},\quad i=1,\dots,n.\]</span></li>
<li>Ajuster un arbre sur l’échantillon <span class="math inline">\((x_1,U_1),\dots,(x_n,U_n)\)</span>, on le note <span class="math inline">\(h_m\)</span>.</li>
<li>Mise à jour : <span class="math inline">\(g_m(x)=g_{m-1}(x)+\lambda h_m(x)\)</span>.</li>
</ol></li>
</ol>
<strong>Sortie</strong> : la suite <span class="math inline">\((g_m(x))_m\)</span>.
</div>

<p>Sur <strong>R</strong> On peut utiliser différents packages pour faire du gradient boosting. Nous utilisons ici le package <strong>gbm</strong> <span class="citation">(Ridgeway <a href="#ref-rid06" role="doc-biblioref">2006</a>)</span>.</p>
<div id="un-exemple-simple-en-régression" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Un exemple simple en régression</h3>
<p>On considère un jeu de données <span class="math inline">\((x_i,y_i),i=1,\dots,200\)</span> issu d’un modèle de régression
<span class="math display">\[y_i=m(x_i)+\varepsilon_i\]</span>
où la vraie fonction de régression est la fonction <strong>sinus</strong> (mais on va faire comme si on ne le savait pas).</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="agregation.html#cb53-1"></a>x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">2</span><span class="op">*</span>pi,<span class="dv">2</span><span class="op">*</span>pi,<span class="dt">by=</span><span class="fl">0.01</span>)</span>
<span id="cb53-2"><a href="agregation.html#cb53-2"></a>y &lt;-<span class="st"> </span><span class="kw">sin</span>(x)</span>
<span id="cb53-3"><a href="agregation.html#cb53-3"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb53-4"><a href="agregation.html#cb53-4"></a>X &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">200</span>,<span class="op">-</span><span class="dv">2</span><span class="op">*</span>pi,<span class="dv">2</span><span class="op">*</span>pi)</span>
<span id="cb53-5"><a href="agregation.html#cb53-5"></a>Y &lt;-<span class="st"> </span><span class="kw">sin</span>(X)<span class="op">+</span><span class="kw">rnorm</span>(<span class="dv">200</span>,<span class="dt">sd=</span><span class="fl">0.2</span>)</span>
<span id="cb53-6"><a href="agregation.html#cb53-6"></a>df1 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(X,Y)</span>
<span id="cb53-7"><a href="agregation.html#cb53-7"></a>df2 &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">X=</span>x,<span class="dt">Y=</span>y)</span>
<span id="cb53-8"><a href="agregation.html#cb53-8"></a>p1 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(df1)<span class="op">+</span><span class="kw">aes</span>(<span class="dt">x=</span>X,<span class="dt">y=</span>Y)<span class="op">+</span><span class="kw">geom_point</span>()<span class="op">+</span><span class="kw">geom_line</span>(<span class="dt">data=</span>df2,<span class="dt">size=</span><span class="dv">1</span>)<span class="op">+</span><span class="kw">xlab</span>(<span class="st">&quot;&quot;</span>)<span class="op">+</span><span class="kw">ylab</span>(<span class="st">&quot;&quot;</span>)</span>
<span id="cb53-9"><a href="agregation.html#cb53-9"></a>p1</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-303-1.png" width="672" style="display: block; margin: auto;" /></p>
<ol style="list-style-type: decimal">
<li><p>Rappeler ce que siginifie le <span class="math inline">\(L_2\)</span>-boosting.</p></li>
<li><p>A l’aide de la fonction <strong>gbm</strong> du package <strong>gbm</strong> construire un algorithme de <span class="math inline">\(L_2\)</span>-boosting. On utilisera 500000 itérations et gardera les autres valeurs par défaut de paramètres.</p></li>
<li><p>Visualiser l’estimateur à la première itération. On pourra faire un <strong>predict</strong> avec l’option <code>n.trees</code>.</p></li>
<li><p>Faire de même pour les itérations 1000 et 500000.</p></li>
<li><p>Sélectionner le nombre d’itérations par la procédure de votre choix.</p></li>
</ol>
</div>
<div id="adaboost-et-logitboost-pour-la-classification-binaire." class="section level3">
<h3><span class="header-section-number">5.2.2</span> Adaboost et logitboost pour la classification binaire.</h3>
<p>On considère le jeu de données <strong>spam</strong> du package <strong>kernlab</strong>.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="agregation.html#cb54-1"></a><span class="kw">library</span>(kernlab)</span>
<span id="cb54-2"><a href="agregation.html#cb54-2"></a><span class="kw">data</span>(spam)</span>
<span id="cb54-3"><a href="agregation.html#cb54-3"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb54-4"><a href="agregation.html#cb54-4"></a>spam &lt;-<span class="st"> </span>spam[<span class="kw">sample</span>(<span class="kw">nrow</span>(spam)),]</span></code></pre></div>
<ol style="list-style-type: decimal">
<li><p>Exécuter la commande</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="agregation.html#cb55-1"></a>model_ada1 &lt;-<span class="st"> </span><span class="kw">gbm</span>(type<span class="op">~</span>.,<span class="dt">data=</span>spam,<span class="dt">distribution=</span><span class="st">&quot;adaboost&quot;</span>,<span class="dt">interaction.depth=</span><span class="dv">2</span>,</span>
<span id="cb55-2"><a href="agregation.html#cb55-2"></a>              <span class="dt">shrinkage=</span><span class="fl">0.05</span>,<span class="dt">n.trees=</span><span class="dv">500</span>)</span></code></pre></div></li>
<li><p>Proposer une correction permettant de faire fonctionner l’algorithme.</p></li>
<li><p>Expliciter le modèle ajusté par la commande précédente.</p></li>
<li><p>Effectuer un <strong>summary</strong> du modèle ajusté. Expliquer la sortie.</p></li>
<li><p>Utiliser la fonction <strong>vip</strong> du package <strong>vip</strong> pour retrouver ce sorties.</p></li>
<li><p>Sélectionner le nombre d’itérations pour l’algorithme adaboost en faisant de la validation croisée 5 blocs.</p></li>
<li><p>Faire la même procédure en changeant la valeur du paramètre <strong>shrinkage</strong>. Interpréter.</p></li>
<li><p>Expliquer la différence entre <strong>adaboost</strong> et <strong>logitboost</strong> et précisez comment on peut mettre en œuvre ce dernier algorithme.</p></li>
</ol>
</div>
<div id="exo:grad-boost" class="section level3">
<h3><span class="header-section-number">5.2.3</span> Exercices</h3>
<ol style="list-style-type: decimal">
<li><p>Rappeler la fonction de risque adaboost.</p></li>
<li><p>Montrer que le risque est minimum en
<span class="math display">\[f^\star(x)=\frac{1}{2}\log\frac{\mathbf P(Y=1|X=x)}{\mathbf P(Y=-1|X=x)}.\]</span></p></li>
<li><p>Mêmes questions pour le risque logitboost.</p></li>
</ol>

</div>
</div>
</div>
<h3>Références</h3>
<div id="refs" class="references">
<div id="ref-bre01">
<p>Breiman, L. 2001. “Random Forests.” <em>Machine Learning</em> 45: 5–32.</p>
</div>
<div id="ref-fri01">
<p>Friedman, J. H. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” <em>Annals of Statistics</em> 29: 1189–1232.</p>
</div>
<div id="ref-rid06">
<p>Ridgeway, G. 2006. “Generalized Boosted Models: A Guide to the Gbm Package.”</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="SVM.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="deep.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["TUTO_ML.pdf"],
"toc": {
"collapse": "subsection",
"sharing": {
"facebook": true,
"github": true,
"twitter": true
}
},
"highlight": "tango"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
