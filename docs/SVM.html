<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapitre 2 Support Vector Machine (SVM) | Machine learning</title>
  <meta name="description" content="Chapitre 2 Support Vector Machine (SVM) | Machine learning" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapitre 2 Support Vector Machine (SVM) | Machine learning" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapitre 2 Support Vector Machine (SVM) | Machine learning" />
  
  
  

<meta name="author" content="Laurent Rouvière" />


<meta name="date" content="2020-10-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="caret.html"/>
<link rel="next" href="arbres.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning<br> <br> L. Rouvière</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Présentation</a></li>
<li class="chapter" data-level="1" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>1</b> Estimation du risque avec caret</a><ul>
<li class="chapter" data-level="1.1" data-path="caret.html"><a href="caret.html#notion-de-risque-en-apprentissage-supervisé"><i class="fa fa-check"></i><b>1.1</b> Notion de risque en apprentissage supervisé</a></li>
<li class="chapter" data-level="1.2" data-path="caret.html"><a href="caret.html#la-validation-croisée"><i class="fa fa-check"></i><b>1.2</b> La validation croisée</a></li>
<li class="chapter" data-level="1.3" data-path="caret.html"><a href="caret.html#le-package-caret"><i class="fa fa-check"></i><b>1.3</b> Le package caret</a></li>
<li class="chapter" data-level="1.4" data-path="caret.html"><a href="caret.html#compléments"><i class="fa fa-check"></i><b>1.4</b> Compléments</a><ul>
<li class="chapter" data-level="1.4.1" data-path="caret.html"><a href="caret.html#calcul-parallèle"><i class="fa fa-check"></i><b>1.4.1</b> Calcul parallèle</a></li>
<li class="chapter" data-level="1.4.2" data-path="caret.html"><a href="caret.html#répéter-les-méthodes-de-rééchantillonnage"><i class="fa fa-check"></i><b>1.4.2</b> Répéter les méthodes de rééchantillonnage</a></li>
<li class="chapter" data-level="1.4.3" data-path="caret.html"><a href="caret.html#modifier-le-risque"><i class="fa fa-check"></i><b>1.4.3</b> Modifier le risque</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="SVM.html"><a href="SVM.html"><i class="fa fa-check"></i><b>2</b> Support Vector Machine (SVM)</a><ul>
<li class="chapter" data-level="2.1" data-path="SVM.html"><a href="SVM.html#cas-séparable"><i class="fa fa-check"></i><b>2.1</b> Cas séparable</a></li>
<li class="chapter" data-level="2.2" data-path="SVM.html"><a href="SVM.html#cas-non-séparable"><i class="fa fa-check"></i><b>2.2</b> Cas non séparable</a></li>
<li class="chapter" data-level="2.3" data-path="SVM.html"><a href="SVM.html#lastuce-du-noyau"><i class="fa fa-check"></i><b>2.3</b> L’astuce du noyau</a></li>
<li class="chapter" data-level="2.4" data-path="SVM.html"><a href="SVM.html#exercices"><i class="fa fa-check"></i><b>2.4</b> Exercices</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="arbres.html"><a href="arbres.html"><i class="fa fa-check"></i><b>3</b> Arbres</a><ul>
<li class="chapter" data-level="3.1" data-path="arbres.html"><a href="arbres.html#coupures-cart-en-fonction-de-la-nature-des-variables"><i class="fa fa-check"></i><b>3.1</b> Coupures CART en fonction de la nature des variables</a><ul>
<li class="chapter" data-level="3.1.1" data-path="arbres.html"><a href="arbres.html#arbres-de-régression"><i class="fa fa-check"></i><b>3.1.1</b> Arbres de régression</a></li>
<li class="chapter" data-level="3.1.2" data-path="arbres.html"><a href="arbres.html#arbres-de-classification"><i class="fa fa-check"></i><b>3.1.2</b> Arbres de classification</a></li>
<li class="chapter" data-level="3.1.3" data-path="arbres.html"><a href="arbres.html#entrée-qualitative"><i class="fa fa-check"></i><b>3.1.3</b> Entrée qualitative</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="arbres.html"><a href="arbres.html#élagage"><i class="fa fa-check"></i><b>3.2</b> Élagage</a><ul>
<li class="chapter" data-level="3.2.1" data-path="arbres.html"><a href="arbres.html#élagage-pour-un-problème-de-régression"><i class="fa fa-check"></i><b>3.2.1</b> Élagage pour un problème de régression</a></li>
<li class="chapter" data-level="3.2.2" data-path="arbres.html"><a href="arbres.html#élagage-en-classification-binaire-et-matrice-de-coût"><i class="fa fa-check"></i><b>3.2.2</b> Élagage en classification binaire et matrice de coût</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="agregation.html"><a href="agregation.html"><i class="fa fa-check"></i><b>4</b> Agrégation : forêts aléatoires et gradient boosting</a><ul>
<li class="chapter" data-level="4.1" data-path="agregation.html"><a href="agregation.html#forets"><i class="fa fa-check"></i><b>4.1</b> Forêts aléatoires</a></li>
<li class="chapter" data-level="4.2" data-path="agregation.html"><a href="agregation.html#boosting"><i class="fa fa-check"></i><b>4.2</b> Gradient boosting</a><ul>
<li class="chapter" data-level="4.2.1" data-path="agregation.html"><a href="agregation.html#un-exemple-simple-en-régression"><i class="fa fa-check"></i><b>4.2.1</b> Un exemple simple en régression</a></li>
<li class="chapter" data-level="4.2.2" data-path="agregation.html"><a href="agregation.html#adaboost-et-logitboost-pour-la-classification-binaire."><i class="fa fa-check"></i><b>4.2.2</b> Adaboost et logitboost pour la classification binaire.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="deep.html"><a href="deep.html"><i class="fa fa-check"></i><b>5</b> Réseaux de neurones avec Keras</a></li>
<li class="chapter" data-level="" data-path="références.html"><a href="références.html"><i class="fa fa-check"></i>Références</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="SVM" class="section level1">
<h1><span class="header-section-number">Chapitre 2</span> Support Vector Machine (SVM)</h1>
<p>Etant donnée un échantillon <span class="math inline">\((x_1,y_1),\dots,(x_n,y_n)\)</span> où les <span class="math inline">\(x_i\)</span> sont à valeurs dans <span class="math inline">\(\mathbb R^p\)</span> et les <span class="math inline">\(y_i\)</span> sont binaires à valeurs dans <span class="math inline">\(\{-1,1\}\)</span>, l’approche <strong>SVM</strong> cherche le <strong>meilleur hyperplan</strong> en terme de séparation des données. Globalement on veut que les <code>1</code> se trouvent d’un coté de l’hyperplan et les <code>-1</code> de l’autre. Dans cette partie on propose d’étudier la mise en œuvre de cet algorithme tout d’abord dans le cas idéal où les données sont séparables puis dans le cas plus réel où elles ne le sont pas. Nous verrons ensuite comment introduire de la non linéarité ne utilisant l’<strong>astuce du noyau</strong>.</p>
<div id="cas-séparable" class="section level2">
<h2><span class="header-section-number">2.1</span> Cas séparable</h2>
<p>Le cas séparable est le cas facile : il correspond à la situation où il existe effectivement un (même plusieurs) hyperplan(s) qui sépare(nt) parfaitement les 1 des -1. Il ne se produit quasiment jamais en pratique mais il convient de l’étudier pour comprendre comment est construit l’algorithme. Dans ce cas on cherche l’hyperplan d’équation <span class="math inline">\(\langle w,x\rangle+b=w^tx+b=0\)</span> tel que la <strong>marge</strong> (qui peut être vue comme la distance entre les observations les plus proches de l’hyperplan et l’hyperplan) soit maximale. Mathématiquement le problème se réécrit comme un problème d’optimisation sous contraintes :</p>
<p><span class="math display" id="eq:svm-non-sep">\[\begin{equation}
\min_{w,b}\frac{1}{2}\|w\|^2
\tag{2.1}
\end{equation}\]</span>
<span class="math display">\[\text{sous les contraintes } y_i(w^tx_i+b)\geq 1,\ i=1,\dots,n.\]</span></p>
<p>La solution s’obtient de façon classique en résolvant le problème dual et elle s’écrit comme une combinaison linéaire des <span class="math inline">\(x_i\)</span>
<span class="math display">\[w^\star=\sum_{i=1}^n\alpha_i^\star y_ix_i.\]</span>
De plus, les conditions <strong>KKT</strong> impliquent que pour tout <span class="math inline">\(i=1,\dots,n\)</span>:</p>
<ul>
<li><span class="math inline">\(\alpha_i^\star=0\)</span></li>
</ul>
<p>ou</p>
<ul>
<li><span class="math inline">\(y_i(x_i^tw+b)-1=0.\)</span></li>
</ul>
<p>Ces conditions impliquent que <span class="math inline">\(w^\star\)</span> s’écrit comme une combinaison linéaire de quelques points, appelés <strong>vecteurs supports</strong> qui se trouvent <strong>sur la marge</strong>. Nous proposons maintenant de retrouver ces points et de tracer la marge sur un exemple simple.</p>
<p>On considère le nuage de points suivant :</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="SVM.html#cb13-1"></a>n &lt;-<span class="st"> </span><span class="dv">20</span></span>
<span id="cb13-2"><a href="SVM.html#cb13-2"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb13-3"><a href="SVM.html#cb13-3"></a>X1 &lt;-<span class="st"> </span><span class="kw">scale</span>(<span class="kw">runif</span>(n))</span>
<span id="cb13-4"><a href="SVM.html#cb13-4"></a><span class="kw">set.seed</span>(<span class="dv">567</span>)</span>
<span id="cb13-5"><a href="SVM.html#cb13-5"></a>X2 &lt;-<span class="st"> </span><span class="kw">scale</span>(<span class="kw">runif</span>(n))</span>
<span id="cb13-6"><a href="SVM.html#cb13-6"></a>Y &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="op">-</span><span class="dv">1</span>,n)</span>
<span id="cb13-7"><a href="SVM.html#cb13-7"></a>Y[X1<span class="op">&gt;</span>X2] &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb13-8"><a href="SVM.html#cb13-8"></a>Y &lt;-<span class="st"> </span><span class="kw">as.factor</span>(Y)</span>
<span id="cb13-9"><a href="SVM.html#cb13-9"></a>donnees &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">X1=</span>X1,<span class="dt">X2=</span>X2,<span class="dt">Y=</span>Y)</span>
<span id="cb13-10"><a href="SVM.html#cb13-10"></a>p &lt;-<span class="st"> </span><span class="kw">ggplot</span>(donnees)<span class="op">+</span><span class="kw">aes</span>(<span class="dt">x=</span>X2,<span class="dt">y=</span>X1,<span class="dt">color=</span>Y)<span class="op">+</span><span class="kw">geom_point</span>()</span>
<span id="cb13-11"><a href="SVM.html#cb13-11"></a>p</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-23-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>La fonction <strong>svm</strong> du package <strong>e1071</strong> permet d’ajuster une SVM :</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="SVM.html#cb14-1"></a><span class="kw">library</span>(e1071)</span>
<span id="cb14-2"><a href="SVM.html#cb14-2"></a>mod.svm &lt;-<span class="st"> </span><span class="kw">svm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>donnees,<span class="dt">kernel=</span><span class="st">&quot;linear&quot;</span>,<span class="dt">cost=</span><span class="dv">10000000000</span>)</span></code></pre></div>
<ol style="list-style-type: decimal">
<li><p>Récupérer les vecteurs supports et visualiser les sur le graphe (en utilisant une autre couleur par exemple). On les affectera à un <strong>data.frame</strong> dont les 2 premières colonnes représenteront les valeurs de <span class="math inline">\(X_1\)</span> et <span class="math inline">\(X_2\)</span> des vecteurs supports.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="SVM.html#cb15-1"></a>ind.svm &lt;-<span class="st"> </span>mod.svm<span class="op">$</span>index</span>
<span id="cb15-2"><a href="SVM.html#cb15-2"></a>sv &lt;-<span class="st"> </span>donnees <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(ind.svm)</span>
<span id="cb15-3"><a href="SVM.html#cb15-3"></a>...</span></code></pre></div></li>
<li><p>Retrouver ce graphe à l’aide de la fonction <strong>plot</strong>.</p></li>
<li><p>Rappeler la règle de décision associée à la méthode SVM. Donner les estimations des paramètres de la règle de décision sur cet exemple. On pourra notamment regarder la sortie <code>coef</code> de la fonction <strong>svm</strong>.</p></li>
<li><p>On dispose d’un nouvel individu <span class="math inline">\(x=(-0.5,0.5)\)</span>. Expliquer comment on peut prédire son groupe.</p></li>
<li><p>Retrouver les résultats de la question précédente à l’aide de la fonction <strong>predict</strong>. On pourra utiliser l’option <code>decision.values = TRUE</code>.</p></li>
<li><p>Obtenir les probabilités prédites à l’aide de la fonction <strong>predict</strong>. On pourra utiliser <code>probability=TRUE</code> dans la fonction <strong>svm</strong>.</p></li>
</ol>
</div>
<div id="cas-non-séparable" class="section level2">
<h2><span class="header-section-number">2.2</span> Cas non séparable</h2>
<p>Dans la vraie vie, les groupes ne sont généralement pas séparables et il n’existe donc pas de solution au problème <a href="SVM.html#eq:svm-non-sep">(2.1)</a>. On va donc autoriser certains points à être :</p>
<ul>
<li>mal classés</li>
</ul>
<p>et/ou</p>
<ul>
<li>bien classés mais à l’intérieur de la marge.</li>
</ul>
<p>Mathématiquement, cela revient à introduire des <strong>variables ressorts</strong> (<strong>slacks variables</strong>) <span class="math inline">\(\xi_1,\dots,\xi_n\)</span> positives telles que :</p>
<ul>
<li><span class="math inline">\(\xi_i\in [0,1]\Longrightarrow\)</span> <span class="math inline">\(i\)</span> bien classé mais <strong>dans</strong> la région définie par la <strong>marge</strong> ;</li>
<li><span class="math inline">\(\xi_i&gt;1 \Longrightarrow\)</span> <span class="math inline">\(i\)</span> <strong>mal classé</strong>.</li>
</ul>
<p>Le problème d’optimisation est alors de minimiser en <span class="math inline">\((w,b,\xi)\)</span>
<span class="math display">\[\frac{1}{2}\|w\|^2 +C\sum_{i=1}^n\xi_i\]</span>
<span class="math display">\[\textrm{sous les contraintes } 
\left\{
  \begin{array}{l}
y_i(w^tx_i+b)\geq 1 -\xi_i \\ 
\xi_i\geq 0, i=1,\dots,n.
  \end{array}\right.\]</span>
Le paramètre <span class="math inline">\(C&gt;0\)</span> est à <strong>calibrer</strong> et on remarque que le cas séparable correspond à <span class="math inline">\(C\to +\infty\)</span>. Les solutions de ce nouveau problème d’optimisation s’obtiennent de la même façon que dans le cas séparable, en particulier <span class="math inline">\(w^\star\)</span> s’écrite toujours comme une combinaison linéaire
<span class="math display">\[w^\star=\sum_{i=1}^n\alpha_i^\star y_ix_i.\]</span>
de <strong>vecteurs supports</strong> sauf qu’on distingue deux types de vecteurs supports (<span class="math inline">\(\alpha_i^\star&gt;0\)</span>):</p>
<ul>
<li>ceux <strong>sur la frontière</strong> définie par la marge : <span class="math inline">\(\xi_i^\star=0\)</span> ;</li>
<li>ceux <strong>en dehors</strong> : <span class="math inline">\(\xi_i^\star&gt;0\)</span> et <span class="math inline">\(\alpha_i^\star=C\)</span>.</li>
</ul>
<p>Le choix de <span class="math inline">\(C\)</span> est crucial : ce paramètre régule le <strong>compromis biais/variance</strong> de la svm :</p>
<ul>
<li><span class="math inline">\(C\searrow\)</span>: la marge est privilégiée et les <span class="math inline">\(\xi_i\nearrow\)</span> <span class="math inline">\(\Longrightarrow\)</span> beaucoup d’observations dans la marge ou <strong>mal classées</strong> (et donc <strong>beaucoup de vecteurs supports</strong>).</li>
<li><span class="math inline">\(C\nearrow\Longrightarrow\)</span> <span class="math inline">\(\xi_i\searrow\)</span> donc moins d’observations mal classées <span class="math inline">\(\Longrightarrow\)</span> <strong>meilleur ajustement</strong> mais petite marge <span class="math inline">\(\Longrightarrow\)</span> risque de <strong>surajustement</strong>.</li>
</ul>
<p>On choisit généralement ce paramètre à l’aide des techiques présentées dans le chapitre <a href="caret.html#caret">1</a> :</p>
<ul>
<li>choix d’une grille de valeurs de <span class="math inline">\(C\)</span> et d’un critère ;</li>
<li>choix d’une méthode de ré-échantillonnage pour estimer le critère ;</li>
<li>choix de la valeur de <span class="math inline">\(C\)</span> qui minimise le critère estimé.</li>
</ul>
<p>On considère le jeu de données <code>df3</code> définie ci-dessous.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="SVM.html#cb16-1"></a>n &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb16-2"><a href="SVM.html#cb16-2"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb16-3"><a href="SVM.html#cb16-3"></a>df &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">matrix</span>(<span class="kw">runif</span>(<span class="dv">2</span><span class="op">*</span>n),<span class="dt">ncol=</span><span class="dv">2</span>))</span>
<span id="cb16-4"><a href="SVM.html#cb16-4"></a>df1 &lt;-<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(V1<span class="op">&lt;=</span>V2)<span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">Y=</span><span class="kw">rbinom</span>(<span class="kw">nrow</span>(.),<span class="dv">1</span>,<span class="fl">0.95</span>))</span>
<span id="cb16-5"><a href="SVM.html#cb16-5"></a>df2 &lt;-<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(V1<span class="op">&gt;</span>V2)<span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">Y=</span><span class="kw">rbinom</span>(<span class="kw">nrow</span>(.),<span class="dv">1</span>,<span class="fl">0.05</span>))</span>
<span id="cb16-6"><a href="SVM.html#cb16-6"></a>df3 &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(df1,df2) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">Y=</span><span class="kw">as.factor</span>(Y))</span>
<span id="cb16-7"><a href="SVM.html#cb16-7"></a><span class="kw">ggplot</span>(df3)<span class="op">+</span><span class="kw">aes</span>(<span class="dt">x=</span>V2,<span class="dt">y=</span>V1,<span class="dt">color=</span>Y)<span class="op">+</span><span class="kw">geom_point</span>()<span class="op">+</span></span>
<span id="cb16-8"><a href="SVM.html#cb16-8"></a><span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">values=</span><span class="kw">c</span>(<span class="st">&quot;#FFFFC8&quot;</span>, <span class="st">&quot;#7D0025&quot;</span>))<span class="op">+</span></span>
<span id="cb16-9"><a href="SVM.html#cb16-9"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">panel.background =</span> <span class="kw">element_rect</span>(<span class="dt">fill =</span> <span class="st">&quot;#BFD5E3&quot;</span>, <span class="dt">colour =</span> <span class="st">&quot;#6D9EC1&quot;</span>,<span class="dt">size =</span> <span class="dv">2</span>, <span class="dt">linetype =</span> <span class="st">&quot;solid&quot;</span>),</span>
<span id="cb16-10"><a href="SVM.html#cb16-10"></a>        <span class="dt">panel.grid.major =</span> <span class="kw">element_blank</span>(),</span>
<span id="cb16-11"><a href="SVM.html#cb16-11"></a>        <span class="dt">panel.grid.minor =</span> <span class="kw">element_blank</span>())</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-42-1.png" width="672" style="display: block; margin: auto;" /></p>
<ol style="list-style-type: decimal">
<li><p>Ajuster 3 svm en considérant comme valeur de <span class="math inline">\(C\)</span> : 0.000001, 0.1 et 5. On pourra utiliser l’option <code>cost</code>.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="SVM.html#cb17-1"></a>mod.svm1 &lt;-<span class="st"> </span><span class="kw">svm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df3,<span class="dt">kernel=</span><span class="st">&quot;linear&quot;</span>,...)</span>
<span id="cb17-2"><a href="SVM.html#cb17-2"></a>mod.svm2 &lt;-<span class="st"> </span><span class="kw">svm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df3,<span class="dt">kernel=</span><span class="st">&quot;linear&quot;</span>,...)</span>
<span id="cb17-3"><a href="SVM.html#cb17-3"></a>mod.svm3 &lt;-<span class="st"> </span><span class="kw">svm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df3,<span class="dt">kernel=</span><span class="st">&quot;linear&quot;</span>,...)</span></code></pre></div></li>
<li><p>Calculer les nombres de vecteurs supports pour chaque valeur de <span class="math inline">\(C\)</span>.</p></li>
<li><p>Visualiser les 3 svm obtenues. Interpréter.</p></li>
</ol>
</div>
<div id="lastuce-du-noyau" class="section level2">
<h2><span class="header-section-number">2.3</span> L’astuce du noyau</h2>
<p>Les SVM présentées précédemment font l’hypothèse que les groupes sont <strong>linéairement séparables</strong>, ce qui n’est bien entendu pas toujours le cas en pratique. L’<strong>astuce du noyau</strong> permet de mettre de la non linéarité, elle consiste à :</p>
<ul>
<li>plonger les données dans un nouvel espace appelé <strong>espace de représentation</strong> ou <strong>feature space</strong> ;</li>
<li>appliquer une <strong>svm</strong> linéaire dans ce nouvel espace.</li>
</ul>
<p>Le terme <strong>astuce</strong> vient du fait que ce procédé ne nécessite pas de connaître explicitement ce nouvel espace : pour résoudre le problème d’optimisation dans le <strong>feature space</strong> on a juste besoin de connaître le <strong>noyau</strong> associé au feature space. D’un point de vu formel un noyau est une fonction
<span class="math display">\[K:\mathcal X\times\mathcal X\to\mathbb R\]</span>
dont les propriétés sont proches d’un produit scalaire. Il existe donc tout un tas de noyau avec lesquels on peut faire des SVM, par exemple</p>
<ul>
<li><strong>Linéaire</strong> (sur <span class="math inline">\(\mathbb R^d\)</span>) : <span class="math inline">\(K(x,x&#39;)=x^tx&#39;\)</span>.</li>
<li><strong>Polynomial</strong> (sur <span class="math inline">\(\mathbb R^d\)</span>) : <span class="math inline">\(K(x,x&#39;)=(x^tx&#39;+1)^d\)</span>.</li>
<li><strong>Gaussien</strong> (Gaussian radial basis function ou RBF) (sur <span class="math inline">\(\mathbb R^d\)</span>)
<span class="math display">\[K(x,x&#39;)=\exp\left(-\frac{\|x-x&#39;\|}{2\sigma^2}\right).\]</span></li>
<li><strong>Laplace</strong> (sur <span class="math inline">\(\mathbb R\)</span>) : <span class="math inline">\(K(x,x&#39;)=\exp(-\gamma|x-x&#39;|)\)</span>.</li>
<li><strong>Noyau min</strong> (sur <span class="math inline">\(\mathbb R^+\)</span>) : <span class="math inline">\(K(x,x&#39;)=\min(x,x&#39;)\)</span>.</li>
<li>…</li>
</ul>
<p>Bien entendu, en pratique tout le problème va consister à <strong>trouver le bon noyau</strong> !</p>
<p>On considère le jeu de données suivant où le problème est d’expliquer <span class="math inline">\(Y\)</span> par <span class="math inline">\(V1\)</span> et <span class="math inline">\(V2\)</span>.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="SVM.html#cb18-1"></a>n &lt;-<span class="st"> </span><span class="dv">500</span></span>
<span id="cb18-2"><a href="SVM.html#cb18-2"></a><span class="kw">set.seed</span>(<span class="dv">13</span>)</span>
<span id="cb18-3"><a href="SVM.html#cb18-3"></a>X &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n<span class="op">*</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>),<span class="dt">ncol=</span><span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.data.frame</span>()</span>
<span id="cb18-4"><a href="SVM.html#cb18-4"></a>Y &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,n)</span>
<span id="cb18-5"><a href="SVM.html#cb18-5"></a>cond &lt;-<span class="st"> </span>(X<span class="op">$</span>V1<span class="op">^</span><span class="dv">2</span><span class="op">+</span>X<span class="op">$</span>V2<span class="op">^</span><span class="dv">2</span>)<span class="op">&lt;=</span><span class="fl">2.8</span></span>
<span id="cb18-6"><a href="SVM.html#cb18-6"></a>Y[cond] &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="kw">sum</span>(cond),<span class="dv">1</span>,<span class="fl">0.9</span>)</span>
<span id="cb18-7"><a href="SVM.html#cb18-7"></a>Y[<span class="op">!</span>cond] &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="kw">sum</span>(<span class="op">!</span>cond),<span class="dv">1</span>,<span class="fl">0.1</span>)</span>
<span id="cb18-8"><a href="SVM.html#cb18-8"></a>df &lt;-<span class="st"> </span>X <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">Y=</span><span class="kw">as.factor</span>(Y))</span>
<span id="cb18-9"><a href="SVM.html#cb18-9"></a><span class="kw">ggplot</span>(df)<span class="op">+</span><span class="kw">aes</span>(<span class="dt">x=</span>V2,<span class="dt">y=</span>V1,<span class="dt">color=</span>Y)<span class="op">+</span><span class="kw">geom_point</span>()<span class="op">+</span><span class="kw">theme_classic</span>()</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-47-1.png" width="672" style="display: block; margin: auto;" /></p>
<ol style="list-style-type: decimal">
<li><p>Ajuster une svm linéaire et visualiser l’hyperplan séparateur. Que remarquez-vous ?</p></li>
<li><p>Exécuter la commande suivante et commenter la sortie.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="SVM.html#cb19-1"></a>mod.svm1 &lt;-<span class="st"> </span><span class="kw">svm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df,<span class="dt">kernel=</span><span class="st">&quot;radial&quot;</span>,<span class="dt">gamma=</span><span class="dv">1</span>,<span class="dt">cost=</span><span class="dv">1</span>)</span>
<span id="cb19-2"><a href="SVM.html#cb19-2"></a><span class="kw">plot</span>(mod.svm1,df,<span class="dt">grid=</span><span class="dv">250</span>)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-50-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>Faire varier les paramètres <strong>gamma</strong> et <strong>cost</strong>. Interpréter (on pourra notamment étudier l’évolution du nombre de vecteurs supports en fonction du paramètre <strong>cost</strong>).</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="SVM.html#cb20-1"></a>mod.svm2 &lt;-<span class="st"> </span><span class="kw">svm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df,<span class="dt">kernel=</span><span class="st">&quot;radial&quot;</span>,<span class="dt">gamma=</span>...,<span class="dt">cost=</span>...)</span>
<span id="cb20-2"><a href="SVM.html#cb20-2"></a>mod.svm3 &lt;-<span class="st"> </span><span class="kw">svm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df,<span class="dt">kernel=</span><span class="st">&quot;radial&quot;</span>,<span class="dt">gamma=</span>...,<span class="dt">cost=</span>...)</span>
<span id="cb20-3"><a href="SVM.html#cb20-3"></a>mod.svm4 &lt;-<span class="st"> </span><span class="kw">svm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df,<span class="dt">kernel=</span><span class="st">&quot;radial&quot;</span>,<span class="dt">gamma=</span>...,<span class="dt">cost=</span>...)</span>
<span id="cb20-4"><a href="SVM.html#cb20-4"></a></span>
<span id="cb20-5"><a href="SVM.html#cb20-5"></a><span class="kw">plot</span>(mod.svm2,df,<span class="dt">grid=</span><span class="dv">250</span>)</span>
<span id="cb20-6"><a href="SVM.html#cb20-6"></a><span class="kw">plot</span>(mod.svm3,df,<span class="dt">grid=</span><span class="dv">250</span>)</span>
<span id="cb20-7"><a href="SVM.html#cb20-7"></a><span class="kw">plot</span>(mod.svm4,df,<span class="dt">grid=</span><span class="dv">250</span>)</span>
<span id="cb20-8"><a href="SVM.html#cb20-8"></a></span>
<span id="cb20-9"><a href="SVM.html#cb20-9"></a>mod.svm2<span class="op">$</span>nSV</span>
<span id="cb20-10"><a href="SVM.html#cb20-10"></a>mod.svm3<span class="op">$</span>nSV</span>
<span id="cb20-11"><a href="SVM.html#cb20-11"></a>mod.svm4<span class="op">$</span>nSV</span></code></pre></div></li>
<li><p>Sélectionner automatiquement ces paramètres. On pourra utiliser la fonction <strong>tune</strong> en faisant varier <strong>C</strong> dans <strong>c(0.1,1,10,100,1000)</strong> et <strong>gamma</strong> dans <strong>c(0.5,1,2,3,4)</strong>.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="SVM.html#cb21-1"></a>tune.out &lt;-<span class="st"> </span><span class="kw">tune</span>(svm,Y<span class="op">~</span>.,<span class="dt">data=</span>...,<span class="dt">kernel=</span><span class="st">&quot;...&quot;</span>,</span>
<span id="cb21-2"><a href="SVM.html#cb21-2"></a>             <span class="dt">ranges=</span><span class="kw">list</span>(<span class="dt">cost=</span>...,<span class="dt">gamma=</span>...))</span></code></pre></div></li>
<li><p>Faire de même avec <strong>caret</strong>, on utilisera <strong>method=“svmRadial”</strong> et <strong>prob.model=TRUE</strong>.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="SVM.html#cb22-1"></a>C &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.001</span>,<span class="fl">0.01</span>,<span class="dv">1</span>,<span class="dv">10</span>,<span class="dv">100</span>,<span class="dv">1000</span>)</span>
<span id="cb22-2"><a href="SVM.html#cb22-2"></a>sigma &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.5</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>)</span>
<span id="cb22-3"><a href="SVM.html#cb22-3"></a>gr &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">C=</span>C,<span class="dt">sigma=</span>sigma)</span>
<span id="cb22-4"><a href="SVM.html#cb22-4"></a>ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(...)</span>
<span id="cb22-5"><a href="SVM.html#cb22-5"></a>res.caret1 &lt;-<span class="st"> </span><span class="kw">train</span>(...,<span class="dt">prob.model=</span><span class="ot">TRUE</span>)</span>
<span id="cb22-6"><a href="SVM.html#cb22-6"></a>res.caret1</span></code></pre></div></li>
<li><p>Visualiser la règle sélectionnée.</p></li>
</ol>
</div>
<div id="exercices" class="section level2">
<h2><span class="header-section-number">2.4</span> Exercices</h2>

<div class="exercise">
<span id="exr:exo-svm-cas-sep" class="exercise"><strong>Exercice 2.1  (Résolution du problème d’optimisation dans le cas séparable)  </strong></span>
</div>

<p>On considère <span class="math inline">\(n\)</span> observations <span class="math inline">\((x_1,y_1),\dots,(x_n,y_n)\)</span> telles que <span class="math inline">\((x_i,y_i)\in\mathbb R^p\times\{-1,1\}\)</span>. On cherche à expliquer la variable <span class="math inline">\(Y\)</span> par <span class="math inline">\(X\)</span>. On considère l’algorithme SVM et on se place dans le cas où les données sont séparables.</p>
<ol style="list-style-type: decimal">
<li><p>Soit <span class="math inline">\(\mathcal H\)</span> un hyperplan séparateur d’équation <span class="math inline">\(\langle w,x\rangle+b=0\)</span> où <span class="math inline">\(w\in\mathbb R^p,b\in\mathbb R\)</span>. Exprimer la distance entre <span class="math inline">\(x_i,i=1,\dots,n\)</span> et <span class="math inline">\(\mathcal H\)</span> en fonction de <span class="math inline">\(w\)</span> et <span class="math inline">\(b\)</span>.</p></li>
<li><p>Expliquer la logique du problème d’optimisation
<span class="math display">\[\max_{w,b,\|w\|=1}M\]</span>
<span class="math display">\[\textrm{sous les contraintes } y_i(\langle w,x_i\rangle+b)\geq M,\ i=1,\dots,n.\]</span></p></li>
<li><p>Montrer que ce problème peut se réécrire
<span class="math display">\[\min_{w,b}\frac{1}{2}\|w\|^2\]</span>
<span class="math display">\[\textrm{sous les contraintes } y_i(\langle w,x_i\rangle+b)\geq 1,\ i=1,\dots,n.\]</span></p></li>
<li><p>On rappelle que pour la minimisation d’une fonction <span class="math inline">\(h:\mathbb R^p\to\mathbb R\)</span> sous contraintes affines <span class="math inline">\(g_i(u)\geq 0,i=1,\dots,n\)</span>, le Lagrangien s’écrit
<span class="math display">\[L(u,\alpha)=h(u)-\sum_{i=1}^n\alpha_ig_i(u).\]</span>
Si on désigne par <span class="math inline">\(u_\alpha=\mathop{\mathrm{argmin}}_uL(u,\alpha)\)</span>, la fonction duale est alors donnée par
<span class="math display">\[\theta(\alpha)=L(u_\alpha,\alpha)=\min_{u\in\mathbb R^p}L(u,\alpha),\]</span>
et le problème dual consiste à maximiser <span class="math inline">\(\theta(\alpha)\)</span> sous les contraintes <span class="math inline">\(\alpha_i\geq 0\)</span>. En désignant par <span class="math inline">\(\alpha^\star\)</span> la solution de ce problème, on déduit la solution du problème primal <span class="math inline">\(u^\star=u_{\alpha^\star}\)</span>. Les conditions de Karush-Kuhn-Tucker sont données par</p>
<ul>
<li><span class="math inline">\(\alpha_i^\star\geq 0\)</span>.</li>
<li><span class="math inline">\(g_i(u_{\alpha^\star})\geq 0\)</span>.</li>
<li><span class="math inline">\(\alpha_i^\star g_i(u_{\alpha^\star})=0\)</span>.</li>
</ul>
<ol style="list-style-type: lower-alpha">
<li><p>Écrire le Lagrangien du problème considéré et en déduire une expression de <span class="math inline">\(w\)</span> en fonction des <span class="math inline">\(\alpha_i\)</span> et des observations.</p></li>
<li><p>Écrire la fonction duale.</p></li>
<li><p>Écrire les conditions KKT et en déduire les solutions <span class="math inline">\(w^\star\)</span> et <span class="math inline">\(b^\star\)</span>.</p></li>
<li><p>Interpréter les conditions KKT.</p></li>
</ol></li>
</ol>

<div class="exercise">
<span id="exr:exo-svm-regle-main" class="exercise"><strong>Exercice 2.2  (Règle svm à partir de sorties R)  </strong></span>
</div>

<p>On considère <span class="math inline">\(n\)</span> observations <span class="math inline">\((x_1,y_1),\dots,(x_n,y_n)\)</span> telles que <span class="math inline">\((x_i,y_i)\in\mathbb R^3\times\{-1,1\}\)</span>. On cherche à expliquer la variable <span class="math inline">\(Y\)</span> par <span class="math inline">\(X=(X_1,X_2,X_3)\)</span>. On considère l’algorithme SVM et on se place dans le cas où les données sont séparables. On rappelle que cet algorithme consiste à chercher une droite d’équation <span class="math inline">\(w^tx+b=0\)</span> où <span class="math inline">\((w,b)\in\mathbb R^3\times\mathbb R\)</span> sont solutions du problème d’optimisation (problème primal)
<span class="math display">\[\min_{w,b}\frac{1}{2}\|w\|^2\]</span>
<span class="math display">\[\textrm{sous les contraintes } y_i(w^tx_i+b)\geq 1,\ i=1,\dots,n.\]</span>
On désigne par <span class="math inline">\(\alpha_i^\star,i=1,\dots,n\)</span>, les solutions du problème dual et par <span class="math inline">\((w^\star,b^\star)\)</span> les solutions du problème ci-dessus.</p>
<ol style="list-style-type: decimal">
<li><p>Donner la formule permettant de calculer <span class="math inline">\(w^\star\)</span> en fonction des <span class="math inline">\(\alpha_i^\star\)</span>.</p></li>
<li><p>Expliquer comment on classe un nouveau point <span class="math inline">\(x\in\mathbb R^3\)</span> par la méthode <strong>svm</strong>.</p></li>
<li><p>Les données se trouvent dans un dataframe <code>df</code>. On exécute</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="SVM.html#cb23-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb23-2"><a href="SVM.html#cb23-2"></a>n &lt;-<span class="st"> </span><span class="dv">100</span></span>
<span id="cb23-3"><a href="SVM.html#cb23-3"></a>X &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">X1=</span><span class="kw">runif</span>(n),<span class="dt">X2=</span><span class="kw">runif</span>(n),<span class="dt">X3=</span><span class="kw">runif</span>(n))</span>
<span id="cb23-4"><a href="SVM.html#cb23-4"></a>X &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">X1=</span><span class="kw">scale</span>(<span class="kw">runif</span>(n)),<span class="dt">X2=</span><span class="kw">scale</span>(<span class="kw">runif</span>(n)),<span class="dt">X3=</span><span class="kw">scale</span>(<span class="kw">runif</span>(n)))</span>
<span id="cb23-5"><a href="SVM.html#cb23-5"></a>Y &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">100</span>)</span>
<span id="cb23-6"><a href="SVM.html#cb23-6"></a>Y[X[,<span class="dv">1</span>]<span class="op">&lt;</span>X[,<span class="dv">2</span>]] &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb23-7"><a href="SVM.html#cb23-7"></a><span class="co">#Y &lt;- (apply(X,1,sum)&lt;=0) %&gt;% as.numeric() %&gt;% as.factor()</span></span>
<span id="cb23-8"><a href="SVM.html#cb23-8"></a>df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(X,<span class="dt">Y=</span><span class="kw">as.factor</span>(Y))</span></code></pre></div>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="SVM.html#cb24-1"></a>mod.svm &lt;-<span class="st"> </span><span class="kw">svm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df,<span class="dt">kernel=</span><span class="st">&quot;linear&quot;</span>,<span class="dt">cost=</span><span class="dv">10000000000</span>)</span></code></pre></div>
<p>et on obtient</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="SVM.html#cb25-1"></a>df[mod.svm<span class="op">$</span>index,]</span>
<span id="cb25-2"><a href="SVM.html#cb25-2"></a>     X1   X2   X3  Y</span>
<span id="cb25-3"><a href="SVM.html#cb25-3"></a><span class="dv">51</span> <span class="fl">-1.1</span> <span class="fl">-1.0</span> <span class="fl">-1.0</span>  <span class="dv">1</span></span>
<span id="cb25-4"><a href="SVM.html#cb25-4"></a><span class="dv">92</span>  <span class="fl">0.7</span>  <span class="fl">0.8</span>  <span class="fl">1.1</span>  <span class="dv">1</span></span>
<span id="cb25-5"><a href="SVM.html#cb25-5"></a><span class="dv">31</span>  <span class="fl">0.7</span>  <span class="fl">0.5</span> <span class="fl">-1.0</span> <span class="dv">-1</span></span>
<span id="cb25-6"><a href="SVM.html#cb25-6"></a><span class="dv">37</span> <span class="fl">-0.5</span> <span class="fl">-0.6</span>  <span class="fl">0.3</span> <span class="dv">-1</span></span>
<span id="cb25-7"><a href="SVM.html#cb25-7"></a>mod.svm<span class="op">$</span>coefs</span>
<span id="cb25-8"><a href="SVM.html#cb25-8"></a>     [,<span class="dv">1</span>]</span>
<span id="cb25-9"><a href="SVM.html#cb25-9"></a>[<span class="dv">1</span>,]   <span class="dv">59</span></span>
<span id="cb25-10"><a href="SVM.html#cb25-10"></a>[<span class="dv">2</span>,]   <span class="dv">49</span></span>
<span id="cb25-11"><a href="SVM.html#cb25-11"></a>[<span class="dv">3</span>,]  <span class="dv">-30</span></span>
<span id="cb25-12"><a href="SVM.html#cb25-12"></a>[<span class="dv">4</span>,]  <span class="dv">-79</span></span>
<span id="cb25-13"><a href="SVM.html#cb25-13"></a>mod.svm<span class="op">$</span>rho</span>
<span id="cb25-14"><a href="SVM.html#cb25-14"></a>[<span class="dv">1</span>] <span class="fl">-0.5</span></span></code></pre></div>
<p>Calculer les valeurs de <span class="math inline">\(w^\star\)</span> et <span class="math inline">\(b^\star\)</span>. En déduire la règle de classification.</p></li>
<li><p>On dispose d’une nouvelle observation <span class="math inline">\(x=(1,-0.5,-1)\)</span>. Dans quel groupe (<code>-1</code> ou <code>1</code>) l’algorithme affecte cette nouvelle donnée ?</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="caret.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="arbres.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["TUTO_ML.pdf"],
"toc": {
"collapse": "subsection",
"sharing": {
"facebook": true,
"github": true,
"twitter": true
}
},
"highlight": "tango"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
