<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapitre 2 Support Vector Machine (SVM) | Machine learning</title>
  <meta name="description" content="Chapitre 2 Support Vector Machine (SVM) | Machine learning" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapitre 2 Support Vector Machine (SVM) | Machine learning" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapitre 2 Support Vector Machine (SVM) | Machine learning" />
  
  
  

<meta name="author" content="Laurent Rouvière" />


<meta name="date" content="2020-07-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="caret.html"/>
<link rel="next" href="arbres.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/vis-4.20.1/vis.css" rel="stylesheet" />
<script src="libs/vis-4.20.1/vis.min.js"></script>
<script src="libs/visNetwork-binding-2.0.9/visNetwork.js"></script>
<script src="libs/FileSaver-1.1.20151003/FileSaver.min.js"></script>
<script src="libs/Blob-1.0/Blob.js"></script>
<script src="libs/canvas-toBlob-1.0/canvas-toBlob.js"></script>
<script src="libs/html2canvas-0.5.0/html2canvas.js"></script>
<script src="libs/jspdf-1.3.2/jspdf.debug.js"></script>
<link href="libs/jquery-sparkline-2.1.2/jquery.sparkline.css" rel="stylesheet" />
<script src="libs/jquery-sparkline-2.1.2/jquery.sparkline.js"></script>
<script src="libs/sparkline-binding-2.0/sparkline.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Machine Learning<br> <br> L. Rouvière</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Présentation</a></li>
<li class="chapter" data-level="1" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>1</b> Estimation du risque avec caret</a><ul>
<li class="chapter" data-level="1.1" data-path="caret.html"><a href="caret.html#notion-de-risque-en-apprentissage-supervisé"><i class="fa fa-check"></i><b>1.1</b> Notion de risque en apprentissage supervisé</a></li>
<li class="chapter" data-level="1.2" data-path="caret.html"><a href="caret.html#la-validation-croisée"><i class="fa fa-check"></i><b>1.2</b> La validation croisée</a></li>
<li class="chapter" data-level="1.3" data-path="caret.html"><a href="caret.html#le-package-caret"><i class="fa fa-check"></i><b>1.3</b> Le package caret</a></li>
<li class="chapter" data-level="1.4" data-path="caret.html"><a href="caret.html#compléments"><i class="fa fa-check"></i><b>1.4</b> Compléments</a><ul>
<li class="chapter" data-level="1.4.1" data-path="caret.html"><a href="caret.html#calcul-parallèle"><i class="fa fa-check"></i><b>1.4.1</b> Calcul parallèle</a></li>
<li class="chapter" data-level="1.4.2" data-path="caret.html"><a href="caret.html#répéter-les-méthodes-de-rééchantillonnage"><i class="fa fa-check"></i><b>1.4.2</b> Répéter les méthodes de rééchantillonnage</a></li>
<li class="chapter" data-level="1.4.3" data-path="caret.html"><a href="caret.html#modifier-le-risque"><i class="fa fa-check"></i><b>1.4.3</b> Modifier le risque</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="SVM.html"><a href="SVM.html"><i class="fa fa-check"></i><b>2</b> Support Vector Machine (SVM)</a><ul>
<li class="chapter" data-level="2.1" data-path="SVM.html"><a href="SVM.html#cas-séparable"><i class="fa fa-check"></i><b>2.1</b> Cas séparable</a></li>
<li class="chapter" data-level="2.2" data-path="SVM.html"><a href="SVM.html#cas-non-séparable"><i class="fa fa-check"></i><b>2.2</b> Cas non séparable</a></li>
<li class="chapter" data-level="2.3" data-path="SVM.html"><a href="SVM.html#lastuce-du-noyau"><i class="fa fa-check"></i><b>2.3</b> L’astuce du noyau</a></li>
<li class="chapter" data-level="2.4" data-path="SVM.html"><a href="SVM.html#exercices"><i class="fa fa-check"></i><b>2.4</b> Exercices</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="arbres.html"><a href="arbres.html"><i class="fa fa-check"></i><b>3</b> Arbres</a><ul>
<li class="chapter" data-level="3.1" data-path="arbres.html"><a href="arbres.html#coupures-cart-en-fonction-de-la-nature-des-variables"><i class="fa fa-check"></i><b>3.1</b> Coupures CART en fonction de la nature des variables</a><ul>
<li class="chapter" data-level="3.1.1" data-path="arbres.html"><a href="arbres.html#arbres-de-régression"><i class="fa fa-check"></i><b>3.1.1</b> Arbres de régression</a></li>
<li class="chapter" data-level="3.1.2" data-path="arbres.html"><a href="arbres.html#arbres-de-classification"><i class="fa fa-check"></i><b>3.1.2</b> Arbres de classification</a></li>
<li class="chapter" data-level="3.1.3" data-path="arbres.html"><a href="arbres.html#entrée-qualitative"><i class="fa fa-check"></i><b>3.1.3</b> Entrée qualitative</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="arbres.html"><a href="arbres.html#élagage"><i class="fa fa-check"></i><b>3.2</b> Élagage</a><ul>
<li class="chapter" data-level="3.2.1" data-path="arbres.html"><a href="arbres.html#élagage-pour-un-problème-de-régression"><i class="fa fa-check"></i><b>3.2.1</b> Élagage pour un problème de régression</a></li>
<li class="chapter" data-level="3.2.2" data-path="arbres.html"><a href="arbres.html#élagage-en-classification-binaire-et-matrice-de-coût"><i class="fa fa-check"></i><b>3.2.2</b> Élagage en classification binaire et matrice de coût</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="références.html"><a href="références.html"><i class="fa fa-check"></i>Références</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="SVM" class="section level1">
<h1><span class="header-section-number">Chapitre 2</span> Support Vector Machine (SVM)</h1>
<p>Etant donnée un échantillon <span class="math inline">\((x_1,y_1),\dots,(x_n,y_n)\)</span> où les <span class="math inline">\(x_i\)</span> sont à valeurs dans <span class="math inline">\(\mathbb R^p\)</span> et les <span class="math inline">\(y_i\)</span> sont binaires à valeurs dans <span class="math inline">\(\{-1,1\}\)</span>, l’approche <strong>SVM</strong> cherche le <strong>meilleur hyperplan</strong> en terme de séparation des données. Globalement on veut que les <code>1</code> se trouvent d’un coté de l’hyperplan et les <code>-1</code> de l’autre. Dans cette partie on propose d’étudier la mise en œuvre de cet algorithme tout d’abord dans le cas idéal où les données sont séparables puis dans le cas plus réel où elles ne le sont pas. Nous verrons ensuite comment introduire de la non linéarité ne utilisant l’<strong>astuce du noyau</strong>.</p>
<div id="cas-séparable" class="section level2">
<h2><span class="header-section-number">2.1</span> Cas séparable</h2>
<p>Le cas séparable est le cas facile : il correspond à la situation où il existe effectivement un (même plusieurs) hyperplan(s) qui sépare(nt) parfaitement les 1 des -1. Il n’arrive jamais en pratique mais il convient de l’étudier pour comprendre comment est construit l’algorithme. Dans ce cas on cherche l’hyperplan d’équation <span class="math inline">\(\langle w,x\rangle+b=w^tx+b=0\)</span> tel que la <strong>marge</strong> (qui peut être vue comme la distance entre les observations les plus priche de l’hyperplan et l’hyperplan) soit maximale. Mathématiquement le problème se réécrit comme un problème d’optimisation sous contraintes :</p>
<p><span class="math display" id="eq:svm-non-sep">\[\begin{equation}
\min_{w,b}\frac{1}{2}\|w\|^2
\tag{2.1}
\end{equation}\]</span>
<span class="math display">\[\text{sous les contraintes } y_i(w^tx_i+b)\geq 1,\ i=1,\dots,n.\]</span></p>
<p>La solution s’obtient de façon classique en résolvant le problème dual et elle s’écrit comme une combinaison linéaire des <span class="math inline">\(x_i\)</span>
<span class="math display">\[w^\star=\sum_{i=1}^n\alpha_i^\star y_ix_i.\]</span>
De plus, les conditions <strong>KKT</strong> impliquent que pour tout <span class="math inline">\(i=1,\dots,n\)</span>:</p>
<ul>
<li><span class="math inline">\(\alpha_i^\star=0\)</span></li>
</ul>
<p>ou</p>
<ul>
<li><span class="math inline">\(y_i(x_i^tw+b)-1=0.\)</span></li>
</ul>
<p>Ces conditions entraînent que <span class="math inline">\(w^\star\)</span> s’écrit comme uen combinaison linéaire de <strong>quelques points</strong> qui se trouvent <strong>sur la marge</strong>. Nous proposons maintenant de retrouver ces points et de tracer la marge sur un exemple simple.</p>
<p>On considère le nuage de points suivant :</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="SVM.html#cb36-1"></a>n &lt;-<span class="st"> </span><span class="dv">20</span></span>
<span id="cb36-2"><a href="SVM.html#cb36-2"></a><span class="kw">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb36-3"><a href="SVM.html#cb36-3"></a>X1 &lt;-<span class="st"> </span><span class="kw">scale</span>(<span class="kw">runif</span>(n))</span>
<span id="cb36-4"><a href="SVM.html#cb36-4"></a><span class="kw">set.seed</span>(<span class="dv">567</span>)</span>
<span id="cb36-5"><a href="SVM.html#cb36-5"></a>X2 &lt;-<span class="st"> </span><span class="kw">scale</span>(<span class="kw">runif</span>(n))</span>
<span id="cb36-6"><a href="SVM.html#cb36-6"></a>Y &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="op">-</span><span class="dv">1</span>,n)</span>
<span id="cb36-7"><a href="SVM.html#cb36-7"></a>Y[X1<span class="op">&gt;</span>X2] &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb36-8"><a href="SVM.html#cb36-8"></a>Y &lt;-<span class="st"> </span><span class="kw">as.factor</span>(Y)</span>
<span id="cb36-9"><a href="SVM.html#cb36-9"></a>donnees &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">X1=</span>X1,<span class="dt">X2=</span>X2,<span class="dt">Y=</span>Y)</span>
<span id="cb36-10"><a href="SVM.html#cb36-10"></a>p &lt;-<span class="st"> </span><span class="kw">ggplot</span>(donnees)<span class="op">+</span><span class="kw">aes</span>(<span class="dt">x=</span>X2,<span class="dt">y=</span>X1,<span class="dt">color=</span>Y)<span class="op">+</span><span class="kw">geom_point</span>()</span>
<span id="cb36-11"><a href="SVM.html#cb36-11"></a>p</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-23-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>La fonction <strong>svm</strong> du package <strong>e1071</strong> permet d’ajuster une SVM :</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="SVM.html#cb37-1"></a><span class="kw">library</span>(e1071)</span>
<span id="cb37-2"><a href="SVM.html#cb37-2"></a>mod.svm &lt;-<span class="st"> </span><span class="kw">svm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>donnees,<span class="dt">kernel=</span><span class="st">&quot;linear&quot;</span>,<span class="dt">cost=</span><span class="dv">10000000000</span>)</span></code></pre></div>
<ol style="list-style-type: decimal">
<li><p>Récupérer les vecteurs supports et visualiser les sur le graphe. On les affectera à un <strong>data.frame</strong> dont les 2 premières colonnes représenteront les valeurs de <span class="math inline">\(X_1\)</span> et <span class="math inline">\(X_2\)</span> des vecteurs supports.</p>
<div class="corR">
<p>
Les vecteurs supports se trouvent dans la sortie <code>index</code> de la fonction <strong>svm</strong> :
</p>
</div>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="SVM.html#cb38-1"></a>ind.svm &lt;-<span class="st"> </span>mod.svm<span class="op">$</span>index</span>
<span id="cb38-2"><a href="SVM.html#cb38-2"></a>sv &lt;-<span class="st"> </span>donnees <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">slice</span>(ind.svm)</span>
<span id="cb38-3"><a href="SVM.html#cb38-3"></a>sv</span></code></pre></div>
<pre><code>           X1         X2  Y
1 -1.61179777 -0.6599042 -1
2  0.06962369  0.7140262 -1
3 -0.31095135 -0.5332139  1</code></pre>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="SVM.html#cb40-1"></a>p1 &lt;-<span class="st"> </span>p<span class="op">+</span><span class="kw">geom_point</span>(<span class="dt">data=</span>sv,<span class="kw">aes</span>(<span class="dt">x=</span>X2,<span class="dt">y=</span>X1),<span class="dt">color=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">size=</span><span class="dv">2</span>)</span></code></pre></div>
<div class="corR">
<p>
On peut ainsi représenter la marge en traçant les droites qui passent par ces points.
</p>
</div>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="SVM.html#cb41-1"></a>sv1 &lt;-<span class="st"> </span>sv[,<span class="dv">2</span><span class="op">:</span><span class="dv">1</span>]</span>
<span id="cb41-2"><a href="SVM.html#cb41-2"></a>b &lt;-<span class="st"> </span>(sv1[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">-</span>sv1[<span class="dv">2</span>,<span class="dv">2</span>])<span class="op">/</span>(sv1[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">-</span>sv1[<span class="dv">2</span>,<span class="dv">1</span>])</span>
<span id="cb41-3"><a href="SVM.html#cb41-3"></a>a &lt;-<span class="st"> </span>sv1[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">-</span>b<span class="op">*</span>sv1[<span class="dv">1</span>,<span class="dv">1</span>]</span>
<span id="cb41-4"><a href="SVM.html#cb41-4"></a>a1 &lt;-<span class="st"> </span>sv1[<span class="dv">3</span>,<span class="dv">2</span>]<span class="op">-</span>b<span class="op">*</span>sv1[<span class="dv">3</span>,<span class="dv">1</span>]</span>
<span id="cb41-5"><a href="SVM.html#cb41-5"></a>p1<span class="op">+</span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="kw">c</span>(a,a1),<span class="dt">slope=</span>b,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">size=</span><span class="dv">1</span>)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-29-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>Retrouver ce graphe à l’aide de la fonction <strong>plot</strong>.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="SVM.html#cb42-1"></a><span class="kw">plot</span>(mod.svm,<span class="dt">data=</span>donnees,<span class="dt">grid=</span><span class="dv">250</span>)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-30-1.png" width="672" style="display: block; margin: auto;" /></p></li>
<li><p>Rappeler la règle de décision associée la méthode SVM. Donner les estimations des paramètres de la règle de décision sur cet exemple. On pourra notamment regarder la sortie <code>coef</code> de la fonction <strong>svm</strong>.</p>
<div class="corR">
<p>
Une fois <span class="math inline"><span class="math inline">\(w^\star\)</span></span> et <span class="math inline"><span class="math inline">\(b^\star\)</span></span> obtenus, la règle s’écrit <span class="math display"><span class="math display">\[g(x)=1_{\langle w^\star,x\rangle+b^\star\leq 0}-1_{\langle w^\star,x\rangle+b^\star&gt;0}.\]</span></span>
</p>
<p>
L’objet <code>mod.svm$coefs</code> contient les coefficients <span class="math inline"><span class="math inline">\(\alpha_i^\star y_i\)</span></span> pour chaque vecteur support. On peut ainsi récupérer l’équation de l’hyperplan et faire la prévision avec
</p>
</div>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="SVM.html#cb43-1"></a>w &lt;-<span class="st"> </span><span class="kw">apply</span>(mod.svm<span class="op">$</span>coefs<span class="op">*</span>donnees[mod.svm<span class="op">$</span>index,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>],<span class="dv">2</span>,sum)</span>
<span id="cb43-2"><a href="SVM.html#cb43-2"></a>w</span></code></pre></div>
<pre><code>       X1        X2 
-1.745100  2.136029 </code></pre>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="SVM.html#cb45-1"></a>b &lt;-<span class="st"> </span><span class="op">-</span>mod.svm<span class="op">$</span>rho</span>
<span id="cb45-2"><a href="SVM.html#cb45-2"></a>b</span></code></pre></div>
<pre><code>[1] -0.4035113</code></pre>
<div class="corR">
<p>
L’hyperplan séparateur a donc pour équation : <span class="math display"><span class="math display">\[-1.74x_1+2.12x_2-0.40=0.\]</span></span>
</p>
</div></li>
<li><p>On dispose d’un nouvel individu <span class="math inline">\(x=(-0.5,0.5)\)</span>. Expliquer comment on peut prédire son groupe.</p>
<div class="corR">
<p>
Il suffit de calculer <span class="math inline"><span class="math inline">\(\langle w^\star,x\rangle+b\)</span></span> et de prédire en fonction du signe de cette valeur :
</p>
</div>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="SVM.html#cb47-1"></a>newX &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">X1=</span><span class="op">-</span><span class="fl">0.5</span>,<span class="dt">X2=</span><span class="fl">0.5</span>)</span>
<span id="cb47-2"><a href="SVM.html#cb47-2"></a><span class="kw">sum</span>(w<span class="op">*</span>newX)<span class="op">+</span>b</span></code></pre></div>
<pre><code>[1] 1.537053</code></pre>
<div class="corR">
<p>
On prédira le groupe <code>-1</code> pour ce nouvel individu.
</p>
</div></li>
<li><p>Retrouver les résultats de la question précédente à l’aide de la fonction <strong>predict</strong>. On pourra utiliser l’option <code>decision.values = TRUE</code>.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="SVM.html#cb49-1"></a><span class="kw">predict</span>(mod.svm,newX,<span class="dt">decision.values =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<pre><code> 1 
-1 
attr(,&quot;decision.values&quot;)
      -1/1
1 1.537053
Levels: -1 1</code></pre>
<div class="corR">
<p>
Plus cette valeur est élevée, plus on est loin de l’hyperplan. On peut donc l’interpréter comme un score.
</p>
</div></li>
<li><p>Obtenir les probabilités prédites à l’aide de la fonction <strong>predict</strong>. On pourra utiliser <code>probability=TRUE</code> dans la fonction <strong>svm</strong>.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="SVM.html#cb51-1"></a>mod.svm1 &lt;-<span class="st"> </span><span class="kw">svm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>donnees,<span class="dt">kernel=</span><span class="st">&quot;linear&quot;</span>,<span class="dt">cost=</span><span class="dv">10000000000</span>,<span class="dt">probability=</span><span class="ot">TRUE</span>)</span>
<span id="cb51-2"><a href="SVM.html#cb51-2"></a><span class="kw">predict</span>(mod.svm1,newX,<span class="dt">decision.values=</span><span class="ot">TRUE</span>,<span class="dt">probability=</span><span class="ot">TRUE</span>)</span></code></pre></div>
<pre><code> 1 
-1 
attr(,&quot;decision.values&quot;)
      -1/1
1 1.537053
attr(,&quot;probabilities&quot;)
         -1         1
1 0.8294474 0.1705526
Levels: -1 1</code></pre>
<div class="corR">
<p>
Comme souvent, il est possible d’obtenir une estimation des probabilités d’être dans les groupes <code>-1</code> et <code>1</code> à partir du score, il “suffit” de ramener ce score sur l’échelle <span class="math inline"><span class="math inline">\([0,1]\)</span></span> avec des transformations de type logit par exemple. Pour la svm, ces probabilités sont obtenues en ajustant un modèle logistique sur les scores <span class="math inline"><span class="math inline">\(S(x)\)</span></span> : <span class="math display"><span class="math display">\[P(Y=1|X=x)=\frac{1}{1+\exp(aS(x)+b)}.\]</span></span> On peut retrouver ces probabilités avec :
</p>
</div>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="SVM.html#cb53-1"></a>score.newX &lt;-<span class="st"> </span><span class="kw">sum</span>(w<span class="op">*</span>newX)<span class="op">+</span>b</span>
<span id="cb53-2"><a href="SVM.html#cb53-2"></a><span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(<span class="op">-</span>(mod.svm1<span class="op">$</span>probB<span class="op">+</span>mod.svm1<span class="op">$</span>probA<span class="op">*</span>score.newX)))</span></code></pre></div>
<pre><code>[1] 0.1705526</code></pre></li>
</ol>
</div>
<div id="cas-non-séparable" class="section level2">
<h2><span class="header-section-number">2.2</span> Cas non séparable</h2>
<p>Dans la vraie vie, les groupes ne sont généralement pas séparables et il n’existe donc pas de solution au problème <a href="SVM.html#eq:svm-non-sep">(2.1)</a>. On va donc autoriser certains points à être :</p>
<ul>
<li>mal classés</li>
</ul>
<p>et/ou</p>
<ul>
<li>bien classés mais à l’intérieur de la marge.</li>
</ul>
<p>Mathématiquement, cela revient à introduire des <strong>variables ressorts</strong> (<strong>slacks variables</strong>) <span class="math inline">\(\xi_1,\dots,\xi_n\)</span> positives telles que :</p>
<ul>
<li><span class="math inline">\(\xi_i\in [0,1]\Longrightarrow\)</span> <span class="math inline">\(i\)</span> bien classé mais <strong>dans</strong> la région définie par la <strong>marge</strong> ;</li>
<li><span class="math inline">\(\xi_i&gt;1 \Longrightarrow\)</span> <span class="math inline">\(i\)</span> <strong>mal classé</strong>.</li>
</ul>
<p>Le problème d’optimisation devient alors minimiser en <span class="math inline">\((w,b,\xi)\)</span>
<span class="math display">\[\frac{1}{2}\|w\|^2 +C\sum_{i=1}^n\xi_i\]</span>
<span class="math display">\[\textrm{sous les contraintes } 
\left\{
  \begin{array}{l}
y_i(w^tx_i+b)\geq 1 -\xi_i \\ 
\xi_i\geq 0, i=1,\dots,n.
  \end{array}\right.\]</span>
Le paramètre <span class="math inline">\(C&gt;0\)</span> est à <strong>calibrer</strong> et on remarque que le cas séparable correspond à <span class="math inline">\(C\to +\infty\)</span>. Les solutions de ce nouveau problème d’ptimisation s’obtiennent de la même façon que dans le cas séparable, en particulier <span class="math inline">\(w^\star\)</span> s’écrite toujours comme une combinaison linéaire
<span class="math display">\[w^\star=\sum_{i=1}^n\alpha_i^\star y_ix_i.\]</span>
de <strong>vecteurs supports</strong> sauf qu’on distingue deux cas de vecteurs supports (<span class="math inline">\(\alpha_i^\star&gt;0\)</span>):</p>
<ul>
<li>ceux <strong>sur la frontière</strong> définie par la marge : <span class="math inline">\(\xi_i^\star=0\)</span> ;</li>
<li>ceux <strong>en dehors</strong> : <span class="math inline">\(\xi_i^\star&gt;0\)</span> et <span class="math inline">\(\alpha_i^\star=C\)</span>.</li>
</ul>
<p>Le choix de <span class="math inline">\(C\)</span> est crucial : ce paramètre régule le <strong>compromis biais/variance</strong> de la svm :</p>
<ul>
<li><span class="math inline">\(C\searrow\)</span>: la marge est privilégiée et les <span class="math inline">\(\xi_i\nearrow\)</span> <span class="math inline">\(\Longrightarrow\)</span> beaucoup d’observations dans la marge ou <strong>mal classées</strong> (et donc <strong>beaucoup de vecteurs supports</strong>).</li>
<li><span class="math inline">\(C\nearrow\Longrightarrow\)</span> <span class="math inline">\(\xi_i\searrow\)</span> donc moins d’observations mal classées <span class="math inline">\(\Longrightarrow\)</span> <strong>meilleur ajustement</strong> mais petite marge <span class="math inline">\(\Longrightarrow\)</span> risque de <strong>surajustement</strong>.</li>
</ul>
<p>On choisit généralement ce paramètre à l’aide des techiques présentées dans le chapitre <a href="caret.html#caret">1</a> :</p>
<ul>
<li>choix d’une grille de valeurs de <span class="math inline">\(C\)</span> et d’un critère ;</li>
<li>choix d’une méthode de ré-échantillonnage pour estimer le critère ;</li>
<li>choix de la valeur de <span class="math inline">\(C\)</span> qui minimise le critère estimé.</li>
</ul>
<p>On considère le jeu de données <code>df3</code> définie ci-dessous.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="SVM.html#cb55-1"></a>n &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb55-2"><a href="SVM.html#cb55-2"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb55-3"><a href="SVM.html#cb55-3"></a>df &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">matrix</span>(<span class="kw">runif</span>(<span class="dv">2</span><span class="op">*</span>n),<span class="dt">ncol=</span><span class="dv">2</span>))</span>
<span id="cb55-4"><a href="SVM.html#cb55-4"></a>df1 &lt;-<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(V1<span class="op">&lt;=</span>V2)<span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">Y=</span><span class="kw">rbinom</span>(<span class="kw">nrow</span>(.),<span class="dv">1</span>,<span class="fl">0.95</span>))</span>
<span id="cb55-5"><a href="SVM.html#cb55-5"></a>df2 &lt;-<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(V1<span class="op">&gt;</span>V2)<span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">Y=</span><span class="kw">rbinom</span>(<span class="kw">nrow</span>(.),<span class="dv">1</span>,<span class="fl">0.05</span>))</span>
<span id="cb55-6"><a href="SVM.html#cb55-6"></a>df3 &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(df1,df2) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">Y=</span><span class="kw">as.factor</span>(Y))</span>
<span id="cb55-7"><a href="SVM.html#cb55-7"></a><span class="kw">ggplot</span>(df3)<span class="op">+</span><span class="kw">aes</span>(<span class="dt">x=</span>V2,<span class="dt">y=</span>V1,<span class="dt">color=</span>Y)<span class="op">+</span><span class="kw">geom_point</span>()<span class="op">+</span></span>
<span id="cb55-8"><a href="SVM.html#cb55-8"></a><span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">values=</span><span class="kw">c</span>(<span class="st">&quot;#FFFFC8&quot;</span>, <span class="st">&quot;#7D0025&quot;</span>))<span class="op">+</span></span>
<span id="cb55-9"><a href="SVM.html#cb55-9"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">panel.background =</span> <span class="kw">element_rect</span>(<span class="dt">fill =</span> <span class="st">&quot;#BFD5E3&quot;</span>, <span class="dt">colour =</span> <span class="st">&quot;#6D9EC1&quot;</span>,<span class="dt">size =</span> <span class="dv">2</span>, <span class="dt">linetype =</span> <span class="st">&quot;solid&quot;</span>),</span>
<span id="cb55-10"><a href="SVM.html#cb55-10"></a>        <span class="dt">panel.grid.major =</span> <span class="kw">element_blank</span>(),</span>
<span id="cb55-11"><a href="SVM.html#cb55-11"></a>        <span class="dt">panel.grid.minor =</span> <span class="kw">element_blank</span>())</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-42-1.png" width="672" style="display: block; margin: auto;" /></p>
<ol style="list-style-type: decimal">
<li><p>Ajuster 3 svm en considérant comme valeur de <span class="math inline">\(C\)</span> : 0.000001, 0.1 et 5. On pourra utiliser l’option <code>cost</code>.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="SVM.html#cb56-1"></a>mod.svm1 &lt;-<span class="st"> </span><span class="kw">svm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df3,<span class="dt">kernel=</span><span class="st">&quot;linear&quot;</span>,<span class="dt">cost=</span><span class="fl">0.000001</span>)</span>
<span id="cb56-2"><a href="SVM.html#cb56-2"></a>mod.svm2 &lt;-<span class="st"> </span><span class="kw">svm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df3,<span class="dt">kernel=</span><span class="st">&quot;linear&quot;</span>,<span class="dt">cost=</span><span class="fl">0.1</span>)</span>
<span id="cb56-3"><a href="SVM.html#cb56-3"></a>mod.svm3 &lt;-<span class="st"> </span><span class="kw">svm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df3,<span class="dt">kernel=</span><span class="st">&quot;linear&quot;</span>,<span class="dt">cost=</span><span class="dv">5</span>)</span></code></pre></div></li>
<li><p>Calculer les nombres de vecteurs supports pour chaque valeur de <span class="math inline">\(C\)</span>.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="SVM.html#cb57-1"></a>mod.svm1<span class="op">$</span>nSV</span></code></pre></div>
<pre><code>[1] 469 469</code></pre>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="SVM.html#cb59-1"></a>mod.svm2<span class="op">$</span>nSV</span></code></pre></div>
<pre><code>[1] 178 178</code></pre>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="SVM.html#cb61-1"></a>mod.svm3<span class="op">$</span>nSV</span></code></pre></div>
<pre><code>[1] 150 150</code></pre></li>
<li><p>Visualiser les 3 svm obtenues. Interpréter.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="SVM.html#cb63-1"></a><span class="kw">plot</span>(mod.svm1,<span class="dt">data=</span>df3,<span class="dt">grid=</span><span class="dv">250</span>)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/trace-trois-svm-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="SVM.html#cb64-1"></a><span class="kw">plot</span>(mod.svm2,<span class="dt">data=</span>df3,<span class="dt">grid=</span><span class="dv">250</span>)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/trace-trois-svm-2.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="SVM.html#cb65-1"></a><span class="kw">plot</span>(mod.svm3,<span class="dt">data=</span>df3,<span class="dt">grid=</span><span class="dv">250</span>)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/trace-trois-svm-3.png" width="672" style="display: block; margin: auto;" /></p>
<div class="corR">
<p>
Pour <span class="math inline"><span class="math inline">\(C\)</span></span> petit, toutes les observations sont classées <code>0</code>, la marge est grande et le nombre de vecteurs supports importants. On remarque que ces deux quantités diminuent lorsque <span class="math inline"><span class="math inline">\(C\)</span></span> augmente.
</p>
</div></li>
</ol>
</div>
<div id="lastuce-du-noyau" class="section level2">
<h2><span class="header-section-number">2.3</span> L’astuce du noyau</h2>
<p>Les SVM présentées précédemment font l’hypothèse que les groupes sont <strong>linéairement séparables</strong>, ce qui n’est bien entendu pas toujours le cas en pratique. L’<strong>astuce du noyau</strong> permet de mettre de la non linéarité, elle consiste à :</p>
<ul>
<li>plonger les données dans un nouvel espace appelé <strong>espace de représentation</strong> ou <strong>feature space</strong></li>
<li>appliquer une <strong>svm</strong> linéaire dans ce nouvel espace.</li>
</ul>
<p>Le terme <strong>astuce</strong> vient du fait que ce procédé ne nécessite pas de connaître explicitement ce nouvel espace : pour résoudre le problème d’optimisation dans le <strong>faeture space</strong> on a juste besoin de connaître le <strong>noyau</strong> associé au feature space. D’un point de vu formel un noyau est une fonction
<span class="math display">\[K:\mathcal X\times\mathcal X\to\mathbb R\]</span>
dont les propriétés sont proches d’un produit scalaire. Il existe donc tout un tas de noyau avec lesquels on peut faire des SVM, par exemple</p>
<ul>
<li><strong>Linéaire</strong> (sur <span class="math inline">\(\mathbb R^d\)</span>) : <span class="math inline">\(K(x,x&#39;)=x^tx&#39;\)</span>.</li>
<li><strong>Polynomial</strong> (sur <span class="math inline">\(\mathbb R^d\)</span>) : <span class="math inline">\(K(x,x&#39;)=(x^tx&#39;+1)^d\)</span>.</li>
<li><strong>Gaussien</strong> (Gaussian radial basis function ou RBF) (sur <span class="math inline">\(\mathbb R^d\)</span>)
<span class="math display">\[K(x,x&#39;)=\exp\left(-\frac{\|x-x&#39;\|}{2\sigma^2}\right).\]</span></li>
<li><strong>Laplace</strong> (sur <span class="math inline">\(\mathbb R\)</span>) : <span class="math inline">\(K(x,x&#39;)=\exp(-\gamma|x-x&#39;|)\)</span>.</li>
<li><strong>Noyau min</strong> (sur <span class="math inline">\(\mathbb R^+\)</span>) : <span class="math inline">\(K(x,x&#39;)=\min(x,x&#39;)\)</span>.</li>
<li>…</li>
</ul>
<p>Bien entendu, en pratique tout le problème va consister à <strong>trouver le bon noyau</strong> !</p>
<p>On considère le jeu de données suivant où le problème est d’expliquer <span class="math inline">\(Y\)</span> par <span class="math inline">\(V1\)</span> et <span class="math inline">\(V2\)</span>.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="SVM.html#cb66-1"></a>n &lt;-<span class="st"> </span><span class="dv">500</span></span>
<span id="cb66-2"><a href="SVM.html#cb66-2"></a><span class="kw">set.seed</span>(<span class="dv">13</span>)</span>
<span id="cb66-3"><a href="SVM.html#cb66-3"></a>X &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n<span class="op">*</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>),<span class="dt">ncol=</span><span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.data.frame</span>()</span>
<span id="cb66-4"><a href="SVM.html#cb66-4"></a>Y &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,n)</span>
<span id="cb66-5"><a href="SVM.html#cb66-5"></a>cond &lt;-<span class="st"> </span>(X<span class="op">$</span>V1<span class="op">^</span><span class="dv">2</span><span class="op">+</span>X<span class="op">$</span>V2<span class="op">^</span><span class="dv">2</span>)<span class="op">&lt;=</span><span class="fl">2.8</span></span>
<span id="cb66-6"><a href="SVM.html#cb66-6"></a>Y[cond] &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="kw">sum</span>(cond),<span class="dv">1</span>,<span class="fl">0.9</span>)</span>
<span id="cb66-7"><a href="SVM.html#cb66-7"></a>Y[<span class="op">!</span>cond] &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="kw">sum</span>(<span class="op">!</span>cond),<span class="dv">1</span>,<span class="fl">0.1</span>)</span>
<span id="cb66-8"><a href="SVM.html#cb66-8"></a>df &lt;-<span class="st"> </span>X <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mutate</span>(<span class="dt">Y=</span><span class="kw">as.factor</span>(Y))</span>
<span id="cb66-9"><a href="SVM.html#cb66-9"></a><span class="kw">ggplot</span>(df)<span class="op">+</span><span class="kw">aes</span>(<span class="dt">x=</span>V2,<span class="dt">y=</span>V1,<span class="dt">color=</span>Y)<span class="op">+</span><span class="kw">geom_point</span>()<span class="op">+</span><span class="kw">theme_classic</span>()</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-47-1.png" width="672" style="display: block; margin: auto;" /></p>
<ol style="list-style-type: decimal">
<li><p>Ajuster une svm linéaire et visualiser l’hyperplan séparateur. Que remarquez-vous ?</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="SVM.html#cb67-1"></a>mod.svm0 &lt;-<span class="st"> </span><span class="kw">svm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df,<span class="dt">kernel=</span><span class="st">&quot;linear&quot;</span>,<span class="dt">cost=</span><span class="dv">1</span>)</span>
<span id="cb67-2"><a href="SVM.html#cb67-2"></a><span class="kw">plot</span>(mod.svm0,df,<span class="dt">grid=</span><span class="dv">250</span>)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-48-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="corR">
<p>
La svm linéaire ne permet pas de séparer les groupes (on pouvait s’y attendre).
</p>
</div></li>
<li><p>Exécuter la commande suivante et commenter la sortie.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="SVM.html#cb68-1"></a>mod.svm1 &lt;-<span class="st"> </span><span class="kw">svm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df,<span class="dt">kernel=</span><span class="st">&quot;radial&quot;</span>,<span class="dt">gamma=</span><span class="dv">1</span>,<span class="dt">cost=</span><span class="dv">1</span>)</span>
<span id="cb68-2"><a href="SVM.html#cb68-2"></a><span class="kw">plot</span>(mod.svm1,df,<span class="dt">grid=</span><span class="dv">250</span>)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-50-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="corR">
<p>
Le noyau radial permet de mettre en évidence une séparation non linéaire.
</p>
</div></li>
<li><p>Faire varier les paramètres <strong>gamma</strong> et <strong>cost</strong>. Interpréter (on pourra notamment étudier l’évolution du nombre de vecteurs supports en fonction du paramètre <strong>cost</strong>).</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="SVM.html#cb69-1"></a>mod.svm2 &lt;-<span class="st"> </span><span class="kw">svm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df,<span class="dt">kernel=</span><span class="st">&quot;radial&quot;</span>,<span class="dt">gamma=</span><span class="dv">1</span>,<span class="dt">cost=</span><span class="fl">0.0001</span>)</span>
<span id="cb69-2"><a href="SVM.html#cb69-2"></a>mod.svm3 &lt;-<span class="st"> </span><span class="kw">svm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df,<span class="dt">kernel=</span><span class="st">&quot;radial&quot;</span>,<span class="dt">gamma=</span><span class="dv">1</span>,<span class="dt">cost=</span><span class="dv">1</span>)</span>
<span id="cb69-3"><a href="SVM.html#cb69-3"></a>mod.svm4 &lt;-<span class="st"> </span><span class="kw">svm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df,<span class="dt">kernel=</span><span class="st">&quot;radial&quot;</span>,<span class="dt">gamma=</span><span class="dv">1</span>,<span class="dt">cost=</span><span class="dv">100000</span>)</span>
<span id="cb69-4"><a href="SVM.html#cb69-4"></a></span>
<span id="cb69-5"><a href="SVM.html#cb69-5"></a><span class="kw">plot</span>(mod.svm2,df,<span class="dt">grid=</span><span class="dv">250</span>)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/svm-test-gamma-cost-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="SVM.html#cb70-1"></a><span class="kw">plot</span>(mod.svm3,df,<span class="dt">grid=</span><span class="dv">250</span>)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/svm-test-gamma-cost-2.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="SVM.html#cb71-1"></a><span class="kw">plot</span>(mod.svm4,df,<span class="dt">grid=</span><span class="dv">250</span>)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/svm-test-gamma-cost-3.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="SVM.html#cb72-1"></a>mod.svm2<span class="op">$</span>nSV</span></code></pre></div>
<pre><code>[1] 244 244</code></pre>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="SVM.html#cb74-1"></a>mod.svm3<span class="op">$</span>nSV</span></code></pre></div>
<pre><code>[1] 114 114</code></pre>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="SVM.html#cb76-1"></a>mod.svm4<span class="op">$</span>nSV</span></code></pre></div>
<pre><code>[1] 78 77</code></pre>
<div class="corR">
<p>
Le nombre de vecteurs supports diminue lorsque <span class="math inline"><span class="math inline">\(C\)</span></span> augmente. Une forte valeur de <span class="math inline"><span class="math inline">\(C\)</span></span> autorise moins d’observations à être dans la marge, elle a donc tendance à diminuer (risque de surapprentissage).
</p>
</div></li>
<li><p>Sélectionner automatiquement ces paramètres. On pourra utiliser la fonction <strong>tune</strong> en faisant varier <strong>C</strong> dans <strong>c(0.1,1,10,100,1000)</strong> et <strong>gamma</strong> dans <strong>c(0.5,1,2,3,4)</strong>.</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="SVM.html#cb78-1"></a><span class="kw">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb78-2"><a href="SVM.html#cb78-2"></a>tune.out &lt;-<span class="st"> </span><span class="kw">tune</span>(svm,Y<span class="op">~</span>.,<span class="dt">data=</span>df,<span class="dt">kernel=</span><span class="st">&quot;radial&quot;</span>,</span>
<span id="cb78-3"><a href="SVM.html#cb78-3"></a>             <span class="dt">ranges=</span><span class="kw">list</span>(<span class="dt">cost=</span><span class="kw">c</span>(<span class="fl">0.1</span>,<span class="dv">1</span>,<span class="dv">10</span>,<span class="dv">100</span>,<span class="dv">1000</span>),<span class="dt">gamma=</span><span class="kw">c</span>(<span class="fl">0.5</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>)))</span>
<span id="cb78-4"><a href="SVM.html#cb78-4"></a><span class="kw">summary</span>(tune.out)</span></code></pre></div>
<pre><code>
Parameter tuning of &#39;svm&#39;:

- sampling method: 10-fold cross validation 

- best parameters:
 cost gamma
   10   0.5

- best performance: 0.108 

- Detailed performance results:
    cost gamma error dispersion
1  1e-01   0.5 0.182 0.04565572
2  1e+00   0.5 0.148 0.03155243
3  1e+01   0.5 0.108 0.03425395
4  1e+02   0.5 0.116 0.03373096
5  1e+03   0.5 0.112 0.03425395
6  1e-01   1.0 0.184 0.04402020
7  1e+00   1.0 0.120 0.03651484
8  1e+01   1.0 0.120 0.03126944
9  1e+02   1.0 0.112 0.03155243
10 1e+03   1.0 0.120 0.03887301
11 1e-01   2.0 0.170 0.04136558
12 1e+00   2.0 0.124 0.02458545
13 1e+01   2.0 0.122 0.03457681
14 1e+02   2.0 0.124 0.03502380
15 1e+03   2.0 0.142 0.03705851
16 1e-01   3.0 0.160 0.03651484
17 1e+00   3.0 0.124 0.02458545
18 1e+01   3.0 0.126 0.03134042
19 1e+02   3.0 0.132 0.04022161
20 1e+03   3.0 0.166 0.03272783
21 1e-01   4.0 0.154 0.03777124
22 1e+00   4.0 0.124 0.02458545
23 1e+01   4.0 0.126 0.03134042
24 1e+02   4.0 0.138 0.04467164
25 1e+03   4.0 0.190 0.05754226</code></pre>
<div class="corR">
<p>
La sélection est faite en minimisant l’erreur de classification par validation croisée 10 blocs.
</p>
</div></li>
<li><p>Faire de même avec <strong>caret</strong>, on utilisera <strong>method=“svmRadial”</strong> et <strong>prob.model=TRUE</strong>.</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="SVM.html#cb80-1"></a>C &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.001</span>,<span class="fl">0.01</span>,<span class="dv">1</span>,<span class="dv">10</span>,<span class="dv">100</span>,<span class="dv">1000</span>)</span>
<span id="cb80-2"><a href="SVM.html#cb80-2"></a>sigma &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.5</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>)</span>
<span id="cb80-3"><a href="SVM.html#cb80-3"></a>gr &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">C=</span>C,<span class="dt">sigma=</span>sigma)</span>
<span id="cb80-4"><a href="SVM.html#cb80-4"></a>ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>)</span>
<span id="cb80-5"><a href="SVM.html#cb80-5"></a>res.caret1 &lt;-<span class="st"> </span><span class="kw">train</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df,<span class="dt">method=</span><span class="st">&quot;svmRadial&quot;</span>,<span class="dt">trControl=</span>ctrl,<span class="dt">tuneGrid=</span>gr,<span class="dt">prob.model=</span><span class="ot">TRUE</span>)</span>
<span id="cb80-6"><a href="SVM.html#cb80-6"></a>res.caret1</span></code></pre></div>
<pre><code>Support Vector Machines with Radial Basis Function Kernel 

500 samples
  2 predictor
  2 classes: &#39;0&#39;, &#39;1&#39; 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 449, 450, 451, 450, 449, 450, ... 
Resampling results across tuning parameters:

  C      sigma  Accuracy   Kappa    
  1e-03  0.5    0.8359976  0.6734429
  1e-03  1.0    0.8439584  0.6890006
  1e-03  2.0    0.8398359  0.6806352
  1e-03  3.0    0.8578816  0.7164180
  1e-03  4.0    0.8577623  0.7162786
  1e-02  0.5    0.8400384  0.6813511
  1e-02  1.0    0.8419584  0.6851382
  1e-02  2.0    0.8458784  0.6926478
  1e-02  3.0    0.8518407  0.7044624
  1e-02  4.0    0.8577623  0.7162786
  1e+00  0.5    0.8676871  0.7347434
  1e+00  1.0    0.8857719  0.7713024
  1e+00  2.0    0.8838127  0.7672737
  1e+00  3.0    0.8798519  0.7594972
  1e+00  4.0    0.8838928  0.7675507
  1e+01  0.5    0.8798095  0.7596483
  1e+01  1.0    0.8818111  0.7634823
  1e+01  2.0    0.8838928  0.7676862
  1e+01  3.0    0.8778503  0.7554248
  1e+01  4.0    0.8720480  0.7438434
  1e+02  0.5    0.8818111  0.7635668
  1e+02  1.0    0.8839320  0.7677447
  1e+02  2.0    0.8680480  0.7359331
  1e+02  3.0    0.8517231  0.7031601
  1e+02  4.0    0.8377591  0.6749363
  1e+03  0.5    0.8760088  0.7521217
  1e+03  1.0    0.8760496  0.7520939
  1e+03  2.0    0.8498816  0.6998254
  1e+03  3.0    0.8297967  0.6590033
  1e+03  4.0    0.8100352  0.6192088

Accuracy was used to select the optimal model using the largest value.
The final values used for the model were sigma = 1 and C = 1.</code></pre>
<div class="corR">
<p>
On peut également répéter plusieurs fois la validation croisée pour stabiliser les résultats (on parallélise avec <strong>doParallel</strong>) :
</p>
</div>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="SVM.html#cb82-1"></a><span class="kw">library</span>(doParallel) <span class="co">## pour paralléliser</span></span>
<span id="cb82-2"><a href="SVM.html#cb82-2"></a>cl &lt;-<span class="st"> </span><span class="kw">makePSOCKcluster</span>(<span class="dv">4</span>)</span>
<span id="cb82-3"><a href="SVM.html#cb82-3"></a><span class="kw">registerDoParallel</span>(cl)</span>
<span id="cb82-4"><a href="SVM.html#cb82-4"></a><span class="kw">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb82-5"><a href="SVM.html#cb82-5"></a>ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;repeatedcv&quot;</span>,<span class="dt">number=</span><span class="dv">10</span>,<span class="dt">repeats=</span><span class="dv">5</span>)</span>
<span id="cb82-6"><a href="SVM.html#cb82-6"></a>res.caret2 &lt;-<span class="st"> </span><span class="kw">train</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df,<span class="dt">method=</span><span class="st">&quot;svmRadial&quot;</span>,<span class="dt">trControl=</span>ctrl,<span class="dt">tuneGrid=</span>gr,<span class="dt">prob.model=</span><span class="ot">TRUE</span>)</span>
<span id="cb82-7"><a href="SVM.html#cb82-7"></a><span class="kw">on.exit</span>(<span class="kw">stopCluster</span>(cl))</span>
<span id="cb82-8"><a href="SVM.html#cb82-8"></a>res.caret2</span></code></pre></div>
<pre><code>Support Vector Machines with Radial Basis Function Kernel 

500 samples
  2 predictor
  2 classes: &#39;0&#39;, &#39;1&#39; 

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 5 times) 
Summary of sample sizes: 450, 449, 450, 451, 450, 449, ... 
Resampling results across tuning parameters:

  C      sigma  Accuracy   Kappa    
  1e-03  0.5    0.8222641  0.6462662
  1e-03  1.0    0.8458598  0.6928018
  1e-03  2.0    0.8507000  0.7022920
  1e-03  3.0    0.8555163  0.7118454
  1e-03  4.0    0.8607898  0.7222353
  1e-02  0.5    0.8242566  0.6502516
  1e-02  1.0    0.8462519  0.6935931
  1e-02  2.0    0.8499238  0.7007974
  1e-02  3.0    0.8543078  0.7094622
  1e-02  4.0    0.8639097  0.7284572
  1e+00  0.5    0.8640626  0.7275460
  1e+00  1.0    0.8839933  0.7678344
  1e+00  2.0    0.8843858  0.7685479
  1e+00  3.0    0.8823531  0.7644392
  1e+00  4.0    0.8799766  0.7596759
  1e+01  0.5    0.8848178  0.7696785
  1e+01  1.0    0.8803851  0.7606211
  1e+01  2.0    0.8775757  0.7549666
  1e+01  3.0    0.8751989  0.7501291
  1e+01  4.0    0.8727989  0.7453460
  1e+02  0.5    0.8815531  0.7631204
  1e+02  1.0    0.8751107  0.7501217
  1e+02  2.0    0.8743443  0.7484731
  1e+02  3.0    0.8615653  0.7229593
  1e+02  4.0    0.8507228  0.7011391
  1e+03  0.5    0.8803600  0.7607328
  1e+03  1.0    0.8731277  0.7462531
  1e+03  2.0    0.8499715  0.7000011
  1e+03  3.0    0.8319834  0.6640949
  1e+03  4.0    0.8089513  0.6174340

Accuracy was used to select the optimal model using the largest value.
The final values used for the model were sigma = 0.5 and C = 10.</code></pre></li>
<li><p>Visualiser la règle sélectionnée.</p>
<div class="corR">
<p>
<strong>caret</strong> utilise la fonction <strong>ksvm</strong> du package <strong>kernlab</strong>. Ce package propose un choix plus large pour les noyaux. Par conséquent, si on souhaite visualiser la svm sélectionnée par caret, il est préférable d’utiliser cette fonction.
</p>
</div>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="SVM.html#cb84-1"></a><span class="kw">library</span>(kernlab)</span>
<span id="cb84-2"><a href="SVM.html#cb84-2"></a>C.opt &lt;-<span class="st"> </span>res.caret2<span class="op">$</span>bestTune<span class="op">$</span>C</span>
<span id="cb84-3"><a href="SVM.html#cb84-3"></a>sigma.opt &lt;-<span class="st"> </span>res.caret2<span class="op">$</span>bestTune<span class="op">$</span>sigma</span>
<span id="cb84-4"><a href="SVM.html#cb84-4"></a>svm.sel &lt;-<span class="st"> </span><span class="kw">ksvm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df,<span class="dt">kernel=</span><span class="st">&quot;rbfdot&quot;</span>,<span class="dt">kpar=</span><span class="kw">list</span>(<span class="dt">sigma=</span>sigma.opt),<span class="dt">C=</span>C.opt)</span>
<span id="cb84-5"><a href="SVM.html#cb84-5"></a><span class="kw">plot</span>(svm.sel,<span class="dt">data=</span>df)</span></code></pre></div>
<p><img src="TUTO_ML_files/figure-html/unnamed-chunk-59-1.png" width="672" style="display: block; margin: auto;" /></p></li>
</ol>
</div>
<div id="exercices" class="section level2">
<h2><span class="header-section-number">2.4</span> Exercices</h2>

<div class="exercise">
<span id="exr:exo-svm-cas-sep" class="exercise"><strong>Exercice 2.1  (Résolution du problème d’optimisation dans le cas séparable)  </strong></span>
</div>

<p>On considère <span class="math inline">\(n\)</span> observations <span class="math inline">\((x_1,y_1),\dots,(x_n,y_n)\)</span> telles que <span class="math inline">\((x_i,y_i)\in\mathbb R^p\times\{-1,1\}\)</span>. On cherche à expliquer la variable <span class="math inline">\(Y\)</span> par <span class="math inline">\(X\)</span>. On considère l’algorithme SVM et on se place dans le cas où les données sont séparables.</p>
<ol style="list-style-type: decimal">
<li><p>Soit <span class="math inline">\(\mathcal H\)</span> un hyperplan séparateur d’équation <span class="math inline">\(\langle w,x\rangle+b=0\)</span> où <span class="math inline">\(w\in\mathbb R^p,b\in\mathbb R\)</span>. Exprimer la distance entre <span class="math inline">\(x_i,i=1,\dots,n\)</span> et <span class="math inline">\(\mathcal H\)</span> en fonction de <span class="math inline">\(w\)</span> et <span class="math inline">\(b\)</span>.</p>
<div class="correction">
<p>
Soit <span class="math inline"><span class="math inline">\(x_0\in\mathcal H\)</span></span>. La solution correspond à la norme du projeté orthogonal de <span class="math inline"><span class="math inline">\(x-x_0\)</span></span> sur <span class="math inline"><span class="math inline">\(\mathcal H\)</span></span>, elle est donc colinéaire à <span class="math inline"><span class="math inline">\(w\)</span></span> (car <span class="math inline"><span class="math inline">\(w\)</span></span> est normal à <span class="math inline"><span class="math inline">\(\mathcal H\)</span></span>) et s’écrit <span class="math display"><span class="math display">\[\frac{\langle x-x_0,w\rangle}{\|w\|}w=\frac{\langle x,w\rangle}{\|w\|}w-\frac{\langle x_0,w\rangle}{\|w\|}w,\]</span></span> Comme <span class="math inline"><span class="math inline">\(\langle x_0,w\rangle=-b\)</span></span>, on déduit <span class="math inline"><span class="math inline">\(d_{\mathcal H}(x)=\frac{|\langle w,x\rangle+b|}{\|w\|}=x^tw+b\)</span></span> si <span class="math inline"><span class="math inline">\(\|w\|=1\)</span></span>.
</p>
</div></li>
<li><p>Expliquer la logique du problème d’optimisation
<span class="math display">\[\max_{w,b,\|w\|=1}M\]</span>
<span class="math display">\[\textrm{sous les contraintes } y_i(\langle w,x_i\rangle+b)\geq M,\ i=1,\dots,n.\]</span></p>
<div class="correction">
<p>
<p>L’approche consiste à choisir l’hyperplan :</p>
</p>
<ul>
<li>
<p>qui sépare les groupes ;</p>
</li>
<li>
<p>tel que la distance entre les observations et lui soit maximale.</p>
</li>
</ul>
</div></li>
<li><p>Montrer que ce problème peut se réécrire
<span class="math display">\[\min_{w,b}\frac{1}{2}\|w\|^2\]</span>
<span class="math display">\[\textrm{sous les contraintes } y_i(\langle w,x_i\rangle+b)\geq 1,\ i=1,\dots,n.\]</span></p>
<div class="correction">
<p>
Il suffit de poser comme contrainte <span class="math inline"><span class="math inline">\(M=1/\|w\|\)</span></span>.
</p>
</div></li>
<li><p>On rappelle que pour la minimisation d’une fonction <span class="math inline">\(h:\mathbb R^p\to\mathbb R\)</span> sous contraintes affines <span class="math inline">\(g_i(u)\geq 0,i=1,\dots,n\)</span>, le Lagrangien s’écrit
<span class="math display">\[L(u,\alpha)=h(u)-\sum_{i=1}^n\alpha_ig_i(u).\]</span>
Si on désigne par <span class="math inline">\(u_\alpha=\mathop{\mathrm{argmin}}_uL(u,\alpha)\)</span>, la fonction duale est alors donnée par
<span class="math display">\[\theta(\alpha)=L(u_\alpha,\alpha)=\min_{u\in\mathbb R^p}L(u,\alpha),\]</span>
et le problème dual consiste à maximiser <span class="math inline">\(\theta(\alpha)\)</span> sous les contraintes <span class="math inline">\(\alpha_i\geq 0\)</span>. En désignant par <span class="math inline">\(\alpha^\star\)</span> la solution de ce problème, on déduit la solution du problème primal <span class="math inline">\(u^\star=u_{\alpha^\star}\)</span>. Les conditions de Karush-Kuhn-Tucker sont données par</p>
<ul>
<li><span class="math inline">\(\alpha_i^\star\geq 0\)</span>.</li>
<li><span class="math inline">\(g_i(u_{\alpha^\star})\geq 0\)</span>.</li>
<li><span class="math inline">\(\alpha_i^\star g_i(u_{\alpha^\star})=0\)</span>.</li>
</ul>
<ol style="list-style-type: lower-alpha">
<li>Écrire le Lagrangien du problème considéré et en déduire une expression de <span class="math inline">\(w\)</span> en fonction des <span class="math inline">\(\alpha_i\)</span> et des observations.
<div class="correction">
<p>
Le lagrangien s’écrit <span class="math display"><span class="math display">\[L(w,b;\alpha)=\frac{1}{2}\|w\|^2-\sum_{i=1}^n\alpha_i[y_i(x_i^tw+b)-1].\]</span></span> On a alors <span class="math display"><span class="math display">\[\frac{\partial L(w,b;\alpha)}{\partial w}=w-\sum_{i=1}^n\alpha_iy_ix_i=0\]</span></span> et <span class="math display"><span class="math display">\[\frac{\partial L(w,b;\alpha)}{\partial b}=-\sum_{i=1}^n\alpha_iy_i=0.\]</span></span> D’où <span class="math inline"><span class="math inline">\(w_\alpha=\sum_{i=1}^n\alpha_iy_ix_i\)</span></span>.
</p>
</div></li>
<li>Écrire la fonction duale.
<div class="correction">
<p>
La fonction duale s’écrit <span class="math display"><span class="math display">\[\begin{align*}
   \theta(\alpha)=L(w_\alpha,b_\alpha;\alpha)= &amp;\        \frac{1}{2}\langle \sum_i\alpha_iy_ix_i,\sum_j\alpha_jy_jx_j\rangle-\sum_i\alpha_iy_i\langle \sum_j\alpha_jy_jx_j,x_i\rangle-\sum_i\alpha_iy_ib+\sum_i\alpha_i \\
   = &amp;\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_jx_i^tx_j
   \end{align*}\]</span></span>
</p>
</div></li>
<li>Écrire les conditions KKT et en déduire les solutions <span class="math inline">\(w^\star\)</span> et <span class="math inline">\(b^\star\)</span>.
<div class="correction">
<p>
Les conditions KKT sont pour tout <span class="math inline"><span class="math inline">\(i=1,\dots,n\)</span></span> : <span class="math display"><span class="math display">\[\alpha_i^\star\geq 0 \quad\text{et}\quad \alpha_i^\star[y_i(x_i^tw+b)-1]=0.\]</span></span> On obtient ainsi <span class="math display"><span class="math display">\[w^\star=\sum_{i=1}^n\alpha_i^\star y_ix_i.\]</span></span> et <span class="math inline"><span class="math inline">\(b^\star\)</span></span> en résolvant <span class="math display"><span class="math display">\[\alpha_i^\star[y_i(x_i^tw+b)-1]=0\]</span></span> pour un <span class="math inline"><span class="math inline">\(\alpha_i^\star\)</span></span> non nul.
</p>
</div></li>
<li>Interpréter les conditions KKT.
<div class="correction">
<p>
Les <span class="math inline"><span class="math inline">\(x_i\)</span></span> tels que <span class="math inline"><span class="math inline">\(\alpha_i^\star&gt;0\)</span></span> vérifient <span class="math display"><span class="math display">\[y_i(x_i^tw^\star+b^\star)=1.\]</span></span> Ils se situent donc sur la frontière définissant la marge maximale. Ce sont les vecteurs supports.
</p>
</div></li>
</ol></li>
</ol>

<div class="exercise">
<span id="exr:exo-svm-regle-main" class="exercise"><strong>Exercice 2.2  (Règle svm à partir de sorties R)  </strong></span>
</div>

<p>On considère <span class="math inline">\(n\)</span> observations <span class="math inline">\((x_1,y_1),\dots,(x_n,y_n)\)</span> telles que <span class="math inline">\((x_i,y_i)\in\mathbb R^3\times\{-1,1\}\)</span>. On cherche à expliquer la variable <span class="math inline">\(Y\)</span> par <span class="math inline">\(X=(X_1,X_2,X_3)\)</span>. On considère l’algorithme SVM et on se place dans le cas où les données sont séparables. On rappelle que cet algorithme consiste à chercher une droite d’équation <span class="math inline">\(w^tx+b=0\)</span> où <span class="math inline">\((w,b)\in\mathbb R^3\times\mathbb R\)</span> sont solutions du problème d’optimisation (problème primal)
<span class="math display">\[\min_{w,b}\frac{1}{2}\|w\|^2\]</span>
<span class="math display">\[\textrm{sous les contraintes } y_i(w^tx_i+b)\geq 1,\ i=1,\dots,n.\]</span>
On désigne par <span class="math inline">\(\alpha_i^\star,i=1,\dots,n\)</span>, les solutions du problème dual et par <span class="math inline">\((w^\star,b^\star)\)</span> les solutions du problème ci-dessus.</p>
<ol style="list-style-type: decimal">
<li><p>Donner la formule permettant de calculer <span class="math inline">\(w^\star\)</span> en fonction des <span class="math inline">\(\alpha_i^\star\)</span>.</p>
<div class="correction">
<p>
<span class="math inline"><span class="math inline">\(w^\star\)</span></span> se calcule selon <span class="math display"><span class="math display">\[w^\star=\sum_{i=1}^n\alpha_i^\star y_ix_i.\]</span></span> Les <span class="math inline"><span class="math inline">\(\alpha_i^\star\)</span></span> étant nuls pour les vecteurs non supports, il suffit de sommer sur les vecteur supports.
</p>
</div></li>
<li><p>Expliquer comment on classe un nouveau point <span class="math inline">\(x\in\mathbb R^3\)</span> par la méthode <strong>svm</strong>.</p>
<div class="correction">
<p>
Une fois <span class="math inline"><span class="math inline">\(w^\star\)</span></span> et <span class="math inline"><span class="math inline">\(b^\star\)</span></span> obtenus, la règle s’écrit <span class="math display"><span class="math display">\[g(x)=1_{\langle w^\star,x\rangle+b^\star\leq 0}-1_{\langle w^\star,x\rangle+b^\star&gt;0}.\]</span></span>
</p>
</div></li>
<li><p>Les données se trouvent dans un dataframe <code>df</code>. On exécute</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="SVM.html#cb85-1"></a>mod.svm &lt;-<span class="st"> </span><span class="kw">svm</span>(Y<span class="op">~</span>.,<span class="dt">data=</span>df,<span class="dt">kernel=</span><span class="st">&quot;linear&quot;</span>,<span class="dt">cost=</span><span class="dv">10000000000</span>)</span></code></pre></div>
<p>et on obtient</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="SVM.html#cb86-1"></a>df[mod.svm<span class="op">$</span>index,]</span></code></pre></div>
<pre><code>     X1   X2   X3  Y
51 -1.1 -1.0 -1.0  1
92  0.7  0.8  1.1  1
31  0.7  0.5 -1.0 -1
37 -0.5 -0.6  0.3 -1</code></pre>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="SVM.html#cb88-1"></a>mod.svm<span class="op">$</span>coefs</span></code></pre></div>
<pre><code>     [,1]
[1,]   59
[2,]   49
[3,]  -30
[4,]  -79</code></pre>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="SVM.html#cb90-1"></a>mod.svm<span class="op">$</span>rho</span></code></pre></div>
<pre><code>[1] -0.5</code></pre>
<p>Calculer les valeurs de <span class="math inline">\(w^\star\)</span> et <span class="math inline">\(b^\star\)</span>. En déduire la règle de classification.</p>
<div class="correction">
<p>
<span class="math inline"><span class="math inline">\(b^\star\)</span></span> est l’opposé de <code>mod.svm$rho</code>. Pour <span class="math inline"><span class="math inline">\(w^\star\)</span></span> il suffit d’appliquer la formule et on trouve
</p>
</div>
<pre><code>   X1    X2    X3 
-12.1  12.6   1.2 </code></pre></li>
<li><p>On dispose d’une nouvelle observation <span class="math inline">\(x=(1,-0.5,-1)\)</span>. Dans quel groupe (<code>-1</code> ou <code>1</code>) l’algorithme affecte cette nouvelle donnée ?</p>
<div class="correction">
<p>
On calcule la combinaison linéaire <span class="math inline"><span class="math inline">\(\langle w^\star,x\rangle+b^\star\)</span></span> :
</p>
</div>
<pre><code>[1] -19.1</code></pre>
<div class="correction">
<p>
On affectera donc la nouvelle donnée au groupe -1.
</p>
</div></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="caret.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="arbres.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["TUTO_ML.pdf", "TUTO_ML.epub"],
"toc": {
"collapse": "subsection",
"sharing": {
"facebook": true,
"github": true,
"twitter": true
}
},
"highlight": "tango"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
